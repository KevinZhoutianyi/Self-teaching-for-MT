{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import  AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 10000, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 2000, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = -1, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=32,     help='Batch size for test and validation')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=16,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=8,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_de2en', type=str,             default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='SST2,withnoise',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=200,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=2000,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=5,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-1 ,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.999,    help='momentum')\n",
    "parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=10,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=1,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "parser.add_argument('--freeze', type=int,                       default=0,    help='whether freeze the pretrained encoder')\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=1 ,     help='whether train A')\n",
    "\n",
    "parser.add_argument('--embedding_dim', type=int,                default=300 ,     help='whether train A')\n",
    "parser.add_argument('--out_dim', type=int,                      default=2 ,     help='whether train A')\n",
    "parser.add_argument('--hidden_size', type=int,                  default=64 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "args.test_num = args.test_num//args.batch_size * args.batch_size\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tianyi-vol/Self-teaching-for-machine-translation/BERT/wandb/run-20220703_081557-zpnkl6gw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/zpnkl6gw\" target=\"_blank\">SST2,withnoise</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/zpnkl6gw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb7476cb4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY'] = 'a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME'] = args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "wandb.init(project=\"Selftraining\", config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:05 AM |\t  Reusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1238.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:05 AM |\t  Namespace(A_lr=0.01, batch_size=32, beta1=0.9, beta2=0.999, decay=0.001, decay_lr=1, embedding_dim=300, epochs=50, exp_name='SST2,withnoise', freeze=0, gpu=0, grad_acc_count=-1, grad_clip=5, hidden_size=64, learning_rate_min=1e-08, model_name_de2en='roberta-base', model_name_student='roberta-base', model_name_teacher='roberta-base', num_step_lr=10, num_workers=0, out_dim=2, pre_epochs=0, rep_num=192, syndata_loss_ratio=1, test_num=1984, test_num_points=-1, train_A=1, train_A_num_points=8, train_num_points=2000, train_v_num_points=0, train_v_synthetic_num_points=8, train_w_num_points=16, traindata_loss_ratio=0, unrolled_v_lr=2e-06, unrolled_w_lr=2e-06, v_lr=2e-06, valid_begin=1, valid_num_points=10000, w_lr=2e-06, warm=10)\n",
      "07/03 08:16:05 AM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "07/03 08:16:05 AM |\t  {'sentence': \"that 's far too tragic to merit such superficial treatment \", 'label': 0, 'idx': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:21 AM |\t  modelsize:124.697433MB\n",
      "07/03 08:16:34 AM |\t  modelsize:124.697433MB\n",
      "07/03 08:16:47 AM |\t  modelsize:124.697433MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "modelname = args.model_name_teacher\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_de2en\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:47 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4180464d96c536a6.arrow\n",
      "07/03 08:16:47 AM |\t  Loading cached split indices for dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e3214aea03fb311a.arrow and /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e21fdb64303159bd.arrow\n",
      "07/03 08:16:47 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-dbd9131dc8b0f2ea.arrow\n",
      "07/03 08:16:47 AM |\t  train len: 1984\n",
      "07/03 08:16:47 AM |\t  train_w_num_points_len and train_v_num_points_len: 992\n",
      "07/03 08:16:47 AM |\t  train_v_synthetic_num_points_len: 496\n",
      "07/03 08:16:47 AM |\t  train_A_num_points_len: 496\n",
      "07/03 08:16:48 AM |\t  valid len: 9984\n",
      "07/03 08:16:48 AM |\t  test len: 872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "\n",
    "def r(a, b):\n",
    "    return b*(a//b)\n",
    "temp = dataset['train'].shuffle(seed=seed_).select(range(r(args.train_num_points, args.batch_size) + r(args.valid_num_points, args.batch_size))).\\\n",
    "    train_test_split(test_size=r(args.valid_num_points, args.batch_size)/(\n",
    "        r(args.train_num_points, args.batch_size) + r(args.valid_num_points, args.batch_size)))\n",
    "train = temp['train']\n",
    "valid = temp['test']\n",
    "test = dataset['validation'].shuffle(seed=seed_)\n",
    "\n",
    "logging.info(\"train len: %d\", len(train))\n",
    "\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len and train_v_num_points_len: %d\", train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",\n",
    "             train_v_synthetic_num_points_len)\n",
    "# logging.info(\"train_v_num_points_len: %d\", train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\", train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\", len(valid))\n",
    "logging.info(\"test len: %d\", len(test))\n",
    "# logging.info(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:48 AM |\t  train w data size:batchsize:16\t numofbatch:62\t totoal:992\n",
      "07/03 08:16:48 AM |\t  train syn data size:batchsize:8\t numofbatch:62\t totoal:496\n",
      "07/03 08:16:48 AM |\t  train A data size:batchsize:8\t numofbatch:62\t totoal:496\n",
      "07/03 08:16:48 AM |\t  validation data size:batchsize:32\t numofbatch:312\t totoal:9984\n",
      "07/03 08:16:49 AM |\t  test data size:batchsize:32\t numofbatch:28\t totoal:896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data_idx = get_data_idx(train[:train_w_num_points_len], tokenizer,train_w_num_points_len)\n",
    "train_data = get_data(train[train_w_num_points_len:], tokenizer)\n",
    "indices = list(range(len(train)-train_w_num_points_len))\n",
    "\n",
    "train_w_dataloader = DataLoader(train_data_idx, sampler=SequentialSampler(train_data_idx),\n",
    "                              batch_size=args.train_w_num_points, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train w data size:{get_dataloader_size(train_w_dataloader)}')\n",
    "\n",
    "\n",
    "train_syn_dataloader = DataLoader(train_data, sampler=SubsetRandomSampler(indices[:train_v_synthetic_num_points_len]),\n",
    "                              batch_size=args.train_v_synthetic_num_points, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train syn data size:{get_dataloader_size(train_syn_dataloader)}')\n",
    "\n",
    "\n",
    "train_A_dataloader = DataLoader(train_data, sampler=SubsetRandomSampler(indices[train_v_synthetic_num_points_len:train_v_synthetic_num_points_len+train_A_num_points_len]),\n",
    "                              batch_size=args.train_A_num_points, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train A data size:{get_dataloader_size(train_A_dataloader)}')\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "valid_data = get_data(valid, tokenizer)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data),\n",
    "                              batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'validation data size:{get_dataloader_size(valid_dataloader)}')\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "test_data = get_data(test, tokenizer)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                             batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)  # , sampler=RandomSampler(test_data)\n",
    "logging.info(f'test data size:{get_dataloader_size(test_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(tokenizer, args)  # half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = Model(tokenizer, args, 'teacher')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(\n",
    "),  lr=args.w_lr,  betas=(args.beta1, args.beta2), eps=1e-8)\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_w = StepLR(\n",
    "    w_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "model_v = Model(tokenizer, args, 'student')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(\n",
    "),  lr=args.v_lr,  betas=(args.beta1, args.beta2), eps=1e-8)\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_v = StepLR(\n",
    "    v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    objs_top1 = AvgrageMeter()\n",
    "    objs_top5 = AvgrageMeter()\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        logits,ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,model)\n",
    "        n = test_dataloaderx.shape[0]\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        prec1, prec5 = accuracy(logits, test_dataloadery, topk=(1, 1))\n",
    "                \n",
    "        objs_top1.update(prec1.item(), n)\n",
    "        \n",
    "        objs_top5.update(prec5.item(), n)\n",
    "    acc = objs_top1.avg\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    logging.info('%s top1 : %f',model.name,objs_top1.avg)\n",
    "    objs_top1.reset()\n",
    "    logging.info('%s top5 : %f',model.name,objs_top5.avg)\n",
    "    objs_top5.reset()\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    model.eval()\n",
    "    return acc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, wdataloader,syndataloader,Adataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter, past_v_accu):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    objs_w_top1 = AvgrageMeter()\n",
    "    objs_w_top5 = AvgrageMeter()\n",
    "    objs_v_top1 = AvgrageMeter()\n",
    "    objs_v_top5 = AvgrageMeter()\n",
    "    improvementacc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.train_w_num_points\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points\n",
    "    Asize = args.train_A_num_points\n",
    "    loader_len = len(wdataloader)\n",
    "    w_model.eval()\n",
    "    v_model.eval()\n",
    "\n",
    "    for step, w_batch in enumerate(wdataloader):\n",
    "\n",
    "\n",
    "\n",
    "        input_w = Variable(w_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_w_attn = Variable(w_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        output_w = Variable(w_batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        attn_idx = Variable(w_batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "        syn_batch = next(iter(syndataloader))\n",
    "        input_syn = Variable(syn_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_syn_attn = Variable(syn_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "\n",
    "        A_batch = next(iter(Adataloader))\n",
    "        input_A_v = Variable(A_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_A_v_attn = Variable(A_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        output_A_v = Variable(A_batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "\n",
    "        tot_iter[0] += input_w.shape[0]\n",
    "        \n",
    "        if(True):  # let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "        output_w[:8]= 1-output_w[:8]# noise input\n",
    "\n",
    "        v_star_val_loss=0\n",
    "        if (args.train_A == 1 and epoch>=args.pre_epochs):\n",
    "            epsilon_w = args.unrolled_w_lr\n",
    "            epsilon_v  = args.unrolled_v_lr\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, attn_idx,v_optimizer,\n",
    "                                             epsilon_w, epsilon_v,args.grad_clip)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "\n",
    "            \n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        logits, loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                                  A,attn_idx, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "        prec1, prec5 = accuracy(logits, output_w, topk=(1, 1))\n",
    "        objs_w_top1.update(prec1.item(), wsize)\n",
    "        objs_w_top5.update(prec5.item(), wsize)\n",
    "\n",
    "        if(epoch >= args.pre_epochs):\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(\n",
    "                input_syn, input_syn_attn, w_model, v_model)\n",
    "            logits, loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                                    v_model)\n",
    "            v_loss = (args.traindata_loss_ratio*loss +\n",
    "                      loss_aug*args.syndata_loss_ratio)\n",
    "            v_loss.backward()\n",
    "            objs_v_syn.update(loss_aug.item(), synsize)\n",
    "            objs_v_train.update(loss.item(), vsize)\n",
    "            v_optimizer.step()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "            prec1, prec5 = accuracy(logits, output_v, topk=(1, 1))\n",
    "            objs_v_top1.update(prec1.item(), vsize)\n",
    "            objs_v_top5.update(prec5.item(), vsize)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _,new_v_loss = my_loss2(\n",
    "            input_A_v, input_A_v_attn,  output_A_v,model_v)\n",
    "            improvementacc+=v_star_val_loss-new_v_loss.item()\n",
    "            objs_v_val.update(new_v_loss.item(), Asize)\n",
    "\n",
    "        output_w[:8]= 1-output_w[:8]# noise input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            w_accu = my_test(validdataloader, model_w, epoch)\n",
    "            v_accu = my_test(validdataloader, model_v, epoch)\n",
    "            wandb.log({'W_test_accuracy': w_accu})\n",
    "            wandb.log({'v_test_accuracy':v_accu})\n",
    "            if(v_accu>past_v_accu):\n",
    "                past_v_accu = v_accu\n",
    "                logging.info('find a better model')\n",
    "                torch.save(model_w, './model/'+'model_w.pt')  # +now+\n",
    "                torch.save(model_v, './model/'+'model_v.pt')\n",
    "                torch.save(A, './model/'+'A.pt')\n",
    "                torch.save(model_w.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_w.pt\"))\n",
    "                torch.save(model_v.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_v.pt\"))\n",
    "                torch.save(A.state_dict(), os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "                wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "\n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info(f\"{progress:5.3}%:\\t  W_train_loss:{objs_w.avg:^.7f}\\tV_train_syn_loss:{objs_v_syn.avg:^.7f}\\tV_train_loss:{objs_v_train.avg:^.7f}\\t  V_val_loss:{objs_v_val.avg:^.7f}\\t  V_star_val_loss:{objs_v_star_val.avg:^.7f}\\t  improvement:{objs_v_star_val.avg-objs_v_val.avg:^.7f}\\t w_top1:{objs_w_top1.avg:^.7f}\\t  w_top5:{objs_w_top5.avg:^.7f}\\t v_top1:{objs_v_top1.avg:^.7f}\\t v_top5:{objs_v_top5.avg:^.7f}\\t \")\n",
    "            with torch.no_grad():\n",
    "                temp = A(input_w, input_w_attn, attn_idx)\n",
    "            logging.info(f\"weight:{temp}\")\n",
    "            logging.info(f'noise:{torch.mean(temp[0:8])} mean:{torch.mean(temp)} max: {torch.max(temp)} min: {torch.min(temp)}')\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'W_accuracy': objs_w_top1.avg})\n",
    "            wandb.log({'v_accuracy': objs_v_top1.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "            objs_w_top1.reset()\n",
    "            objs_w_top5.reset()\n",
    "    logging.info(f'improvment:{improvementacc}')\n",
    "    return w_trainloss_acc,past_v_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/03 08:16:50 AM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:17:14 AM |\t   18.0%:\t  W_train_loss:0.7250958\tV_train_syn_loss:0.6966705\tV_train_loss:0.6960093\t  V_val_loss:0.6894631\t  V_star_val_loss:0.6894967\t  improvement:0.0000335\t w_top1:48.4375000\t  w_top5:48.4375000\t v_top1:47.3958333\t v_top5:47.3958333\t \n",
      "07/03 08:17:14 AM |\t  weight:tensor([1.0012, 0.9843, 0.9932, 1.0235, 1.0121, 1.0202, 1.0129, 0.9888, 0.9821,\n",
      "        1.0142, 0.9906, 1.0171, 0.9888, 0.9897, 0.9911, 0.9902],\n",
      "       device='cuda:0')\n",
      "07/03 08:17:14 AM |\t  noise:1.0045220851898193 mean:1.0 max: 1.0234780311584473 min: 0.9820737242698669\n",
      "07/03 08:17:39 AM |\t   37.7%:\t  W_train_loss:0.7041691\tV_train_syn_loss:0.6833729\tV_train_loss:0.6951724\t  V_val_loss:0.6821715\t  V_star_val_loss:0.6821717\t  improvement:0.0000001\t w_top1:51.0416667\t  w_top5:51.0416667\t v_top1:49.2187500\t v_top5:49.2187500\t \n",
      "07/03 08:17:39 AM |\t  weight:tensor([0.9881, 0.9933, 0.9981, 1.0258, 1.0140, 1.0082, 0.9938, 1.0218, 0.9839,\n",
      "        1.0021, 1.0017, 0.9924, 1.0010, 1.0128, 0.9871, 0.9758],\n",
      "       device='cuda:0')\n",
      "07/03 08:17:39 AM |\t  noise:1.0053954124450684 mean:1.0000001192092896 max: 1.0257549285888672 min: 0.975838303565979\n",
      "07/03 08:18:03 AM |\t   57.4%:\t  W_train_loss:0.7031913\tV_train_syn_loss:0.6907141\tV_train_loss:0.7119855\t  V_val_loss:0.7052593\t  V_star_val_loss:0.7052594\t  improvement:0.0000001\t w_top1:48.4375000\t  w_top5:48.4375000\t v_top1:48.9583333\t v_top5:48.9583333\t \n",
      "07/03 08:18:03 AM |\t  weight:tensor([1.0296, 0.9745, 1.0214, 0.9824, 0.9862, 0.9925, 0.9965, 0.9802, 1.0057,\n",
      "        0.9934, 1.0253, 1.0059, 0.9818, 1.0122, 1.0163, 0.9962],\n",
      "       device='cuda:0')\n",
      "07/03 08:18:03 AM |\t  noise:0.9953979849815369 mean:1.0 max: 1.0296028852462769 min: 0.974460244178772\n",
      "07/03 08:18:27 AM |\t   77.0%:\t  W_train_loss:0.6875080\tV_train_syn_loss:0.6935042\tV_train_loss:0.6806058\t  V_val_loss:0.6940710\t  V_star_val_loss:0.6940710\t  improvement:0.0000000\t w_top1:58.8541667\t  w_top5:58.8541667\t v_top1:51.4322917\t v_top5:51.4322917\t \n",
      "07/03 08:18:27 AM |\t  weight:tensor([1.0023, 1.0063, 1.0006, 0.9996, 0.9791, 0.9753, 0.9847, 1.0320, 0.9891,\n",
      "        0.9844, 1.0229, 1.0093, 1.0003, 1.0021, 1.0120, 0.9999],\n",
      "       device='cuda:0')\n",
      "07/03 08:18:27 AM |\t  noise:0.9974923133850098 mean:1.0000001192092896 max: 1.0319674015045166 min: 0.9753178954124451\n",
      "07/03 08:18:52 AM |\t   96.7%:\t  W_train_loss:0.6963798\tV_train_syn_loss:0.6898904\tV_train_loss:0.6935654\t  V_val_loss:0.6952193\t  V_star_val_loss:0.6952193\t  improvement:0.0000000\t w_top1:51.0416667\t  w_top5:51.0416667\t v_top1:51.3541667\t v_top5:51.3541667\t \n",
      "07/03 08:18:52 AM |\t  weight:tensor([1.0084, 0.9910, 1.0164, 0.9940, 0.9870, 1.0131, 0.9789, 1.0045, 0.9878,\n",
      "        0.9959, 0.9915, 0.9955, 1.0062, 1.0144, 0.9951, 1.0202],\n",
      "       device='cuda:0')\n",
      "07/03 08:18:52 AM |\t  noise:0.9991690516471863 mean:1.0 max: 1.020227313041687 min: 0.9789247512817383\n",
      "07/03 08:18:56 AM |\t  improvment:0.00040608644485473633\n",
      "07/03 08:18:56 AM |\t  w_train_loss:43.567186534404755\n",
      "07/03 08:18:56 AM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:19:17 AM |\t   14.8%:\t  W_train_loss:0.6949557\tV_train_syn_loss:0.6900772\tV_train_loss:0.6953081\t  V_val_loss:0.6962414\t  V_star_val_loss:0.6962414\t  improvement:-0.0000000\t w_top1:50.0000000\t  w_top5:50.0000000\t v_top1:50.0000000\t v_top5:50.0000000\t \n",
      "07/03 08:19:17 AM |\t  weight:tensor([1.0126, 0.9814, 1.0207, 0.9944, 0.9779, 0.9858, 0.9869, 1.0154, 1.0109,\n",
      "        1.0074, 0.9965, 1.0154, 0.9793, 0.9975, 1.0086, 1.0092],\n",
      "       device='cuda:0')\n",
      "07/03 08:19:17 AM |\t  noise:0.9968880414962769 mean:1.0 max: 1.02065908908844 min: 0.9778725504875183\n",
      "07/03 08:19:41 AM |\t   34.4%:\t  W_train_loss:0.6939826\tV_train_syn_loss:0.6925495\tV_train_loss:0.6968948\t  V_val_loss:0.6918723\t  V_star_val_loss:0.6918723\t  improvement:-0.0000000\t w_top1:49.4791667\t  w_top5:49.4791667\t v_top1:49.7159091\t v_top5:49.7159091\t \n",
      "07/03 08:19:42 AM |\t  weight:tensor([1.0036, 1.0016, 1.0030, 1.0156, 0.9727, 1.0140, 1.0189, 0.9968, 1.0013,\n",
      "        0.9927, 0.9907, 0.9879, 0.9917, 0.9989, 0.9900, 1.0206],\n",
      "       device='cuda:0')\n",
      "07/03 08:19:42 AM |\t  noise:1.0032639503479004 mean:1.0 max: 1.0206414461135864 min: 0.9727101922035217\n",
      "07/03 08:20:06 AM |\t   54.1%:\t  W_train_loss:0.6925502\tV_train_syn_loss:0.6938055\tV_train_loss:0.6968216\t  V_val_loss:0.6899964\t  V_star_val_loss:0.6899964\t  improvement:-0.0000000\t w_top1:54.1666667\t  w_top5:54.1666667\t v_top1:48.8970588\t v_top5:48.8970588\t \n",
      "07/03 08:20:06 AM |\t  weight:tensor([0.9979, 1.0122, 1.0030, 0.9934, 1.0097, 0.9871, 1.0073, 1.0053, 0.9875,\n",
      "        1.0070, 0.9827, 1.0031, 0.9865, 0.9979, 0.9936, 1.0258],\n",
      "       device='cuda:0')\n",
      "07/03 08:20:06 AM |\t  noise:1.0019824504852295 mean:1.0 max: 1.0258358716964722 min: 0.9826502203941345\n",
      "07/03 08:20:30 AM |\t   73.8%:\t  W_train_loss:0.6893822\tV_train_syn_loss:0.6934114\tV_train_loss:0.6897666\t  V_val_loss:0.6904977\t  V_star_val_loss:0.6904976\t  improvement:-0.0000000\t w_top1:60.4166667\t  w_top5:60.4166667\t v_top1:52.1739130\t v_top5:52.1739130\t \n",
      "07/03 08:20:30 AM |\t  weight:tensor([0.9945, 1.0197, 0.9886, 0.9780, 1.0059, 1.0088, 1.0025, 1.0033, 0.9962,\n",
      "        0.9822, 1.0121, 0.9965, 0.9960, 0.9910, 1.0077, 1.0169],\n",
      "       device='cuda:0')\n",
      "07/03 08:20:30 AM |\t  noise:1.0001670122146606 mean:1.0 max: 1.0196952819824219 min: 0.9780303239822388\n",
      "07/03 08:20:55 AM |\t   93.4%:\t  W_train_loss:0.6892148\tV_train_syn_loss:0.6927059\tV_train_loss:0.6927849\t  V_val_loss:0.6925817\t  V_star_val_loss:0.6925817\t  improvement:0.0000000\t w_top1:52.6041667\t  w_top5:52.6041667\t v_top1:52.0474138\t v_top5:52.0474138\t \n",
      "07/03 08:20:55 AM |\t  weight:tensor([0.9840, 1.0083, 1.0014, 1.0001, 0.9931, 1.0026, 1.0251, 0.9846, 1.0152,\n",
      "        1.0028, 1.0126, 1.0002, 0.9940, 0.9870, 0.9909, 0.9978],\n",
      "       device='cuda:0')\n",
      "07/03 08:20:55 AM |\t  noise:0.9999200105667114 mean:1.0 max: 1.0251291990280151 min: 0.9840221405029297\n",
      "07/03 08:21:13 AM |\t  teacher test loss : 0.181284\n",
      "07/03 08:21:13 AM |\t  teacher top1 : 56.560497\n",
      "07/03 08:21:13 AM |\t  teacher top5 : 56.560497\n",
      "07/03 08:21:13 AM |\t  teacher test loss : 0.181284\n",
      "07/03 08:21:23 AM |\t  student test loss : 0.181284\n",
      "07/03 08:21:23 AM |\t  student top1 : 56.560497\n",
      "07/03 08:21:23 AM |\t  student top5 : 56.560497\n",
      "07/03 08:21:23 AM |\t  student test loss : 0.181284\n",
      "07/03 08:21:23 AM |\t  find a better model\n",
      "07/03 08:21:27 AM |\t  improvment:-2.980232238769531e-07\n",
      "07/03 08:21:27 AM |\t  w_train_loss:42.93473505973816\n",
      "07/03 08:21:27 AM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:21:44 AM |\t   11.5%:\t  W_train_loss:0.6903922\tV_train_syn_loss:0.6913682\tV_train_loss:0.6943008\t  V_val_loss:0.6831307\t  V_star_val_loss:0.6831308\t  improvement:0.0000000\t w_top1:50.0000000\t  w_top5:50.0000000\t v_top1:50.0000000\t v_top5:50.0000000\t \n",
      "07/03 08:21:44 AM |\t  weight:tensor([1.0025, 0.9831, 0.9906, 1.0083, 1.0029, 0.9907, 1.0145, 0.9860, 0.9976,\n",
      "        0.9959, 1.0004, 1.0091, 1.0045, 0.9929, 1.0079, 1.0133],\n",
      "       device='cuda:0')\n",
      "07/03 08:21:44 AM |\t  noise:0.9973039031028748 mean:1.0 max: 1.014545202255249 min: 0.9830541014671326\n",
      "07/03 08:22:08 AM |\t   31.1%:\t  W_train_loss:0.6907564\tV_train_syn_loss:0.6928095\tV_train_loss:0.6966534\t  V_val_loss:0.6887223\t  V_star_val_loss:0.6887223\t  improvement:0.0000000\t w_top1:50.5208333\t  w_top5:50.5208333\t v_top1:49.3750000\t v_top5:49.3750000\t \n",
      "07/03 08:22:08 AM |\t  weight:tensor([1.0099, 1.0122, 0.9995, 0.9978, 1.0096, 0.9990, 0.9832, 0.9928, 1.0113,\n",
      "        0.9795, 1.0003, 1.0119, 0.9785, 0.9995, 0.9915, 1.0235],\n",
      "       device='cuda:0')\n",
      "07/03 08:22:08 AM |\t  noise:1.0004956722259521 mean:0.9999998807907104 max: 1.023495078086853 min: 0.9785205721855164\n",
      "07/03 08:22:33 AM |\t   50.8%:\t  W_train_loss:0.6875007\tV_train_syn_loss:0.6941115\tV_train_loss:0.6948361\t  V_val_loss:0.6867187\t  V_star_val_loss:0.6867187\t  improvement:-0.0000000\t w_top1:58.8541667\t  w_top5:58.8541667\t v_top1:49.2187500\t v_top5:49.2187500\t \n",
      "07/03 08:22:33 AM |\t  weight:tensor([0.9679, 1.0054, 0.9974, 1.0135, 0.9934, 1.0072, 1.0009, 0.9862, 1.0030,\n",
      "        1.0095, 0.9818, 0.9981, 0.9893, 1.0204, 0.9986, 1.0275],\n",
      "       device='cuda:0')\n",
      "07/03 08:22:33 AM |\t  noise:0.9964803457260132 mean:1.0 max: 1.0274659395217896 min: 0.9678753018379211\n",
      "07/03 08:22:58 AM |\t   70.5%:\t  W_train_loss:0.6841182\tV_train_syn_loss:0.6934696\tV_train_loss:0.6932628\t  V_val_loss:0.6925576\t  V_star_val_loss:0.6925576\t  improvement:0.0000000\t w_top1:61.9791667\t  w_top5:61.9791667\t v_top1:48.8636364\t v_top5:48.8636364\t \n",
      "07/03 08:22:58 AM |\t  weight:tensor([1.0009, 0.9916, 0.9816, 0.9925, 1.0155, 0.9921, 1.0083, 0.9789, 0.9949,\n",
      "        1.0047, 1.0118, 1.0165, 1.0162, 0.9785, 1.0002, 1.0159],\n",
      "       device='cuda:0')\n",
      "07/03 08:22:58 AM |\t  noise:0.9951714277267456 mean:1.0 max: 1.0164538621902466 min: 0.9785115122795105\n",
      "07/03 08:23:22 AM |\t   90.2%:\t  W_train_loss:0.6819806\tV_train_syn_loss:0.6933672\tV_train_loss:0.6935773\t  V_val_loss:0.6923918\t  V_star_val_loss:0.6923918\t  improvement:0.0000000\t w_top1:56.2500000\t  w_top5:56.2500000\t v_top1:48.5491071\t v_top5:48.5491071\t \n",
      "07/03 08:23:22 AM |\t  weight:tensor([0.9985, 0.9948, 0.9928, 1.0048, 0.9918, 0.9981, 0.9856, 0.9937, 1.0162,\n",
      "        1.0011, 0.9945, 1.0119, 1.0133, 0.9916, 1.0016, 1.0098],\n",
      "       device='cuda:0')\n",
      "07/03 08:23:22 AM |\t  noise:0.9950084090232849 mean:0.9999998807907104 max: 1.0162498950958252 min: 0.9856147766113281\n",
      "07/03 08:23:34 AM |\t  improvment:5.364418029785156e-07\n",
      "07/03 08:23:34 AM |\t  w_train_loss:42.588342130184174\n",
      "07/03 08:23:34 AM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:23:47 AM |\t    8.2%:\t  W_train_loss:0.6774779\tV_train_syn_loss:0.6920874\tV_train_loss:0.6911710\t  V_val_loss:0.7005642\t  V_star_val_loss:0.7005642\t  improvement:-0.0000000\t w_top1:59.3750000\t  w_top5:59.3750000\t v_top1:53.1250000\t v_top5:53.1250000\t \n",
      "07/03 08:23:47 AM |\t  weight:tensor([0.9931, 0.9910, 0.9870, 0.9940, 0.9924, 1.0033, 1.0200, 1.0369, 0.9901,\n",
      "        0.9967, 1.0005, 1.0093, 1.0039, 0.9953, 0.9972, 0.9891],\n",
      "       device='cuda:0')\n",
      "07/03 08:23:47 AM |\t  noise:1.0022281408309937 mean:0.9999999403953552 max: 1.0368764400482178 min: 0.9870491027832031\n",
      "07/03 08:24:11 AM |\t   27.9%:\t  W_train_loss:0.6807587\tV_train_syn_loss:0.6944428\tV_train_loss:0.6980898\t  V_val_loss:0.6835649\t  V_star_val_loss:0.6835649\t  improvement:0.0000000\t w_top1:61.9791667\t  w_top5:61.9791667\t v_top1:48.6111111\t v_top5:48.6111111\t \n",
      "07/03 08:24:11 AM |\t  weight:tensor([0.9963, 1.0257, 1.0116, 1.0074, 0.9931, 1.0011, 1.0157, 1.0121, 0.9832,\n",
      "        0.9944, 1.0045, 0.9809, 0.9696, 0.9872, 1.0018, 1.0151],\n",
      "       device='cuda:0')\n",
      "07/03 08:24:11 AM |\t  noise:1.007896900177002 mean:1.0 max: 1.0257409811019897 min: 0.9695691466331482\n",
      "07/03 08:24:35 AM |\t   47.5%:\t  W_train_loss:0.6726521\tV_train_syn_loss:0.6937532\tV_train_loss:0.6927513\t  V_val_loss:0.6952408\t  V_star_val_loss:0.6952408\t  improvement:-0.0000000\t w_top1:59.8958333\t  w_top5:59.8958333\t v_top1:49.1666667\t v_top5:49.1666667\t \n",
      "07/03 08:24:35 AM |\t  weight:tensor([1.0087, 1.0141, 1.0376, 0.9829, 0.9861, 0.9864, 0.9999, 0.9862, 0.9829,\n",
      "        1.0016, 0.9869, 0.9965, 1.0189, 1.0104, 0.9845, 1.0162],\n",
      "       device='cuda:0')\n",
      "07/03 08:24:35 AM |\t  noise:1.0002620220184326 mean:0.9999998807907104 max: 1.0375993251800537 min: 0.9828962087631226\n",
      "07/03 08:25:00 AM |\t   67.2%:\t  W_train_loss:0.6604947\tV_train_syn_loss:0.6911447\tV_train_loss:0.6980985\t  V_val_loss:0.6957411\t  V_star_val_loss:0.6957410\t  improvement:-0.0000000\t w_top1:67.1875000\t  w_top5:67.1875000\t v_top1:48.5119048\t v_top5:48.5119048\t \n",
      "07/03 08:25:00 AM |\t  weight:tensor([0.9759, 0.9953, 0.9966, 0.9811, 1.0132, 1.0126, 0.9801, 1.0072, 1.0106,\n",
      "        1.0311, 1.0140, 1.0085, 1.0115, 1.0074, 0.9687, 0.9863],\n",
      "       device='cuda:0')\n",
      "07/03 08:25:00 AM |\t  noise:0.9952497482299805 mean:1.0 max: 1.0311273336410522 min: 0.9686768651008606\n",
      "07/03 08:25:24 AM |\t   86.9%:\t  W_train_loss:0.6422836\tV_train_syn_loss:0.6964450\tV_train_loss:0.7023687\t  V_val_loss:0.6920911\t  V_star_val_loss:0.6920910\t  improvement:-0.0000000\t w_top1:70.3125000\t  w_top5:70.3125000\t v_top1:47.2222222\t v_top5:47.2222222\t \n",
      "07/03 08:25:24 AM |\t  weight:tensor([0.9981, 1.0296, 0.9920, 0.9863, 1.0083, 1.0040, 0.9796, 0.9997, 0.9894,\n",
      "        1.0143, 0.9874, 0.9990, 0.9742, 1.0216, 1.0112, 1.0053],\n",
      "       device='cuda:0')\n",
      "07/03 08:25:24 AM |\t  noise:0.9997062683105469 mean:0.9999998807907104 max: 1.0296413898468018 min: 0.9741787314414978\n",
      "07/03 08:25:50 AM |\t  teacher test loss : 0.155120\n",
      "07/03 08:25:50 AM |\t  teacher top1 : 48.397436\n",
      "07/03 08:25:50 AM |\t  teacher top5 : 48.397436\n",
      "07/03 08:25:50 AM |\t  teacher test loss : 0.155120\n",
      "07/03 08:26:00 AM |\t  student test loss : 0.142793\n",
      "07/03 08:26:00 AM |\t  student top1 : 44.551282\n",
      "07/03 08:26:00 AM |\t  student top5 : 44.551282\n",
      "07/03 08:26:00 AM |\t  student test loss : 0.142793\n",
      "07/03 08:26:00 AM |\t  improvment:-2.384185791015625e-07\n",
      "07/03 08:26:00 AM |\t  w_train_loss:41.235826432704926\n",
      "07/03 08:26:00 AM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:26:09 AM |\t   4.92%:\t  W_train_loss:0.6246769\tV_train_syn_loss:0.6926668\tV_train_loss:0.6917759\t  V_val_loss:0.6922738\t  V_star_val_loss:0.6922738\t  improvement:-0.0000000\t w_top1:70.3125000\t  w_top5:70.3125000\t v_top1:46.8750000\t v_top5:46.8750000\t \n",
      "07/03 08:26:09 AM |\t  weight:tensor([1.0193, 0.9807, 0.9970, 0.9963, 1.0271, 1.0053, 0.9872, 0.9728, 0.9926,\n",
      "        1.0152, 1.0114, 1.0108, 0.9799, 1.0026, 0.9877, 1.0141],\n",
      "       device='cuda:0')\n",
      "07/03 08:26:09 AM |\t  noise:0.9982131719589233 mean:1.0 max: 1.0270943641662598 min: 0.9728173017501831\n",
      "07/03 08:26:33 AM |\t   24.6%:\t  W_train_loss:0.6342995\tV_train_syn_loss:0.6914110\tV_train_loss:0.6938649\t  V_val_loss:0.6975333\t  V_star_val_loss:0.6975331\t  improvement:-0.0000001\t w_top1:67.1875000\t  w_top5:67.1875000\t v_top1:47.6562500\t v_top5:47.6562500\t \n",
      "07/03 08:26:33 AM |\t  weight:tensor([1.0330, 1.0074, 0.9817, 1.0164, 1.0003, 0.9932, 1.0373, 0.9763, 0.9920,\n",
      "        0.9528, 1.0403, 1.0187, 0.9864, 1.0259, 0.9603, 0.9779],\n",
      "       device='cuda:0')\n",
      "07/03 08:26:33 AM |\t  noise:1.0056923627853394 mean:0.9999999403953552 max: 1.0402662754058838 min: 0.9528481364250183\n",
      "07/03 08:26:57 AM |\t   44.3%:\t  W_train_loss:0.5921628\tV_train_syn_loss:0.6901526\tV_train_loss:0.6926861\t  V_val_loss:0.6959210\t  V_star_val_loss:0.6959210\t  improvement:0.0000000\t w_top1:72.9166667\t  w_top5:72.9166667\t v_top1:49.1071429\t v_top5:49.1071429\t \n",
      "07/03 08:26:58 AM |\t  weight:tensor([1.0590, 1.0489, 0.9955, 1.0360, 0.9649, 0.9642, 1.0216, 0.9968, 1.0176,\n",
      "        0.9539, 0.9734, 0.9902, 1.0097, 0.9866, 0.9609, 1.0209],\n",
      "       device='cuda:0')\n",
      "07/03 08:26:58 AM |\t  noise:1.0108566284179688 mean:1.0000001192092896 max: 1.0589642524719238 min: 0.9539024233818054\n",
      "07/03 08:27:23 AM |\t   63.9%:\t  W_train_loss:0.5261996\tV_train_syn_loss:0.6844541\tV_train_loss:0.7001943\t  V_val_loss:0.7252593\t  V_star_val_loss:0.7252607\t  improvement:0.0000014\t w_top1:80.7291667\t  w_top5:80.7291667\t v_top1:48.7500000\t v_top5:48.7500000\t \n",
      "07/03 08:27:23 AM |\t  weight:tensor([1.0887, 0.9639, 1.0193, 0.9989, 0.9796, 0.9433, 1.1369, 1.0928, 0.9384,\n",
      "        1.0220, 0.9124, 0.9701, 1.0142, 1.0339, 0.9182, 0.9672],\n",
      "       device='cuda:0')\n",
      "07/03 08:27:23 AM |\t  noise:1.027941346168518 mean:0.9999999403953552 max: 1.1368862390518188 min: 0.9124413132667542\n",
      "07/03 08:27:47 AM |\t   83.6%:\t  W_train_loss:0.4876794\tV_train_syn_loss:0.7027852\tV_train_loss:0.7098038\t  V_val_loss:0.7036244\t  V_star_val_loss:0.7036268\t  improvement:0.0000025\t w_top1:78.6458333\t  w_top5:78.6458333\t v_top1:47.1153846\t v_top5:47.1153846\t \n",
      "07/03 08:27:47 AM |\t  weight:tensor([0.9760, 0.9282, 1.0110, 0.9827, 0.9391, 1.0007, 0.9229, 1.0185, 0.9863,\n",
      "        1.0281, 1.0628, 1.0771, 1.0153, 0.9479, 1.0685, 1.0349],\n",
      "       device='cuda:0')\n",
      "07/03 08:27:47 AM |\t  noise:0.9723958969116211 mean:0.9999998807907104 max: 1.0770939588546753 min: 0.9229366183280945\n",
      "07/03 08:28:08 AM |\t  improvment:5.27501106262207e-05\n",
      "07/03 08:28:08 AM |\t  w_train_loss:34.19417816400528\n",
      "07/03 08:28:08 AM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n",
      "07/03 08:28:12 AM |\t   1.64%:\t  W_train_loss:0.3909777\tV_train_syn_loss:0.6921696\tV_train_loss:0.6912259\t  V_val_loss:0.6775921\t  V_star_val_loss:0.6775942\t  improvement:0.0000021\t w_top1:90.6250000\t  w_top5:90.6250000\t v_top1:56.2500000\t v_top5:56.2500000\t \n",
      "07/03 08:28:12 AM |\t  weight:tensor([0.9923, 1.0374, 1.0067, 0.9184, 1.0432, 0.9580, 0.9702, 0.8412, 0.9651,\n",
      "        1.1030, 1.0791, 1.0669, 0.9999, 1.1147, 0.9341, 0.9698],\n",
      "       device='cuda:0')\n",
      "07/03 08:28:12 AM |\t  noise:0.970923125743866 mean:1.0 max: 1.1146974563598633 min: 0.8412068486213684\n",
      "07/03 08:28:36 AM |\t   21.3%:\t  W_train_loss:0.4299479\tV_train_syn_loss:0.6937656\tV_train_loss:0.6944574\t  V_val_loss:0.7017332\t  V_star_val_loss:0.7017357\t  improvement:0.0000026\t w_top1:78.6458333\t  w_top5:78.6458333\t v_top1:49.5535714\t v_top5:49.5535714\t \n",
      "07/03 08:28:36 AM |\t  weight:tensor([1.0000, 0.9999, 1.0001, 1.0000, 1.0002, 0.9999, 0.9998, 1.0000, 1.0001,\n",
      "        1.0000, 1.0000, 0.9999, 1.0001, 0.9999, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "07/03 08:28:36 AM |\t  noise:0.9999977350234985 mean:0.9999999403953552 max: 1.0001575946807861 min: 0.9998272061347961\n",
      "07/03 08:29:01 AM |\t   41.0%:\t  W_train_loss:0.3764222\tV_train_syn_loss:0.6895329\tV_train_loss:0.6965874\t  V_val_loss:0.7082278\t  V_star_val_loss:0.7082278\t  improvement:-0.0000000\t w_top1:84.3750000\t  w_top5:84.3750000\t v_top1:48.5576923\t v_top5:48.5576923\t \n",
      "07/03 08:29:01 AM |\t  weight:tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0001,\n",
      "        1.0000, 1.0001, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "07/03 08:29:01 AM |\t  noise:0.9999856948852539 mean:1.0 max: 1.000071406364441 min: 0.9999129176139832\n",
      "07/03 08:29:25 AM |\t   60.7%:\t  W_train_loss:0.3239829\tV_train_syn_loss:0.6891311\tV_train_loss:0.6937121\t  V_val_loss:0.7051126\t  V_star_val_loss:0.7051128\t  improvement:0.0000001\t w_top1:90.1041667\t  w_top5:90.1041667\t v_top1:49.1776316\t v_top5:49.1776316\t \n",
      "07/03 08:29:25 AM |\t  weight:tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0001, 1.0000, 1.0001, 1.0000, 1.0001],\n",
      "       device='cuda:0')\n",
      "07/03 08:29:25 AM |\t  noise:0.9999763369560242 mean:1.0 max: 1.0001001358032227 min: 0.9999123811721802\n",
      "07/03 08:29:49 AM |\t   80.3%:\t  W_train_loss:0.2716267\tV_train_syn_loss:0.6919688\tV_train_loss:0.7014702\t  V_val_loss:0.7014685\t  V_star_val_loss:0.7014684\t  improvement:-0.0000000\t w_top1:93.2291667\t  w_top5:93.2291667\t v_top1:47.6250000\t v_top5:47.6250000\t \n",
      "07/03 08:29:49 AM |\t  weight:tensor([1.0001, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0001, 1.0000,\n",
      "        1.0000, 1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "07/03 08:29:49 AM |\t  noise:1.0000088214874268 mean:1.0000001192092896 max: 1.000085473060608 min: 0.9999259114265442\n",
      "07/03 08:30:24 AM |\t  teacher test loss : 0.151557\n",
      "07/03 08:30:24 AM |\t  teacher top1 : 47.285657\n",
      "07/03 08:30:24 AM |\t  teacher top5 : 47.285657\n",
      "07/03 08:30:24 AM |\t  teacher test loss : 0.151557\n",
      "07/03 08:30:34 AM |\t  student test loss : 0.161990\n",
      "07/03 08:30:34 AM |\t  student top1 : 50.540865\n",
      "07/03 08:30:34 AM |\t  student top5 : 50.540865\n",
      "07/03 08:30:34 AM |\t  student test loss : 0.161990\n",
      "07/03 08:30:34 AM |\t  1e+02%:\t  W_train_loss:0.2635422\tV_train_syn_loss:0.6954726\tV_train_loss:0.6971382\t  V_val_loss:0.7023580\t  V_star_val_loss:0.7023579\t  improvement:-0.0000001\t w_top1:90.6250000\t  w_top5:90.6250000\t v_top1:48.1854839\t v_top5:48.1854839\t \n",
      "07/03 08:30:34 AM |\t  weight:tensor([1.0001, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0001, 1.0001, 1.0000, 1.0001, 1.0000],\n",
      "       device='cuda:0')\n",
      "07/03 08:30:34 AM |\t  noise:0.9999905824661255 mean:1.0 max: 1.0001262426376343 min: 0.9999355673789978\n",
      "07/03 08:30:34 AM |\t  improvment:3.421306610107422e-05\n",
      "07/03 08:30:34 AM |\t  w_train_loss:20.768219344317913\n",
      "07/03 08:30:34 AM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:2e-06,\t\tlr_v:2e-06,\t\tlr_A:0.01----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-690bfba752be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     w_train_loss,v_accu = my_train(epoch, train_w_dataloader,train_syn_dataloader,train_A_dataloader, valid_dataloader, model_w,\n\u001b[0m\u001b[1;32m     16\u001b[0m                             model_v,  architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter,v_accu)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fff999bd88f6>\u001b[0m in \u001b[0;36mmy_train\u001b[0;34m(epoch, wdataloader, syndataloader, Adataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter, past_v_accu)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mepsilon_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munrolled_w_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mepsilon_v\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munrolled_v_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, w_optimizer,\n\u001b[0m\u001b[1;32m     70\u001b[0m                                              \u001b[0minput_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_v_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                              \u001b[0minput_A_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_A_v_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_A_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/architect.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, input_w, output_w, input_w_attn, w_optimizer, input_v, input_v_attn, output_v, input_syn, input_syn_attn, input_A_v, input_A_v_attn, output_A_v, attn_idx, v_optimizer, lr_w, lr_v, clip)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mvector_s_dash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munrolled_v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         implicit_grads_A = self._outer_A(vector_s_dash, input_w, output_w, input_w_attn,\n\u001b[0m\u001b[1;32m    186\u001b[0m                                          input_v, input_v_attn,  attn_idx,  unrolled_w_model, lr_w, lr_v)\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/architect.py\u001b[0m in \u001b[0;36m_outer_A\u001b[0;34m(self, vector_s_dash, w_input, w_target, w_input_attn, input_v, input_v_attn, attn_idx, unrolled_w_model, eta_w, eta_v, r)\u001b[0m\n\u001b[1;32m    249\u001b[0m             loss_aug_m, unrolled_w_model.parameters(), retain_graph=True)\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         grad_part2 = self._hessian_vector_product_A(\n\u001b[0m\u001b[1;32m    252\u001b[0m             vector_dash, w_input, w_target, w_input_attn,attn_idx)\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/architect.py\u001b[0m in \u001b[0;36m_hessian_vector_product_A\u001b[0;34m(self, vector, input, target, input_attn, attn_idx, r)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         _,loss = CTG_loss(input, input_attn, target, \n\u001b[0m\u001b[1;32m    208\u001b[0m                            self.A,attn_idx, self.w_model)\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/losses.py\u001b[0m in \u001b[0;36mCTG_loss\u001b[0;34m(input_ids, input_attn, target_ids, attention_parameters, attn_idx, model)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# logging.info(f\"attentionweight:{attention_weights}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     logits,loss_vec = model.get_loss_vec(\n\u001b[0m\u001b[1;32m     34\u001b[0m         input_ids,input_attn, target_ids)\n\u001b[1;32m     35\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/model.py\u001b[0m in \u001b[0;36mget_loss_vec\u001b[0;34m(self, input_ids, input_attn, target)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mloss_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, input_attn)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0minp_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         )\n\u001b[0;32m--> 848\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin == 1):\n",
    "#     my_test(valid_dataloader, model_w, -1)  # before train\n",
    "#     my_test(valid_dataloader, model_v, -1)\n",
    "\n",
    "tot_iter = [0]\n",
    "v_accu = 0\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(\n",
    "        f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss,v_accu = my_train(epoch, train_w_dataloader,train_syn_dataloader,train_A_dataloader, valid_dataloader, model_w,\n",
    "                            model_v,  architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter,v_accu)\n",
    "\n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "    architect.scheduler_A.step()\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss}\")\n",
    "\n",
    "\n",
    "torch.save(model_v, './model/'+now+'model_w.pt')\n",
    "torch.save(model_v, './model/'+now+'model_v.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,4).unsqueeze(-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
