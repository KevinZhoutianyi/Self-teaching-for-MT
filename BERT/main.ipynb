{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import  AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = -1, help='validation data number')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default = 4000, help='train data number')#80*x\n",
    "parser.add_argument('--train_A_num_points', type=int,           default = 2000, help='train data number')\n",
    "parser.add_argument('--unlabel_num_points', type=int,           default = 2000, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = -1, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=32,     help='Batch size for test and validation')\n",
    "\n",
    "parser.add_argument('--w_bs', type=int,                         default=16,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--syn_bs', type=int,                       default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "# parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--A_bs', type=int,                         default=8,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_de2en', type=str,             default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='Yelp',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=-1,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=-1,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=10,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "# parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=100 ,   help='learning rate for A')\n",
    "# parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "# parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.999,    help='momentum')\n",
    "# parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=10,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=1,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "parser.add_argument('--freeze', type=int,                       default=0,    help='whether freeze the pretrained encoder')\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "parser.add_argument('--attack', type=int,                       default=0 ,     help='whether att')\n",
    "parser.add_argument('--clean_A_data', type=int,                 default=1 ,     help='whether att')\n",
    "parser.add_argument('--clean_w_data', type=int,                 default=1 ,     help='whether att')\n",
    "\n",
    "\n",
    "parser.add_argument('--load_A', type=int,                       default=0 ,     help='whether att')\n",
    "parser.add_argument('--A_path', type=str,                       default='/tianyi-vol/Self-teaching-for-machine-translation/BERT/trainedA/A.pt' ,     help='whether att')\n",
    "\n",
    "# parser.add_argument('--embedding_dim', type=int,                default=300 ,     help='whether train A')\n",
    "parser.add_argument('--out_dim', type=int,                      default=2 ,     help='whether train A')\n",
    "# parser.add_argument('--hidden_size', type=int,                  default=64 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "\n",
    "args.test_num = args.train_w_num_points #TODO: test each epoch\n",
    "args.rep_num = (args.train_w_num_points//5)//args.w_bs * args.w_bs#TODO: test each epoch\n",
    "\n",
    "args.train_w_num_points= args.train_w_num_points//args.w_bs * args.w_bs\n",
    "args.train_A_num_points= args.train_A_num_points//args.A_bs * args.A_bs\n",
    "args.unlabel_num_points= args.unlabel_num_points//args.syn_bs * args.syn_bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tianyi-vol/Self-teaching-for-machine-translation/BERT/wandb/run-20220709_032551-x5rj13rk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/x5rj13rk\" target=\"_blank\">Yelp</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/x5rj13rk?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe9fb4a8820>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY'] = 'a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME'] = args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "wandb.init(project=\"Selftraining\", config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:25:59 AM |\t  Using custom data configuration default-89b305f2de5c2a24\n",
      "07/09 03:25:59 AM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-89b305f2de5c2a24/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 842.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:25:59 AM |\t  Using custom data configuration default-9e3205a4e9940313\n",
      "07/09 03:25:59 AM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-9e3205a4e9940313/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1071.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:00 AM |\t  Using custom data configuration default-9c4aa945ceecc93d\n",
      "07/09 03:26:00 AM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-9c4aa945ceecc93d/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 945.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:00 AM |\t  Using custom data configuration default-99489eb87fbc339f\n",
      "07/09 03:26:00 AM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-99489eb87fbc339f/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:00 AM |\t  Using custom data configuration default-43e18063088a1046\n",
      "07/09 03:26:00 AM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-43e18063088a1046/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1073.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "l = ['dev','test','train','unlabeled']\n",
    "dev = load_dataset('json', data_files='/tianyi-vol/yelp/dev_data.json', field='data')\n",
    "test = load_dataset('json', data_files='/tianyi-vol/yelp/test_data.json', field='data')\n",
    "train = load_dataset('json', data_files='/tianyi-vol/yelp/train_data.json', field='data')\n",
    "unlabeled = load_dataset('json', data_files='/tianyi-vol/yelp/unlabeled_data.json', field='data')\n",
    "unlabeled_diff = load_dataset('json', data_files='/tianyi-vol/IMDB/unlabeled_data.json', field='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 25165\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 5235\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 2485\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train,dev,unlabeled,unlabeled_diff,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:13 AM |\t  modelsize:124.697433MB\n",
      "07/09 03:26:23 AM |\t  modelsize:124.697433MB\n",
      "07/09 03:26:32 AM |\t  modelsize:124.697433MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "modelname = args.model_name_teacher\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_de2en\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:32 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-9c4aa945ceecc93d/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-a565eec58ed7e408.arrow\n",
      "07/09 03:26:32 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-89b305f2de5c2a24/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-5967255ee27fdb41.arrow\n",
      "07/09 03:26:32 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-99489eb87fbc339f/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-5706d4fed9149c74.arrow\n",
      "07/09 03:26:32 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-43e18063088a1046/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-caded9d1ed4e186f.arrow\n",
      "07/09 03:26:32 AM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-9e3205a4e9940313/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-7e682076f91c470f.arrow\n",
      "07/09 03:26:32 AM |\t  train len: 6000\n",
      "07/09 03:26:32 AM |\t  train_w_num_points_len and train_v_num_points_len: 4000\n",
      "07/09 03:26:32 AM |\t  train_v_synthetic_num_points_len: 2000\n",
      "07/09 03:26:32 AM |\t  train_A_num_points_len: 2000\n",
      "07/09 03:26:33 AM |\t  valid len: 3800\n",
      "07/09 03:26:33 AM |\t  test len: 3800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train =train[\"train\"].shuffle(seed=seed_).select(range(args.train_w_num_points+args.train_A_num_points)) # A and W)\n",
    "valid = dev[\"train\"].shuffle(seed=seed_)#.select(range( r(args.valid_num_points, args.batch_size))) # dev\n",
    "unlabeled = unlabeled[\"train\"].shuffle(seed=seed_).select(range( args.unlabel_num_points) )# dev\n",
    "unlabeled_diff = unlabeled_diff[\"train\"].shuffle(seed=seed_).select(range( args.unlabel_num_points) )# dev\n",
    "test = test[\"train\"].shuffle(seed=seed_)#.select(range( r(args.valid_num_points, args.batch_size))) # dev # test\n",
    "\n",
    "logging.info(\"train len: %d\", len(train))\n",
    "\n",
    "train_w_num_points_len = args.train_w_num_points\n",
    "\n",
    "\n",
    "\n",
    "train_v_synthetic_num_points_len = args.unlabel_num_points\n",
    "train_A_num_points_len =  args.train_A_num_points\n",
    "\n",
    "logging.info(\"train_w_num_points_len and train_v_num_points_len: %d\", train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",\n",
    "             train_v_synthetic_num_points_len)\n",
    "# logging.info(\"train_v_num_points_len: %d\", train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\", train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\", len(valid))\n",
    "logging.info(\"test len: %d\", len(test))\n",
    "# logging.info(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:36 AM |\t  train w data size:batchsize:16\t numofbatch:250\t totoal:4000\n",
      "07/09 03:26:36 AM |\t  train syn data size:batchsize:8\t numofbatch:250\t totoal:2000\n",
      "07/09 03:26:36 AM |\t  train syn diff domain data size:batchsize:8\t numofbatch:250\t totoal:2000\n",
      "07/09 03:26:36 AM |\t  train A data size:batchsize:8\t numofbatch:250\t totoal:2000\n",
      "07/09 03:26:37 AM |\t  validation data size:batchsize:32\t numofbatch:119\t totoal:3808\n",
      "07/09 03:26:39 AM |\t  test data size:batchsize:32\t numofbatch:119\t totoal:3808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_w_data = get_data_idx(train[:train_w_num_points_len], tokenizer,train_w_num_points_len)\n",
    "train_A_data = get_data_A(train[train_w_num_points_len:], tokenizer,args.clean_A_data)#TODO:use label now\n",
    "train_syn_data = get_syn_data(unlabeled, tokenizer)\n",
    "train_syn_diff_data = get_syn_data(unlabeled_diff, tokenizer)\n",
    "\n",
    "# indices = list(range(len(train)-train_w_num_points_len))\n",
    "\n",
    "train_w_dataloader = DataLoader(train_w_data, sampler=SequentialSampler(train_w_data),\n",
    "                              batch_size=args.w_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train w data size:{get_dataloader_size(train_w_dataloader)}')\n",
    "\n",
    "\n",
    "train_syn_dataloader = DataLoader(train_syn_data, sampler=RandomSampler(train_syn_data),\n",
    "                              batch_size=args.syn_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train syn data size:{get_dataloader_size(train_syn_dataloader)}')\n",
    "\n",
    "train_syn_diff_dataloader = DataLoader(train_syn_diff_data, sampler=RandomSampler(train_syn_diff_data),\n",
    "                              batch_size=args.syn_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train syn diff domain data size:{get_dataloader_size(train_syn_diff_dataloader)}')\n",
    "\n",
    "\n",
    "train_A_dataloader = DataLoader(train_A_data,  sampler=RandomSampler(train_A_data),\n",
    "                              batch_size=args.A_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train A data size:{get_dataloader_size(train_A_dataloader)}')\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "valid_data = get_data(valid, tokenizer)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data),\n",
    "                              batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'validation data size:{get_dataloader_size(valid_dataloader)}')\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "test_data = get_data(test, tokenizer)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                             batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)  # , sampler=RandomSampler(test_data)\n",
    "logging.info(f'test data size:{get_dataloader_size(test_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(tokenizer, args, train_w_num_points_len)  # half of train regarded as u\n",
    "A = A.cuda()\n",
    "if(args.load_A==1):\n",
    "    state_dict = A.state_dict()\n",
    "    state_dict['alpha'] = torch.load(args.A_path)['alpha']\n",
    "    A.load_state_dict(state_dict)\n",
    "# TODO: model loaded from saved model\n",
    "model_w = Model(tokenizer, args, 'teacher')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.AdamW(model_w.parameters(\n",
    "),  lr=args.w_lr,  betas=(args.beta1, args.beta2), eps=1e-8,weight_decay=1e-4)\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "\n",
    "scheduler_w = get_linear_schedule_with_warmup(w_optimizer, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "model_v = Model(tokenizer, args, 'student')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.AdamW(model_v.parameters(\n",
    "),  lr=args.v_lr,  betas=(args.beta1, args.beta2), eps=1e-8,weight_decay=1e-4)\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "\n",
    "scheduler_v = get_linear_schedule_with_warmup(v_optimizer, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)\n",
    "#  scheduler_v = StepLR(\n",
    "    # v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n",
    "architect.scheduler_A = get_linear_schedule_with_warmup(architect.optimizer_A, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    objs_top1 = AvgrageMeter()\n",
    "    objs_top5 = AvgrageMeter()\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        logits,ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,model)\n",
    "        n = test_dataloaderx.shape[0]\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        prec1, prec5 = accuracy(logits, test_dataloadery, topk=(1, 1))\n",
    "                \n",
    "        objs_top1.update(prec1.item(), n)\n",
    "        \n",
    "        objs_top5.update(prec5.item(), n)\n",
    "    acc = objs_top1.avg\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    logging.info('%s top1 : %f',model.name,objs_top1.avg)\n",
    "    objs_top1.reset()\n",
    "    logging.info('%s top5 : %f',model.name,objs_top5.avg)\n",
    "    objs_top5.reset()\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    model.train()\n",
    "    return acc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_v = None\n",
    "best_w = None\n",
    "def my_train(epoch, wdataloader,syndataloader,syndiffdataloader,Adataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer,  scheduler_w, scheduler_v, tot_iter, past_v_accu):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    objs_w_top1 = AvgrageMeter()\n",
    "    objs_w_top5 = AvgrageMeter()\n",
    "    objs_v_top1 = AvgrageMeter()\n",
    "    objs_v_top5 = AvgrageMeter()\n",
    "    objs_weight = AvgrageMeter_tensor()\n",
    "    objs_cor_weight = AvgrageMeter()\n",
    "    objs_incor_weight = AvgrageMeter()\n",
    "    objs_w_uncertainty = AvgrageMeter()\n",
    "    improvementacc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.w_bs\n",
    "    synsize = args.syn_bs\n",
    "    vsize = -1\n",
    "    Asize = args.A_bs\n",
    "    loader_len = len(wdataloader)\n",
    "    w_model.train()\n",
    "    v_model.train()\n",
    "    for step, w_batch in enumerate(wdataloader):\n",
    "        scheduler_w.step()\n",
    "        scheduler_v.step()\n",
    "        architect.scheduler_A.step()\n",
    "\n",
    "        input_w = Variable(w_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_w_attn = Variable(w_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        if(args.clean_w_data==1):\n",
    "            output_w = Variable(w_batch[4], requires_grad=False).to(\n",
    "                device, non_blocking=False)\n",
    "        else:\n",
    "            output_w = Variable(w_batch[2], requires_grad=False).to(\n",
    "                device, non_blocking=False)\n",
    "        attn_idx = Variable(w_batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        real = Variable(w_batch[4], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        \n",
    "        \n",
    "        syn_batch = next(iter(syndataloader))#syn is IMDB, v train on unlabeled  IMDB  and  test uncertain on IMDB\n",
    "        input_syn = Variable(syn_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_syn_attn = Variable(syn_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        \n",
    "        syn_diff_batch = next(iter(syndataloader))\n",
    "        input_syn_diff = Variable(syn_diff_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_syn_diff_attn = Variable(syn_diff_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "\n",
    "        A_batch = next(iter(Adataloader))\n",
    "        input_A_v = Variable(A_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_A_v_attn = Variable(A_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        output_A_v = Variable(A_batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "        tot_iter[0] += input_w.shape[0]\n",
    "        \n",
    "        \n",
    "        if(True):  # let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "        v_star_val_loss=0\n",
    "        if (args.train_A == 1 and epoch>=args.pre_epochs):\n",
    "            epsilon_w = scheduler_w.get_lr()[0]\n",
    "            epsilon_v  = scheduler_v.get_lr()[0]\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, attn_idx,v_optimizer,\n",
    "                                             epsilon_w, epsilon_v, args.grad_clip)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "\n",
    "\n",
    "\n",
    "        if(args.train_A==1):\n",
    "            with torch.no_grad():    \n",
    "                sampleweight = A(input_w, input_w_attn, attn_idx).data\n",
    "                iscor = real==output_w\n",
    "                cor_mean = torch.mean(sampleweight[iscor])#correct label mean weight\n",
    "                incor_mean = torch.mean(sampleweight[~iscor])#incorrect label mean weight\n",
    "                objs_weight.update(sampleweight)\n",
    "                objs_cor_weight.update(cor_mean,torch.sum(iscor))\n",
    "                objs_incor_weight.update(incor_mean,torch.sum(~iscor))\n",
    "        else:\n",
    "            objs_weight.update(torch.zeros(20))\n",
    "\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        logits, loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                                  A,attn_idx, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "        prec1, prec5 = accuracy(logits, output_w, topk=(1, 1))\n",
    "        objs_w_top1.update(prec1.item(), wsize)\n",
    "        objs_w_top5.update(prec5.item(), wsize)\n",
    "\n",
    "\n",
    "        if(epoch >= args.pre_epochs):\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(\n",
    "                input_syn, input_syn_attn, w_model, v_model)\n",
    "            logits, loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                                    v_model)\n",
    "            v_loss = (args.traindata_loss_ratio*loss +\n",
    "                      loss_aug*args.syndata_loss_ratio)\n",
    "            v_loss.backward()\n",
    "            objs_v_syn.update(loss_aug.item(), synsize)\n",
    "            objs_v_train.update(loss.item(), vsize)\n",
    "            v_optimizer.step()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "            prec1, prec5 = accuracy(logits, output_v, topk=(1, 1))\n",
    "            objs_v_top1.update(prec1.item(), vsize)\n",
    "            objs_v_top5.update(prec5.item(), vsize)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unc = uncertainty(input_syn_diff,input_syn_diff_attn,v_model)\n",
    "            objs_w_uncertainty.update(unc.mean(),input_syn_diff_attn.shape[0])\n",
    "\n",
    "        with torch.no_grad():#new V validation \n",
    "            _,new_v_loss = my_loss2(\n",
    "            input_A_v, input_A_v_attn,  output_A_v,model_v)\n",
    "            improvementacc+=v_star_val_loss-new_v_loss.item()\n",
    "            objs_v_val.update(new_v_loss.item(), Asize)\n",
    "\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "\n",
    "        \n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info('\\n')\n",
    "            \n",
    "            logging.info(f\"{progress:5.3}%:||W_train_loss:{objs_w.avg:^.7f}|V_train_syn_loss:{objs_v_syn.avg:^.7f}|V_train_loss:{objs_v_train.avg:^.7f}\\n\\\n",
    "                |V_val_loss:{objs_v_val.avg:^.7f}|V_star_val_loss:{objs_v_star_val.avg:^.7f}\\n\\\n",
    "                    |improvement:{objs_v_star_val.avg-objs_v_val.avg:^.7f}|w_top1:{objs_w_top1.avg:^.7f}|w_top5:{objs_w_top5.avg:^.7f}\\n\\\n",
    "                        |v_top1:{objs_v_top1.avg:^.7f}|v_top5:{objs_v_top5.avg:^.7f}|w_syn_unc:{objs_w_uncertainty.avg:^.7f}\")\n",
    "            temp = objs_weight.avg\n",
    "            logging.info(f\"avg weight:{temp}\")\n",
    "            logging.info(f\"current alpha:{A.alpha[attn_idx].data}\")\n",
    "            logging.info(f\"current weight:{A(input_w, input_w_attn, attn_idx)}\")\n",
    "            logging.info(f'noise:{torch.mean(temp[5:8]) if args.attack else None} mean:{torch.mean(temp)} max: {torch.max(temp)} min: {torch.min(temp)}')\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'W_accuracy': objs_w_top1.avg})\n",
    "            wandb.log({'v_accuracy': objs_v_top1.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w_uncertainty.reset()\n",
    "            objs_weight.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "            objs_w_top1.reset()\n",
    "            objs_w_top5.reset()\n",
    "\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            w_accu = my_test(validdataloader, model_w, epoch)\n",
    "            v_accu = my_test(validdataloader, model_v, epoch)\n",
    "            wandb.log({'W_test_accuracy': w_accu})\n",
    "            wandb.log({'v_test_accuracy':v_accu})\n",
    "            wandb.log({'correct_label_mean_weight': objs_cor_weight.avg})\n",
    "            wandb.log({'wrong_label_mean_weight':objs_incor_weight.avg})\n",
    "            logging.info(f'correct label mean weight: {objs_cor_weight.avg}, wrong label mean weight: {objs_incor_weight.avg}')\n",
    "            objs_cor_weight.reset()\n",
    "            objs_incor_weight.reset()\n",
    "            torch.save(A.state_dict(), os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "            if(v_accu>past_v_accu):\n",
    "                past_v_accu = v_accu\n",
    "                logging.info('find a better model')\n",
    "                global best_w\n",
    "                global best_v\n",
    "                best_v = copy.deepcopy(model_v)\n",
    "                best_w = copy.deepcopy(model_w)\n",
    "                torch.save(model_w.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_w.pt\"))\n",
    "                torch.save(model_v.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_v.pt\"))\n",
    "            wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "            \n",
    "            logging.info(f'current best accuracy:{past_v_accu}')\n",
    "    logging.info(f'improvment:{improvementacc}')\n",
    "    return w_trainloss_acc,past_v_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 03:26:40 AM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.0,\t\tlr_v:0.0,\t\tlr_A:0.0----------------\n",
      "07/09 03:27:07 AM |\t  \n",
      "\n",
      "07/09 03:27:07 AM |\t   19.7%:||W_train_loss:0.7062569|V_train_syn_loss:0.6860857|V_train_loss:0.7125715\n",
      "                |V_val_loss:0.6907411|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.6907411|w_top1:49.6250000|w_top5:49.6250000\n",
      "                        |v_top1:49.5000000|v_top5:49.5000000|w_syn_unc:0.6822055\n",
      "07/09 03:27:07 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:27:07 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:27:07 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:27:07 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:27:32 AM |\t  \n",
      "\n",
      "07/09 03:27:32 AM |\t   39.8%:||W_train_loss:0.6811133|V_train_syn_loss:0.6929220|V_train_loss:0.6935745\n",
      "                |V_val_loss:0.6888618|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.6888618|w_top1:57.1250000|w_top5:57.1250000\n",
      "                        |v_top1:51.2500000|v_top5:51.2500000|w_syn_unc:0.6919382\n",
      "07/09 03:27:32 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:27:32 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:27:32 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:27:32 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:27:58 AM |\t  \n",
      "\n",
      "07/09 03:27:58 AM |\t   59.8%:||W_train_loss:0.6591430|V_train_syn_loss:0.6932787|V_train_loss:0.6887830\n",
      "                |V_val_loss:0.6788449|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.6788449|w_top1:68.0000000|w_top5:68.0000000\n",
      "                        |v_top1:51.7083333|v_top5:51.7083333|w_syn_unc:0.6925172\n",
      "07/09 03:27:58 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:27:58 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:27:58 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:27:58 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:28:23 AM |\t  \n",
      "\n",
      "07/09 03:28:23 AM |\t   79.9%:||W_train_loss:0.4768863|V_train_syn_loss:0.6884939|V_train_loss:0.6898849\n",
      "                |V_val_loss:0.6908980|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.6908980|w_top1:90.1250000|w_top5:90.1250000\n",
      "                        |v_top1:52.5000000|v_top5:52.5000000|w_syn_unc:0.6844932\n",
      "07/09 03:28:23 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:28:23 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:28:23 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:28:23 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:28:48 AM |\t  \n",
      "\n",
      "07/09 03:28:48 AM |\t  1e+02%:||W_train_loss:0.2668529|V_train_syn_loss:0.6327392|V_train_loss:0.6777724\n",
      "                |V_val_loss:0.6815134|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.6815134|w_top1:90.7500000|w_top5:90.7500000\n",
      "                        |v_top1:52.3500000|v_top5:52.3500000|w_syn_unc:0.6378160\n",
      "07/09 03:28:48 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:28:48 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:28:48 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:28:48 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:28:56 AM |\t  teacher test loss : 0.779080\n",
      "07/09 03:28:56 AM |\t  teacher top1 : 92.710526\n",
      "07/09 03:28:56 AM |\t  teacher top5 : 92.710526\n",
      "07/09 03:28:56 AM |\t  teacher test loss : 0.779080\n",
      "07/09 03:29:05 AM |\t  student test loss : 0.519239\n",
      "07/09 03:29:05 AM |\t  student top1 : 61.789474\n",
      "07/09 03:29:05 AM |\t  student top5 : 61.789474\n",
      "07/09 03:29:05 AM |\t  student test loss : 0.519239\n",
      "07/09 03:29:05 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:29:05 AM |\t  find a better model\n",
      "07/09 03:29:07 AM |\t  current best accuracy:61.78947365208676\n",
      "07/09 03:29:07 AM |\t  improvment:-171.5429523587227\n",
      "07/09 03:29:07 AM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:1.8072289156626506e-06,\t\tlr_v:1.8072289156626506e-06,\t\tlr_A:90.36144578313254----------------\n",
      "07/09 03:29:32 AM |\t  \n",
      "\n",
      "07/09 03:29:32 AM |\t   19.7%:||W_train_loss:0.1636229|V_train_syn_loss:0.4743647|V_train_loss:0.5467100\n",
      "                |V_val_loss:0.5613736|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.5613736|w_top1:94.6250000|w_top5:94.6250000\n",
      "                        |v_top1:76.2500000|v_top5:76.2500000|w_syn_unc:0.4753407\n",
      "07/09 03:29:32 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:29:32 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:29:32 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:29:32 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:29:57 AM |\t  \n",
      "\n",
      "07/09 03:29:57 AM |\t   39.8%:||W_train_loss:0.2249898|V_train_syn_loss:0.4401496|V_train_loss:0.5147108\n",
      "                |V_val_loss:0.5268295|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.5268295|w_top1:91.6250000|w_top5:91.6250000\n",
      "                        |v_top1:79.7500000|v_top5:79.7500000|w_syn_unc:0.4324005\n",
      "07/09 03:29:57 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:29:57 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:29:57 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:29:57 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:30:23 AM |\t  \n",
      "\n",
      "07/09 03:30:23 AM |\t   59.8%:||W_train_loss:0.1681380|V_train_syn_loss:0.3789692|V_train_loss:0.4812592\n",
      "                |V_val_loss:0.5005354|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.5005354|w_top1:94.3750000|w_top5:94.3750000\n",
      "                        |v_top1:81.8333333|v_top5:81.8333333|w_syn_unc:0.3956494\n",
      "07/09 03:30:23 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:30:23 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:30:23 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:30:23 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:30:48 AM |\t  \n",
      "\n",
      "07/09 03:30:48 AM |\t   79.9%:||W_train_loss:0.1530307|V_train_syn_loss:0.3541524|V_train_loss:0.4587073\n",
      "                |V_val_loss:0.4821201|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4821201|w_top1:94.6250000|w_top5:94.6250000\n",
      "                        |v_top1:83.4062500|v_top5:83.4062500|w_syn_unc:0.3533028\n",
      "07/09 03:30:48 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:30:48 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:30:48 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:30:48 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:31:14 AM |\t  \n",
      "\n",
      "07/09 03:31:14 AM |\t  1e+02%:||W_train_loss:0.1903476|V_train_syn_loss:0.3527797|V_train_loss:0.4619718\n",
      "                |V_val_loss:0.4595842|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4595842|w_top1:93.7500000|w_top5:93.7500000\n",
      "                        |v_top1:84.3500000|v_top5:84.3500000|w_syn_unc:0.3586474\n",
      "07/09 03:31:14 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:31:14 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:31:14 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:31:14 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:31:22 AM |\t  teacher test loss : 0.787041\n",
      "07/09 03:31:22 AM |\t  teacher top1 : 93.657895\n",
      "07/09 03:31:22 AM |\t  teacher top5 : 93.657895\n",
      "07/09 03:31:22 AM |\t  teacher test loss : 0.787041\n",
      "07/09 03:31:30 AM |\t  student test loss : 0.770013\n",
      "07/09 03:31:30 AM |\t  student top1 : 91.631579\n",
      "07/09 03:31:30 AM |\t  student top5 : 91.631579\n",
      "07/09 03:31:30 AM |\t  student test loss : 0.770013\n",
      "07/09 03:31:30 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:31:30 AM |\t  find a better model\n",
      "07/09 03:31:31 AM |\t  current best accuracy:91.63157894736842\n",
      "07/09 03:31:31 AM |\t  improvment:-126.52213951945305\n",
      "07/09 03:31:31 AM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:1.606425702811245e-06,\t\tlr_v:1.606425702811245e-06,\t\tlr_A:80.32128514056225----------------\n",
      "07/09 03:31:57 AM |\t  \n",
      "\n",
      "07/09 03:31:57 AM |\t   19.7%:||W_train_loss:0.1074240|V_train_syn_loss:0.3282439|V_train_loss:0.4195424\n",
      "                |V_val_loss:0.4409844|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4409844|w_top1:96.7500000|w_top5:96.7500000\n",
      "                        |v_top1:91.8750000|v_top5:91.8750000|w_syn_unc:0.3253609\n",
      "07/09 03:31:57 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:31:57 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:31:57 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:31:57 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:32:22 AM |\t  \n",
      "\n",
      "07/09 03:32:22 AM |\t   39.8%:||W_train_loss:0.1493051|V_train_syn_loss:0.3352874|V_train_loss:0.4406307\n",
      "                |V_val_loss:0.4664684|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4664684|w_top1:95.1250000|w_top5:95.1250000\n",
      "                        |v_top1:90.1875000|v_top5:90.1875000|w_syn_unc:0.3086032\n",
      "07/09 03:32:22 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:32:22 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:32:22 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:32:22 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:32:47 AM |\t  \n",
      "\n",
      "07/09 03:32:47 AM |\t   59.8%:||W_train_loss:0.1401590|V_train_syn_loss:0.3226883|V_train_loss:0.4328550\n",
      "                |V_val_loss:0.4297986|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4297986|w_top1:95.5000000|w_top5:95.5000000\n",
      "                        |v_top1:89.8333333|v_top5:89.8333333|w_syn_unc:0.3262812\n",
      "07/09 03:32:47 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:32:47 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:32:47 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:32:47 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:33:13 AM |\t  \n",
      "\n",
      "07/09 03:33:13 AM |\t   79.9%:||W_train_loss:0.1228356|V_train_syn_loss:0.3081776|V_train_loss:0.3996473\n",
      "                |V_val_loss:0.4298718|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4298718|w_top1:95.2500000|w_top5:95.2500000\n",
      "                        |v_top1:90.2812500|v_top5:90.2812500|w_syn_unc:0.3008796\n",
      "07/09 03:33:13 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:33:13 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:33:13 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:33:13 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:33:38 AM |\t  \n",
      "\n",
      "07/09 03:33:38 AM |\t  1e+02%:||W_train_loss:0.1888294|V_train_syn_loss:0.2952902|V_train_loss:0.4070923\n",
      "                |V_val_loss:0.4001872|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.4001872|w_top1:94.0000000|w_top5:94.0000000\n",
      "                        |v_top1:90.4000000|v_top5:90.4000000|w_syn_unc:0.2922386\n",
      "07/09 03:33:38 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:33:38 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:33:38 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:33:38 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:33:47 AM |\t  teacher test loss : 0.789253\n",
      "07/09 03:33:47 AM |\t  teacher top1 : 93.921053\n",
      "07/09 03:33:47 AM |\t  teacher top5 : 93.921053\n",
      "07/09 03:33:47 AM |\t  teacher test loss : 0.789253\n",
      "07/09 03:33:55 AM |\t  student test loss : 0.777090\n",
      "07/09 03:33:55 AM |\t  student top1 : 92.473684\n",
      "07/09 03:33:55 AM |\t  student top5 : 92.473684\n",
      "07/09 03:33:55 AM |\t  student test loss : 0.777090\n",
      "07/09 03:33:55 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:33:55 AM |\t  find a better model\n",
      "07/09 03:33:57 AM |\t  current best accuracy:92.47368421052632\n",
      "07/09 03:33:57 AM |\t  improvment:-108.36551839113235\n",
      "07/09 03:33:57 AM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:1.4056224899598392e-06,\t\tlr_v:1.4056224899598392e-06,\t\tlr_A:70.28112449799197----------------\n",
      "07/09 03:34:22 AM |\t  \n",
      "\n",
      "07/09 03:34:22 AM |\t   19.7%:||W_train_loss:0.0921526|V_train_syn_loss:0.3316713|V_train_loss:0.3747358\n",
      "                |V_val_loss:0.3991060|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3991060|w_top1:97.3750000|w_top5:97.3750000\n",
      "                        |v_top1:94.6250000|v_top5:94.6250000|w_syn_unc:0.3167272\n",
      "07/09 03:34:22 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:34:22 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:34:22 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:34:22 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:34:47 AM |\t  \n",
      "\n",
      "07/09 03:34:47 AM |\t   39.8%:||W_train_loss:0.1347067|V_train_syn_loss:0.2941257|V_train_loss:0.3720471\n",
      "                |V_val_loss:0.3946486|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3946486|w_top1:95.8750000|w_top5:95.8750000\n",
      "                        |v_top1:93.3125000|v_top5:93.3125000|w_syn_unc:0.3063535\n",
      "07/09 03:34:47 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:34:47 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:34:47 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:34:47 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:35:13 AM |\t  \n",
      "\n",
      "07/09 03:35:13 AM |\t   59.8%:||W_train_loss:0.1347989|V_train_syn_loss:0.2879457|V_train_loss:0.3516944\n",
      "                |V_val_loss:0.3703510|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3703510|w_top1:95.2500000|w_top5:95.2500000\n",
      "                        |v_top1:93.2083333|v_top5:93.2083333|w_syn_unc:0.2661647\n",
      "07/09 03:35:13 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:35:13 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:35:13 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:35:13 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:35:38 AM |\t  \n",
      "\n",
      "07/09 03:35:38 AM |\t   79.9%:||W_train_loss:0.0922520|V_train_syn_loss:0.2990780|V_train_loss:0.3324070\n",
      "                |V_val_loss:0.3741671|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3741671|w_top1:97.0000000|w_top5:97.0000000\n",
      "                        |v_top1:93.3125000|v_top5:93.3125000|w_syn_unc:0.3021491\n",
      "07/09 03:35:38 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:35:38 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:35:38 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:35:38 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:36:03 AM |\t  \n",
      "\n",
      "07/09 03:36:03 AM |\t  1e+02%:||W_train_loss:0.1444415|V_train_syn_loss:0.3150829|V_train_loss:0.3695769\n",
      "                |V_val_loss:0.3717030|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3717030|w_top1:95.6250000|w_top5:95.6250000\n",
      "                        |v_top1:92.6500000|v_top5:92.6500000|w_syn_unc:0.2939999\n",
      "07/09 03:36:03 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:36:03 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:36:03 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:36:03 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:36:12 AM |\t  teacher test loss : 0.790801\n",
      "07/09 03:36:12 AM |\t  teacher top1 : 94.105263\n",
      "07/09 03:36:12 AM |\t  teacher top5 : 94.105263\n",
      "07/09 03:36:12 AM |\t  teacher test loss : 0.790801\n",
      "07/09 03:36:20 AM |\t  student test loss : 0.781955\n",
      "07/09 03:36:20 AM |\t  student top1 : 93.052632\n",
      "07/09 03:36:20 AM |\t  student top5 : 93.052632\n",
      "07/09 03:36:20 AM |\t  student test loss : 0.781955\n",
      "07/09 03:36:20 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:36:22 AM |\t  find a better model\n",
      "07/09 03:36:24 AM |\t  current best accuracy:93.05263157894737\n",
      "07/09 03:36:24 AM |\t  improvment:-95.49878577888012\n",
      "07/09 03:36:24 AM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:1.2048192771084338e-06,\t\tlr_v:1.2048192771084338e-06,\t\tlr_A:60.24096385542169----------------\n",
      "07/09 03:36:50 AM |\t  \n",
      "\n",
      "07/09 03:36:50 AM |\t   19.7%:||W_train_loss:0.0819815|V_train_syn_loss:0.2823243|V_train_loss:0.3174516\n",
      "                |V_val_loss:0.3458368|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3458368|w_top1:97.7500000|w_top5:97.7500000\n",
      "                        |v_top1:94.8750000|v_top5:94.8750000|w_syn_unc:0.2838177\n",
      "07/09 03:36:50 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:36:50 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:36:50 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:36:50 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:37:15 AM |\t  \n",
      "\n",
      "07/09 03:37:15 AM |\t   39.8%:||W_train_loss:0.1057505|V_train_syn_loss:0.2761606|V_train_loss:0.3267797\n",
      "                |V_val_loss:0.3560376|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3560376|w_top1:96.6250000|w_top5:96.6250000\n",
      "                        |v_top1:93.9375000|v_top5:93.9375000|w_syn_unc:0.2403190\n",
      "07/09 03:37:15 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:37:15 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:37:15 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:37:15 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:37:41 AM |\t  \n",
      "\n",
      "07/09 03:37:41 AM |\t   59.8%:||W_train_loss:0.1151354|V_train_syn_loss:0.2647584|V_train_loss:0.3249739\n",
      "                |V_val_loss:0.3251172|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3251172|w_top1:95.8750000|w_top5:95.8750000\n",
      "                        |v_top1:93.5000000|v_top5:93.5000000|w_syn_unc:0.2576257\n",
      "07/09 03:37:41 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:37:41 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:37:41 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:37:41 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:38:06 AM |\t  \n",
      "\n",
      "07/09 03:38:06 AM |\t   79.9%:||W_train_loss:0.0746687|V_train_syn_loss:0.2603835|V_train_loss:0.3049166\n",
      "                |V_val_loss:0.3504005|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3504005|w_top1:97.2500000|w_top5:97.2500000\n",
      "                        |v_top1:93.6562500|v_top5:93.6562500|w_syn_unc:0.2760010\n",
      "07/09 03:38:06 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:38:06 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:38:06 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:38:06 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:38:31 AM |\t  \n",
      "\n",
      "07/09 03:38:31 AM |\t  1e+02%:||W_train_loss:0.1403490|V_train_syn_loss:0.2960057|V_train_loss:0.3368948\n",
      "                |V_val_loss:0.3250373|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3250373|w_top1:96.2500000|w_top5:96.2500000\n",
      "                        |v_top1:92.9750000|v_top5:92.9750000|w_syn_unc:0.2761583\n",
      "07/09 03:38:31 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:38:31 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:38:31 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:38:31 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:38:40 AM |\t  teacher test loss : 0.791906\n",
      "07/09 03:38:40 AM |\t  teacher top1 : 94.236842\n",
      "07/09 03:38:40 AM |\t  teacher top5 : 94.236842\n",
      "07/09 03:38:40 AM |\t  teacher test loss : 0.791906\n",
      "07/09 03:38:48 AM |\t  student test loss : 0.781955\n",
      "07/09 03:38:48 AM |\t  student top1 : 93.052632\n",
      "07/09 03:38:48 AM |\t  student top5 : 93.052632\n",
      "07/09 03:38:48 AM |\t  student test loss : 0.781955\n",
      "07/09 03:38:48 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:38:48 AM |\t  current best accuracy:93.05263157894737\n",
      "07/09 03:38:48 AM |\t  improvment:-85.121462225914\n",
      "07/09 03:38:48 AM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:1.0040160642570282e-06,\t\tlr_v:1.0040160642570282e-06,\t\tlr_A:50.20080321285141----------------\n",
      "07/09 03:39:14 AM |\t  \n",
      "\n",
      "07/09 03:39:14 AM |\t   19.7%:||W_train_loss:0.0680114|V_train_syn_loss:0.2701924|V_train_loss:0.2944112\n",
      "                |V_val_loss:0.3073550|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3073550|w_top1:98.2500000|w_top5:98.2500000\n",
      "                        |v_top1:94.8750000|v_top5:94.8750000|w_syn_unc:0.2685205\n",
      "07/09 03:39:14 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:39:14 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:39:14 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:39:14 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:39:39 AM |\t  \n",
      "\n",
      "07/09 03:39:39 AM |\t   39.8%:||W_train_loss:0.0911991|V_train_syn_loss:0.2537887|V_train_loss:0.3032166\n",
      "                |V_val_loss:0.3477064|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3477064|w_top1:97.1250000|w_top5:97.1250000\n",
      "                        |v_top1:93.6250000|v_top5:93.6250000|w_syn_unc:0.2761645\n",
      "07/09 03:39:39 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:39:39 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:39:39 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:39:39 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:40:04 AM |\t  \n",
      "\n",
      "07/09 03:40:04 AM |\t   59.8%:||W_train_loss:0.0922300|V_train_syn_loss:0.2808938|V_train_loss:0.3021006\n",
      "                |V_val_loss:0.2905207|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.2905207|w_top1:97.1250000|w_top5:97.1250000\n",
      "                        |v_top1:93.6250000|v_top5:93.6250000|w_syn_unc:0.2496729\n",
      "07/09 03:40:04 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:40:04 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:40:04 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:40:04 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:40:30 AM |\t  \n",
      "\n",
      "07/09 03:40:30 AM |\t   79.9%:||W_train_loss:0.0944097|V_train_syn_loss:0.2670352|V_train_loss:0.2794953\n",
      "                |V_val_loss:0.3002000|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3002000|w_top1:95.7500000|w_top5:95.7500000\n",
      "                        |v_top1:93.8125000|v_top5:93.8125000|w_syn_unc:0.2539420\n",
      "07/09 03:40:30 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:40:30 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:40:30 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:40:30 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:40:55 AM |\t  \n",
      "\n",
      "07/09 03:40:55 AM |\t  1e+02%:||W_train_loss:0.1160791|V_train_syn_loss:0.2583102|V_train_loss:0.3268692\n",
      "                |V_val_loss:0.3070389|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3070389|w_top1:96.0000000|w_top5:96.0000000\n",
      "                        |v_top1:93.0500000|v_top5:93.0500000|w_syn_unc:0.2595630\n",
      "07/09 03:40:55 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:40:55 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:40:55 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:40:55 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:41:04 AM |\t  teacher test loss : 0.793675\n",
      "07/09 03:41:04 AM |\t  teacher top1 : 94.447368\n",
      "07/09 03:41:04 AM |\t  teacher top5 : 94.447368\n",
      "07/09 03:41:04 AM |\t  teacher test loss : 0.793675\n",
      "07/09 03:41:12 AM |\t  student test loss : 0.771782\n",
      "07/09 03:41:12 AM |\t  student top1 : 91.842105\n",
      "07/09 03:41:12 AM |\t  student top5 : 91.842105\n",
      "07/09 03:41:12 AM |\t  student test loss : 0.771782\n",
      "07/09 03:41:12 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:41:12 AM |\t  current best accuracy:93.05263157894737\n",
      "07/09 03:41:12 AM |\t  improvment:-77.64105147123337\n",
      "07/09 03:41:12 AM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:8.032128514056225e-07,\t\tlr_v:8.032128514056225e-07,\t\tlr_A:40.16064257028113----------------\n",
      "07/09 03:41:37 AM |\t  \n",
      "\n",
      "07/09 03:41:37 AM |\t   19.7%:||W_train_loss:0.0706656|V_train_syn_loss:0.2543329|V_train_loss:0.2662879\n",
      "                |V_val_loss:0.3057834|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3057834|w_top1:97.5000000|w_top5:97.5000000\n",
      "                        |v_top1:95.8750000|v_top5:95.8750000|w_syn_unc:0.2322184\n",
      "07/09 03:41:37 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:41:37 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:41:37 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:41:37 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:42:03 AM |\t  \n",
      "\n",
      "07/09 03:42:03 AM |\t   39.8%:||W_train_loss:0.0837997|V_train_syn_loss:0.2523957|V_train_loss:0.2882164\n",
      "                |V_val_loss:0.3016615|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3016615|w_top1:97.8750000|w_top5:97.8750000\n",
      "                        |v_top1:94.4375000|v_top5:94.4375000|w_syn_unc:0.2580886\n",
      "07/09 03:42:03 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:42:03 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:42:03 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:42:03 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:42:28 AM |\t  \n",
      "\n",
      "07/09 03:42:28 AM |\t   59.8%:||W_train_loss:0.0857601|V_train_syn_loss:0.2510714|V_train_loss:0.2894684\n",
      "                |V_val_loss:0.2979987|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.2979987|w_top1:97.1250000|w_top5:97.1250000\n",
      "                        |v_top1:93.7083333|v_top5:93.7083333|w_syn_unc:0.2469608\n",
      "07/09 03:42:28 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:42:28 AM |\t  current alpha:tensor([ 0.0661, -0.2385, -0.0928,  0.0912, -0.0854, -0.2033,  0.2079,  0.0246,\n",
      "        -0.0091,  0.1396,  0.0342,  0.0378, -0.1839,  0.0646, -0.0239,  0.0323],\n",
      "       device='cuda:0')\n",
      "07/09 03:42:28 AM |\t  current weight:tensor([1.0375, 0.8851, 0.9577, 1.0501, 0.9614, 0.9026, 1.1083, 1.0167, 0.9997,\n",
      "        1.0743, 1.0215, 1.0233, 0.9122, 1.0368, 0.9923, 1.0205],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:42:28 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:42:54 AM |\t  \n",
      "\n",
      "07/09 03:42:54 AM |\t   79.9%:||W_train_loss:0.0541407|V_train_syn_loss:0.2481191|V_train_loss:0.2740678\n",
      "                |V_val_loss:0.3146561|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3146561|w_top1:98.0000000|w_top5:98.0000000\n",
      "                        |v_top1:93.6875000|v_top5:93.6875000|w_syn_unc:0.2487655\n",
      "07/09 03:42:54 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:42:54 AM |\t  current alpha:tensor([-0.0462,  0.0342,  0.1418,  0.1998,  0.1657,  0.0548,  0.0494, -0.1248,\n",
      "         0.0403, -0.0515, -0.1802, -0.0780, -0.0542,  0.0387,  0.1061,  0.0113],\n",
      "       device='cuda:0')\n",
      "07/09 03:42:54 AM |\t  current weight:tensor([0.9677, 1.0075, 1.0606, 1.0891, 1.0724, 1.0176, 1.0150, 0.9288, 1.0105,\n",
      "        0.9650, 0.9015, 0.9519, 0.9637, 1.0097, 1.0430, 0.9961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:42:54 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:43:19 AM |\t  \n",
      "\n",
      "07/09 03:43:19 AM |\t  1e+02%:||W_train_loss:0.1272295|V_train_syn_loss:0.2484690|V_train_loss:0.3064253\n",
      "                |V_val_loss:0.3082081|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.3082081|w_top1:96.1250000|w_top5:96.1250000\n",
      "                        |v_top1:93.2500000|v_top5:93.2500000|w_syn_unc:0.2564428\n",
      "07/09 03:43:19 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:43:19 AM |\t  current alpha:tensor([ 0.0332, -0.0206,  0.0284,  0.0855, -0.0086, -0.0323,  0.0453,  0.0800,\n",
      "         0.0477, -0.0022, -0.0761,  0.1007,  0.1126, -0.1858, -0.0082, -0.1832],\n",
      "       device='cuda:0')\n",
      "07/09 03:43:19 AM |\t  current weight:tensor([1.0161, 0.9892, 1.0136, 1.0422, 0.9952, 0.9833, 1.0221, 1.0394, 1.0233,\n",
      "        0.9984, 0.9615, 1.0497, 1.0557, 0.9069, 0.9953, 0.9081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:43:19 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:43:28 AM |\t  teacher test loss : 0.793897\n",
      "07/09 03:43:28 AM |\t  teacher top1 : 94.473684\n",
      "07/09 03:43:28 AM |\t  teacher top5 : 94.473684\n",
      "07/09 03:43:28 AM |\t  teacher test loss : 0.793897\n",
      "07/09 03:43:36 AM |\t  student test loss : 0.772446\n",
      "07/09 03:43:36 AM |\t  student top1 : 91.921053\n",
      "07/09 03:43:36 AM |\t  student top5 : 91.921053\n",
      "07/09 03:43:36 AM |\t  student test loss : 0.772446\n",
      "07/09 03:43:36 AM |\t  correct label mean weight: 0, wrong label mean weight: 0\n",
      "07/09 03:43:36 AM |\t  current best accuracy:93.05263157894737\n",
      "07/09 03:43:36 AM |\t  improvment:-76.41538721323013\n",
      "07/09 03:43:36 AM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:6.024096385542169e-07,\t\tlr_v:6.024096385542169e-07,\t\tlr_A:30.120481927710845----------------\n",
      "07/09 03:44:02 AM |\t  \n",
      "\n",
      "07/09 03:44:02 AM |\t   19.7%:||W_train_loss:0.0579038|V_train_syn_loss:0.2150141|V_train_loss:0.2426169\n",
      "                |V_val_loss:0.2941602|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.2941602|w_top1:98.5000000|w_top5:98.5000000\n",
      "                        |v_top1:95.6250000|v_top5:95.6250000|w_syn_unc:0.2474707\n",
      "07/09 03:44:02 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:44:02 AM |\t  current alpha:tensor([ 0.0623,  0.1025,  0.0713,  0.0228,  0.0354, -0.1296, -0.0546, -0.0078,\n",
      "         0.1655, -0.0462, -0.2782,  0.1326, -0.0076,  0.0930,  0.0627, -0.1156],\n",
      "       device='cuda:0')\n",
      "07/09 03:44:02 AM |\t  current weight:tensor([1.0276, 1.0476, 1.0321, 1.0079, 1.0142, 0.9321, 0.9694, 0.9927, 1.0789,\n",
      "        0.9736, 0.8589, 1.0626, 0.9928, 1.0429, 1.0278, 0.9390],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:44:02 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n",
      "07/09 03:44:27 AM |\t  \n",
      "\n",
      "07/09 03:44:27 AM |\t   39.8%:||W_train_loss:0.0797708|V_train_syn_loss:0.2402143|V_train_loss:0.2652356\n",
      "                |V_val_loss:0.2479740|V_star_val_loss:0.0000000\n",
      "                    |improvement:-0.2479740|w_top1:97.5000000|w_top5:97.5000000\n",
      "                        |v_top1:94.4375000|v_top5:94.4375000|w_syn_unc:0.2441034\n",
      "07/09 03:44:27 AM |\t  avg weight:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "07/09 03:44:27 AM |\t  current alpha:tensor([ 0.0300,  0.0741,  0.1493, -0.0910,  0.0613, -0.1484, -0.1135,  0.0923,\n",
      "        -0.0197, -0.0911,  0.0272,  0.1290, -0.1499, -0.0152,  0.1025,  0.0718],\n",
      "       device='cuda:0')\n",
      "07/09 03:44:27 AM |\t  current weight:tensor([1.0116, 1.0335, 1.0709, 0.9513, 1.0271, 0.9228, 0.9401, 1.0425, 0.9868,\n",
      "        0.9512, 1.0102, 1.0608, 0.9221, 0.9890, 1.0476, 1.0324],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/09 03:44:27 AM |\t  noise:None mean:0.0 max: 0.0 min: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-26d652d31ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     w_train_loss,v_accu = my_train(epoch, train_w_dataloader,train_syn_dataloader,train_syn_diff_dataloader,train_A_dataloader, valid_dataloader, model_w,\n\u001b[0m\u001b[1;32m     16\u001b[0m                             model_v,  architect, A, w_optimizer, v_optimizer, scheduler_w, scheduler_v, tot_iter,v_accu)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0ca398e7e981>\u001b[0m in \u001b[0;36mmy_train\u001b[0;34m(epoch, wdataloader, syndataloader, syndiffdataloader, Adataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, scheduler_w, scheduler_v, tot_iter, past_v_accu)\u001b[0m\n\u001b[1;32m    124\u001b[0m             v_loss = (args.traindata_loss_ratio*loss +\n\u001b[1;32m    125\u001b[0m                       loss_aug*args.syndata_loss_ratio)\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mobjs_v_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mobjs_v_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin == 1):\n",
    "#     my_test(valid_dataloader, model_w, -1)  # before train\n",
    "#     my_test(valid_dataloader, model_v, -1)\n",
    "\n",
    "tot_iter = [0]\n",
    "v_accu = 0\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(\n",
    "        f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss,v_accu = my_train(epoch, train_w_dataloader,train_syn_dataloader,train_syn_diff_dataloader,train_A_dataloader, valid_dataloader, model_w,\n",
    "                            model_v,  architect, A, w_optimizer, v_optimizer, scheduler_w, scheduler_v, tot_iter,v_accu)\n",
    "\n",
    "    # scheduler_w.step()\n",
    "    # scheduler_v.step()\n",
    "    # architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "\n",
    "w_accu = my_test(test_dataloader,best_w, -2)\n",
    "v_accu = my_test(test_dataloader,best_v, -2)\n",
    "logging.info(f'best w on test:{w_accu} accuracy; best v on test:{v_accu} accuracy')\n",
    "\n",
    "wandb.log({'v_testdata_accuracy': v_accu})\n",
    "wandb.log({'w_testdata_accuracy': w_accu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/08 02:39:00 AM |\t  teacher test loss : 0.790801\n",
      "07/08 02:39:00 AM |\t  teacher top1 : 94.105263\n",
      "07/08 02:39:00 AM |\t  teacher top5 : 94.105263\n",
      "07/08 02:39:00 AM |\t  teacher test loss : 0.790801\n",
      "07/08 02:39:09 AM |\t  student test loss : 0.798319\n",
      "07/08 02:39:09 AM |\t  student top1 : 95.000000\n",
      "07/08 02:39:09 AM |\t  student top5 : 95.000000\n",
      "07/08 02:39:09 AM |\t  student test loss : 0.798319\n",
      "07/08 02:39:09 AM |\t  best w on test:94.10526315789474 accuracy; best v on test:94.99999998393811 accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_accu = my_test(test_dataloader,best_w, -2)\n",
    "v_accu = my_test(test_dataloader,best_v, -2)\n",
    "logging.info(f'best w on test:{w_accu} accuracy; best v on test:{v_accu} accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (keepdim=NoneType, dim=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdim, dim\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6d0c3fc58ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tianyi-vol/Self-teaching-for-machine-translation/BERT/losses.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(p, dim, keepdim)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# can be a scalar, when PyTorch.supports it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (keepdim=NoneType, dim=int, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdim, dim\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "entropy(torch.rand(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0., -0., -0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
