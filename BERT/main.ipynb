{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import  AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 10000, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 30000, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = -1, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=24,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=8,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=8,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "# parser.add_argument('--model_name_teacher', type=str,           default='prajjwal1/bert-small',      help='model_name')\n",
    "# parser.add_argument('--model_name_student', type=str,           default='prajjwal1/bert-small',      help='model_name')\n",
    "# parser.add_argument('--model_name_de2en', type=str,             default='prajjwal1/bert-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='SST2,noA',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=2500,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=30000,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=200,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=5,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=0,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.999,    help='momentum')\n",
    "parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=10,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=1,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "parser.add_argument('--freeze', type=int,                       default=1,    help='whether freeze the pretrained encoder')\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "parser.add_argument('--embedding_dim', type=int,                default=300 ,     help='whether train A')\n",
    "parser.add_argument('--out_dim', type=int,                      default=2 ,     help='whether train A')\n",
    "parser.add_argument('--hidden_size', type=int,                  default=64 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "args.test_num = args.test_num//args.batch_size * args.batch_size\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.20 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\BERT\\wandb\\run-20220629_141508-38dw7twd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/38dw7twd\" target=\"_blank\">SST2,noA</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/38dw7twd?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2028c4d70d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "wandb.init(project=\"Selftraining\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/29 02:15:17 PM |\t  Reusing dataset glue (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 601.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/29 02:15:17 PM |\t  Namespace(A_lr=0, batch_size=24, beta1=0.9, beta2=0.999, decay=0.001, decay_lr=1, embedding_dim=300, epochs=200, exp_name='SST2,noA', freeze=1, gpu=0, grad_acc_count=-1, grad_clip=5, hidden_size=64, learning_rate_min=1e-08, num_step_lr=10, num_workers=0, out_dim=2, pre_epochs=0, rep_num=2496, syndata_loss_ratio=1, test_num=30000, test_num_points=-1, train_A=0, train_A_num_points=8, train_num_points=30000, train_v_num_points=0, train_v_synthetic_num_points=8, train_w_num_points=8, traindata_loss_ratio=0, unrolled_v_lr=0.001, unrolled_w_lr=0.001, v_lr=0.001, valid_begin=1, valid_num_points=10000, w_lr=0.001, warm=10)\n",
      "06/29 02:15:17 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "06/29 02:15:17 PM |\t  {'sentence': \"that 's far too tragic to merit such superficial treatment \", 'idx': 5, 'label': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/29 02:15:19 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-9357be698e35dc75.arrow\n",
      "06/29 02:15:19 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-325e2cadaa1cefad.arrow\n",
      "06/29 02:15:19 PM |\t  train len: 30000\n",
      "06/29 02:15:19 PM |\t  train_w_num_points_len: 10000\n",
      "06/29 02:15:19 PM |\t  train_v_synthetic_num_points_len: 10000\n",
      "06/29 02:15:19 PM |\t  train_v_num_points_len: 0\n",
      "06/29 02:15:19 PM |\t  train_A_num_points_len: 10000\n",
      "06/29 02:15:19 PM |\t  valid len: 30000\n",
      "06/29 02:15:19 PM |\t  test len: 872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess the data, make a dataloader\n",
    "import random\n",
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-6-6')#TODO:\n",
    " \n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = dataset['train'].shuffle(seed=seed_).select(range(args.batch_size*num_batch))\n",
    "valid = train\n",
    "# # valid = dataset['validation'].shuffle(seed=seed_).select(range(args.valid_num_points))\n",
    "test = dataset['validation'].shuffle(seed=seed_)#.select(range(args.test_num_points))#[L_t+L_v:L_t+L_v+L_test]\n",
    "\n",
    "# #TODO: Syn_input should be monolingual data, should try en-fo's en. cuz wmt may align\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "\n",
    "'''\n",
    "each mini batch consist of : \n",
    "1. data to train W\n",
    "2. monolingual data to generate parallel data\n",
    "3. data to train V\n",
    "4. data to train A\n",
    "'''\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "# logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "Input shape: \n",
      "torch.Size([30000, 67]) torch.Size([30000, 67]) torch.Size([30000])\n",
      "06/29 02:15:23 PM |\t  train data get\n",
      "06/29 02:15:23 PM |\t  train data loader get\n",
      "get train data start\n",
      "Input shape: \n",
      "torch.Size([30000, 67]) torch.Size([30000, 67]) torch.Size([30000])\n",
      "06/29 02:15:27 PM |\t  valid data loader get\n",
      "get train data start\n",
      "Input shape: \n",
      "torch.Size([872, 63]) torch.Size([872, 63]) torch.Size([872])\n",
      "06/29 02:15:27 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = get_data(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_data(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_data(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# B = load()[0]\n",
    "# A.load_state_dict(B)\n",
    "\n",
    "\n",
    "A = attention_params(tokenizer.vocab_size - 1, args)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w =  ClassifierModel(tokenizer.vocab_size - 1, args,'teacher')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(),  lr= args.w_lr ,  betas=(args.beta1, args.beta2) ,eps=1e-8 )\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_w  =   StepLR(w_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "\n",
    "model_v = ClassifierModel(tokenizer.vocab_size - 1, args,'student')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(),  lr= args.v_lr ,  betas=(args.beta1,args.beta2) ,eps=1e-8  )\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_v  =   StepLR(v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    objs_top1 = AvgrageMeter()\n",
    "    objs_top5 = AvgrageMeter()\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        logits,ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,model)\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        n = test_dataloaderx.shape[0]\n",
    "        prec1, prec5 = accuracy(logits, test_dataloadery, topk=(1, 1))\n",
    "                \n",
    "        objs_top1.update(prec1.item(), n)\n",
    "        \n",
    "        objs_top5.update(prec5.item(), n)\n",
    "            \n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    logging.info('%s top1 : %f',model.name,objs_top1.avg)\n",
    "    objs_top1.reset()\n",
    "    logging.info('%s top5 : %f',model.name,objs_top5.avg)\n",
    "    objs_top5.reset()\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    model.train()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_train(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter):\n",
    "    fn = torch.nn.CrossEntropyLoss()\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_w_top1 = AvgrageMeter()\n",
    "    objs_w_top5 = AvgrageMeter()\n",
    "    objs_v_top1 = AvgrageMeter()\n",
    "    objs_v_top5 = AvgrageMeter()\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.train_w_num_points\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points\n",
    "    Asize = args.train_A_num_points\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size = [wsize, synsize, vsize, Asize]\n",
    "    bs = args.batch_size\n",
    "    w_model.train()\n",
    "    v_model.train()\n",
    "\n",
    "\n",
    "    logging.info(f\"split size:{split_size}\")\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        tot_iter[0] += bs\n",
    "        \n",
    "\n",
    "        # logging.info(f\"GPU mem :{getGPUMem(device)}%\")\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        (input_w, input_syn, input_v, input_A_v) = torch.split(train_x, split_size)\n",
    "        (input_w_attn, input_syn_attn, input_v_attn,\n",
    "         input_A_v_attn) = torch.split(train_x_attn, split_size)\n",
    "        (output_w, _, output_v, output_A_v) = torch.split(train_y, split_size)\n",
    "            \n",
    "        if(True):# let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "        # input_w[step%wsize]+=1 # noise input\n",
    "        \n",
    "        # if (args.train_A == 1 and epoch>=args.pre_epochs):\n",
    "        #     epsilon_w = args.unrolled_w_lr\n",
    "        #     epsilon_v  = args.unrolled_v_lr\n",
    "        #     v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, w_optimizer,\n",
    "        #                                      input_v, input_v_attn, output_v, input_syn, input_syn_attn,\n",
    "        #                                      input_A_v, input_A_v_attn, output_A_v, v_optimizer,\n",
    "        #                                      epsilon_w, epsilon_v,args.grad_clip)\n",
    "        #     objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "                   \n",
    "        w_optimizer.zero_grad()\n",
    "        logits,loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                          A, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "        # input_w[step%wsize]-=1\n",
    "        torch.nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "        prec1, prec5 = accuracy(logits, output_w, topk=(1, 1))\n",
    "        objs_w_top1.update(prec1.item(), wsize)\n",
    "        objs_w_top5.update(prec5.item(), wsize)\n",
    "\n",
    "\n",
    "        if(epoch>=args.pre_epochs):  \n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "            logits,loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                             v_model)\n",
    "            v_loss = (args.traindata_loss_ratio*loss +\n",
    "                    loss_aug*args.syndata_loss_ratio)\n",
    "            v_loss.backward()\n",
    "            objs_v_syn.update(loss_aug.item(), synsize)\n",
    "            objs_v_train.update(loss.item(), vsize)\n",
    "            v_optimizer.step()\n",
    "        \n",
    "            torch.nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "            prec1, prec5 = accuracy(logits, output_v, topk=(1, 1))\n",
    "            objs_v_top1.update(prec1.item(), vsize)\n",
    "            objs_v_top5.update(prec5.item(), vsize)\n",
    "                \n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            my_test(validdataloader, model_w, epoch)\n",
    "            my_test(validdataloader, model_v, epoch)\n",
    "            # logging.info(str((\"Attention Weights A : \", A.ReLU(A.alpha))))\n",
    "            # torch.save(model_w,'./model/'+'model_w.pt')#+now+\n",
    "            # torch.save(model_v,'./model/'+'model_v.pt')\n",
    "            # torch.save(A,'./model/'+'A.pt')\n",
    "            # torch.save(model_w.state_dict(),os.path.join(wandb.run.dir, \"model_w.pt\"))\n",
    "            # torch.save(model_v.state_dict(),os.path.join(wandb.run.dir, \"model_v.pt\"))\n",
    "            # torch.save(A.state_dict(),os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "            # wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "\n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info(f\"{progress:5.3}%:\\t  W_train_loss:{objs_w.avg:^.7f}\\tV_train_syn_loss:{objs_v_syn.avg:^.7f}\\tV_train_loss:{objs_v_train.avg:^.7f}\\t  V_star_val_loss:{objs_v_star_val.avg:^.7f}\\t w_top1:{objs_w_top1.avg:^.7f}\\t  w_top5:{objs_w_top5.avg:^.7f}\\t v_top1:{objs_v_top1.avg:^.7f}\\t v_top5:{objs_v_top5.avg:^.7f}\\t \")\n",
    "            with torch.no_grad():\n",
    "                temp = A(input_w)\n",
    "            logging.info(f\"weight:{temp}\")\n",
    "            logging.info(f'noise input weight:{temp[step%wsize]}')\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_w_top1.reset()\n",
    "            objs_w_top5.reset()\n",
    "    return w_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/29 02:15:31 PM |\t  teacher test loss : 0.693272\n",
      "06/29 02:15:31 PM |\t  teacher top1 : 50.917430\n",
      "06/29 02:15:31 PM |\t  teacher top5 : 50.917430\n",
      "06/29 02:15:31 PM |\t  teacher test loss : 0.693272\n",
      "06/29 02:15:32 PM |\t  student test loss : 0.697274\n",
      "06/29 02:15:32 PM |\t  student top1 : 49.082568\n",
      "06/29 02:15:32 PM |\t  student top5 : 49.082568\n",
      "06/29 02:15:32 PM |\t  student test loss : 0.697274\n",
      "06/29 02:15:32 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:15:32 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:15:39 PM |\t   8.25%:\t  W_train_loss:0.6832553\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:57.9326923\t  w_top5:57.9326923\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:15:39 PM |\t  weight:tensor([0.9785, 1.0202, 0.9971, 1.0140, 1.0302, 1.0038, 0.9650, 0.9911],\n",
      "       device='cuda:0')\n",
      "06/29 02:15:39 PM |\t  noise input weight:0.9911225438117981\n",
      "06/29 02:15:45 PM |\t   16.6%:\t  W_train_loss:0.6931725\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:53.6057692\t  w_top5:53.6057692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:15:45 PM |\t  weight:tensor([0.9910, 0.9854, 1.0075, 1.0402, 0.9654, 0.9919, 1.0164, 1.0023],\n",
      "       device='cuda:0')\n",
      "06/29 02:15:45 PM |\t  noise input weight:1.0022811889648438\n",
      "06/29 02:15:52 PM |\t   24.9%:\t  W_train_loss:0.6877737\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:57.2115385\t  w_top5:57.2115385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:15:52 PM |\t  weight:tensor([1.0182, 0.9843, 1.0191, 1.0080, 0.9945, 0.9694, 1.0040, 1.0025],\n",
      "       device='cuda:0')\n",
      "06/29 02:15:52 PM |\t  noise input weight:1.0025265216827393\n",
      "06/29 02:15:59 PM |\t   33.2%:\t  W_train_loss:0.6878979\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:55.6490385\t  w_top5:55.6490385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:15:59 PM |\t  weight:tensor([0.9998, 0.9714, 1.0037, 1.0197, 1.0008, 1.0226, 0.9723, 1.0096],\n",
      "       device='cuda:0')\n",
      "06/29 02:15:59 PM |\t  noise input weight:1.009626030921936\n",
      "06/29 02:16:06 PM |\t   41.6%:\t  W_train_loss:0.6883830\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:54.9278846\t  w_top5:54.9278846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:06 PM |\t  weight:tensor([0.9914, 1.0167, 1.0079, 0.9554, 0.9949, 1.0140, 0.9979, 1.0218],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:06 PM |\t  noise input weight:1.021796703338623\n",
      "06/29 02:16:13 PM |\t   49.9%:\t  W_train_loss:0.6868508\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:56.7307692\t  w_top5:56.7307692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:13 PM |\t  weight:tensor([0.9867, 0.9787, 1.0296, 0.9803, 1.0265, 1.0060, 0.9641, 1.0283],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:13 PM |\t  noise input weight:1.0282686948776245\n",
      "06/29 02:16:20 PM |\t   58.2%:\t  W_train_loss:0.6861974\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:56.2500000\t  w_top5:56.2500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:20 PM |\t  weight:tensor([1.0300, 1.0026, 1.0016, 0.9862, 0.9881, 0.9696, 1.0170, 1.0049],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:20 PM |\t  noise input weight:1.0049424171447754\n",
      "06/29 02:16:27 PM |\t   66.5%:\t  W_train_loss:0.6836441\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:57.6923077\t  w_top5:57.6923077\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:27 PM |\t  weight:tensor([0.9590, 1.0368, 0.9818, 0.9684, 1.0208, 1.0042, 1.0395, 0.9894],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:27 PM |\t  noise input weight:0.9894284009933472\n",
      "06/29 02:16:34 PM |\t   74.9%:\t  W_train_loss:0.6817098\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:58.0528846\t  w_top5:58.0528846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:34 PM |\t  weight:tensor([0.9893, 1.0064, 0.9813, 1.0010, 1.0080, 0.9892, 1.0076, 1.0172],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:34 PM |\t  noise input weight:1.0171513557434082\n",
      "06/29 02:16:41 PM |\t   83.2%:\t  W_train_loss:0.6911215\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:52.5240385\t  w_top5:52.5240385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:41 PM |\t  weight:tensor([1.0292, 0.9914, 0.9815, 0.9840, 0.9932, 1.0267, 1.0077, 0.9863],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:41 PM |\t  noise input weight:0.9862525463104248\n",
      "06/29 02:16:48 PM |\t   91.5%:\t  W_train_loss:0.6843461\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:56.2500000\t  w_top5:56.2500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:48 PM |\t  weight:tensor([1.0226, 0.9821, 1.0209, 0.9940, 0.9799, 1.0379, 1.0039, 0.9586],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:48 PM |\t  noise input weight:0.9586207866668701\n",
      "06/29 02:16:55 PM |\t   99.8%:\t  W_train_loss:0.6907572\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:54.9278846\t  w_top5:54.9278846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:16:55 PM |\t  weight:tensor([1.0047, 0.9716, 1.0019, 1.0398, 1.0221, 0.9843, 0.9905, 0.9851],\n",
      "       device='cuda:0')\n",
      "06/29 02:16:55 PM |\t  noise input weight:0.9851043820381165\n",
      "06/29 02:16:55 PM |\t  teacher test loss : 0.693504\n",
      "06/29 02:16:55 PM |\t  teacher top1 : 50.917430\n",
      "06/29 02:16:55 PM |\t  teacher top5 : 50.917430\n",
      "06/29 02:16:55 PM |\t  teacher test loss : 0.693504\n",
      "06/29 02:16:56 PM |\t  student test loss : 0.697274\n",
      "06/29 02:16:56 PM |\t  student top1 : 49.082568\n",
      "06/29 02:16:56 PM |\t  student top5 : 49.082568\n",
      "06/29 02:16:56 PM |\t  student test loss : 0.697274\n",
      "06/29 02:16:56 PM |\t  w_train_loss:858.8739313483238\n",
      "06/29 02:16:56 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:16:56 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:17:03 PM |\t   8.09%:\t  W_train_loss:0.6819654\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:58.3333333\t  w_top5:58.3333333\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:03 PM |\t  weight:tensor([0.9857, 0.9826, 0.9758, 0.9695, 1.0019, 1.0487, 1.0242, 1.0116],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:03 PM |\t  noise input weight:1.0486890077590942\n",
      "06/29 02:17:09 PM |\t   16.4%:\t  W_train_loss:0.6887273\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:53.6057692\t  w_top5:53.6057692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:09 PM |\t  weight:tensor([0.9848, 1.0158, 0.9654, 0.9636, 0.9901, 1.0001, 1.0601, 1.0202],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:09 PM |\t  noise input weight:1.0000603199005127\n",
      "06/29 02:17:16 PM |\t   24.7%:\t  W_train_loss:0.6736344\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:58.2932692\t  w_top5:58.2932692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:16 PM |\t  weight:tensor([0.9813, 0.9843, 1.0327, 1.0044, 0.9884, 0.9780, 1.0254, 1.0056],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:16 PM |\t  noise input weight:0.9779834747314453\n",
      "06/29 02:17:23 PM |\t   33.1%:\t  W_train_loss:0.6792954\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:56.2500000\t  w_top5:56.2500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:23 PM |\t  weight:tensor([1.0001, 1.0145, 1.0107, 0.9883, 1.0013, 0.9717, 1.0150, 0.9985],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:23 PM |\t  noise input weight:0.9716712236404419\n",
      "06/29 02:17:30 PM |\t   41.4%:\t  W_train_loss:0.6705100\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:59.0144231\t  w_top5:59.0144231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:30 PM |\t  weight:tensor([0.9821, 1.0276, 1.0129, 1.0218, 1.0130, 1.0024, 0.9810, 0.9591],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:30 PM |\t  noise input weight:1.0024266242980957\n",
      "06/29 02:17:37 PM |\t   49.7%:\t  W_train_loss:0.6695823\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:60.0961538\t  w_top5:60.0961538\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:37 PM |\t  weight:tensor([0.9903, 0.9933, 0.9848, 1.0059, 0.9892, 1.0056, 0.9991, 1.0317],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:37 PM |\t  noise input weight:1.0056450366973877\n",
      "06/29 02:17:44 PM |\t   58.0%:\t  W_train_loss:0.6380316\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:63.7019231\t  w_top5:63.7019231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:44 PM |\t  weight:tensor([0.9864, 0.9995, 1.0295, 0.9900, 1.0459, 1.0020, 0.9797, 0.9668],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:44 PM |\t  noise input weight:1.0020338296890259\n",
      "06/29 02:17:51 PM |\t   66.4%:\t  W_train_loss:0.6595239\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:64.9038462\t  w_top5:64.9038462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:51 PM |\t  weight:tensor([0.9707, 1.0017, 1.0078, 1.0089, 1.0193, 0.9693, 1.0143, 1.0080],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:51 PM |\t  noise input weight:0.9692874550819397\n",
      "06/29 02:17:58 PM |\t   74.7%:\t  W_train_loss:0.6464727\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:65.3846154\t  w_top5:65.3846154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:17:58 PM |\t  weight:tensor([0.9972, 0.9709, 1.0210, 0.9934, 0.9879, 1.0274, 0.9985, 1.0035],\n",
      "       device='cuda:0')\n",
      "06/29 02:17:58 PM |\t  noise input weight:1.027431607246399\n",
      "06/29 02:18:05 PM |\t   83.0%:\t  W_train_loss:0.6402939\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:63.4615385\t  w_top5:63.4615385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:05 PM |\t  weight:tensor([1.0132, 1.0068, 1.0038, 0.9838, 1.0160, 0.9862, 0.9668, 1.0234],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:05 PM |\t  noise input weight:0.9861679673194885\n",
      "06/29 02:18:12 PM |\t   91.4%:\t  W_train_loss:0.6478528\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:63.5817308\t  w_top5:63.5817308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:12 PM |\t  weight:tensor([0.9787, 1.0027, 1.0090, 0.9913, 1.0268, 1.0024, 1.0045, 0.9846],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:12 PM |\t  noise input weight:1.0023771524429321\n",
      "06/29 02:18:19 PM |\t   99.7%:\t  W_train_loss:0.6662478\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:60.5769231\t  w_top5:60.5769231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:19 PM |\t  weight:tensor([1.0083, 0.9871, 1.0080, 1.0006, 0.9834, 0.9883, 1.0203, 1.0040],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:19 PM |\t  noise input weight:0.9882967472076416\n",
      "06/29 02:18:19 PM |\t  teacher test loss : 0.671095\n",
      "06/29 02:18:19 PM |\t  teacher top1 : 59.174310\n",
      "06/29 02:18:19 PM |\t  teacher top5 : 59.174310\n",
      "06/29 02:18:19 PM |\t  teacher test loss : 0.671095\n",
      "06/29 02:18:20 PM |\t  student test loss : 0.697274\n",
      "06/29 02:18:20 PM |\t  student top1 : 49.082568\n",
      "06/29 02:18:20 PM |\t  student top5 : 49.082568\n",
      "06/29 02:18:20 PM |\t  student test loss : 0.697274\n",
      "06/29 02:18:20 PM |\t  w_train_loss:829.3704234957695\n",
      "06/29 02:18:20 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:18:20 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:18:26 PM |\t   7.93%:\t  W_train_loss:0.6136209\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:68.7500000\t  w_top5:68.7500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:26 PM |\t  weight:tensor([0.9831, 0.9777, 1.0070, 1.0462, 0.9896, 1.0088, 1.0170, 0.9705],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:26 PM |\t  noise input weight:1.0462377071380615\n",
      "06/29 02:18:33 PM |\t   16.3%:\t  W_train_loss:0.6070555\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:69.8317308\t  w_top5:69.8317308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:33 PM |\t  weight:tensor([0.9821, 1.0111, 0.9739, 1.0377, 0.9893, 1.0101, 1.0183, 0.9775],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:33 PM |\t  noise input weight:1.0377109050750732\n",
      "06/29 02:18:40 PM |\t   24.6%:\t  W_train_loss:0.5812285\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:72.7163462\t  w_top5:72.7163462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:40 PM |\t  weight:tensor([1.0202, 0.9921, 0.9714, 0.9896, 1.0156, 1.0053, 0.9983, 1.0074],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:40 PM |\t  noise input weight:0.9896017909049988\n",
      "06/29 02:18:47 PM |\t   32.9%:\t  W_train_loss:0.5195257\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:78.0048077\t  w_top5:78.0048077\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:47 PM |\t  weight:tensor([1.0097, 0.9753, 0.9566, 1.0171, 1.0185, 1.0117, 1.0372, 0.9738],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:47 PM |\t  noise input weight:1.0171234607696533\n",
      "06/29 02:18:54 PM |\t   41.2%:\t  W_train_loss:0.5466331\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:75.7211538\t  w_top5:75.7211538\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:18:54 PM |\t  weight:tensor([1.0291, 0.9756, 1.0364, 0.9813, 0.9563, 1.0115, 0.9901, 1.0197],\n",
      "       device='cuda:0')\n",
      "06/29 02:18:54 PM |\t  noise input weight:0.9813027381896973\n",
      "06/29 02:19:01 PM |\t   49.6%:\t  W_train_loss:0.5719161\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:74.5192308\t  w_top5:74.5192308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:01 PM |\t  weight:tensor([1.0002, 1.0183, 0.9846, 1.0015, 0.9778, 1.0019, 0.9934, 1.0224],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:01 PM |\t  noise input weight:1.001524806022644\n",
      "06/29 02:19:08 PM |\t   57.9%:\t  W_train_loss:0.4983259\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:78.3653846\t  w_top5:78.3653846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:08 PM |\t  weight:tensor([0.9689, 0.9751, 1.0177, 1.0128, 0.9870, 1.0182, 1.0249, 0.9954],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:08 PM |\t  noise input weight:1.0128201246261597\n",
      "06/29 02:19:15 PM |\t   66.2%:\t  W_train_loss:0.5153130\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:76.4423077\t  w_top5:76.4423077\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:15 PM |\t  weight:tensor([1.0002, 0.9404, 0.9928, 0.9851, 1.0314, 1.0233, 1.0050, 1.0217],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:15 PM |\t  noise input weight:0.9850957989692688\n",
      "06/29 02:19:23 PM |\t   74.5%:\t  W_train_loss:0.4918975\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:78.9663462\t  w_top5:78.9663462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:23 PM |\t  weight:tensor([1.0203, 0.9858, 0.9745, 1.0447, 0.9984, 0.9957, 0.9820, 0.9986],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:23 PM |\t  noise input weight:1.0446832180023193\n",
      "06/29 02:19:30 PM |\t   82.9%:\t  W_train_loss:0.5009168\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:78.1250000\t  w_top5:78.1250000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:30 PM |\t  weight:tensor([1.0291, 0.9829, 0.9885, 0.9737, 1.0183, 0.9912, 0.9760, 1.0403],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:30 PM |\t  noise input weight:0.973729133605957\n",
      "06/29 02:19:37 PM |\t   91.2%:\t  W_train_loss:0.4784672\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:81.2500000\t  w_top5:81.2500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:37 PM |\t  weight:tensor([1.0201, 1.0139, 1.0258, 1.0092, 0.9840, 0.9805, 0.9932, 0.9734],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:37 PM |\t  noise input weight:1.0092352628707886\n",
      "06/29 02:19:44 PM |\t   99.5%:\t  W_train_loss:0.4911351\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:78.9663462\t  w_top5:78.9663462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:44 PM |\t  weight:tensor([1.0161, 0.9607, 1.0336, 0.9988, 1.0135, 1.0280, 0.9775, 0.9719],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:44 PM |\t  noise input weight:0.9988452792167664\n",
      "06/29 02:19:45 PM |\t  teacher test loss : 0.624140\n",
      "06/29 02:19:45 PM |\t  teacher top1 : 67.316511\n",
      "06/29 02:19:45 PM |\t  teacher top5 : 67.316511\n",
      "06/29 02:19:45 PM |\t  teacher test loss : 0.624140\n",
      "06/29 02:19:46 PM |\t  student test loss : 0.697274\n",
      "06/29 02:19:46 PM |\t  student top1 : 49.082568\n",
      "06/29 02:19:46 PM |\t  student top5 : 49.082568\n",
      "06/29 02:19:46 PM |\t  student test loss : 0.697274\n",
      "06/29 02:19:46 PM |\t  w_train_loss:667.7545400559902\n",
      "06/29 02:19:46 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:19:46 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:19:52 PM |\t   7.77%:\t  W_train_loss:0.4588307\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:81.6326531\t  w_top5:81.6326531\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:19:52 PM |\t  weight:tensor([1.0245, 0.9674, 1.0290, 0.9867, 1.0018, 1.0212, 0.9989, 0.9705],\n",
      "       device='cuda:0')\n",
      "06/29 02:19:52 PM |\t  noise input weight:0.9673506617546082\n",
      "06/29 02:20:00 PM |\t   16.1%:\t  W_train_loss:0.4415097\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:82.2115385\t  w_top5:82.2115385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:00 PM |\t  weight:tensor([0.9988, 0.9950, 1.0119, 0.9752, 1.0175, 1.0199, 0.9820, 0.9995],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:00 PM |\t  noise input weight:0.9950243830680847\n",
      "06/29 02:20:07 PM |\t   24.4%:\t  W_train_loss:0.4105389\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:83.1730769\t  w_top5:83.1730769\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:07 PM |\t  weight:tensor([1.0150, 0.9940, 1.0175, 1.0039, 1.0047, 1.0119, 0.9997, 0.9533],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:07 PM |\t  noise input weight:0.9940385818481445\n",
      "06/29 02:20:14 PM |\t   32.7%:\t  W_train_loss:0.3752561\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:85.9375000\t  w_top5:85.9375000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:14 PM |\t  weight:tensor([1.0251, 1.0103, 0.9990, 1.0351, 0.9780, 0.9740, 1.0032, 0.9754],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:14 PM |\t  noise input weight:1.0102579593658447\n",
      "06/29 02:20:21 PM |\t   41.1%:\t  W_train_loss:0.4012800\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:84.3750000\t  w_top5:84.3750000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:21 PM |\t  weight:tensor([0.9919, 0.9873, 1.0148, 0.9942, 1.0068, 1.0051, 0.9999, 1.0000],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:21 PM |\t  noise input weight:0.9872934818267822\n",
      "06/29 02:20:28 PM |\t   49.4%:\t  W_train_loss:0.4414278\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:82.3317308\t  w_top5:82.3317308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:28 PM |\t  weight:tensor([0.9930, 0.9987, 0.9988, 0.9902, 0.9921, 1.0040, 1.0220, 1.0012],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:28 PM |\t  noise input weight:0.998706042766571\n",
      "06/29 02:20:35 PM |\t   57.7%:\t  W_train_loss:0.3388682\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:88.5817308\t  w_top5:88.5817308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:35 PM |\t  weight:tensor([1.0142, 0.9978, 0.9706, 0.9795, 0.9946, 1.0438, 1.0036, 0.9959],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:35 PM |\t  noise input weight:0.9977853894233704\n",
      "06/29 02:20:42 PM |\t   66.1%:\t  W_train_loss:0.3639408\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:87.3798077\t  w_top5:87.3798077\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:42 PM |\t  weight:tensor([0.9967, 1.0206, 0.9858, 1.0245, 1.0170, 1.0030, 0.9732, 0.9792],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:42 PM |\t  noise input weight:1.0206212997436523\n",
      "06/29 02:20:49 PM |\t   74.4%:\t  W_train_loss:0.3733262\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:86.6586538\t  w_top5:86.6586538\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:49 PM |\t  weight:tensor([1.0187, 1.0130, 0.9637, 0.9680, 0.9965, 1.0123, 1.0052, 1.0225],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:49 PM |\t  noise input weight:1.0130479335784912\n",
      "06/29 02:20:57 PM |\t   82.7%:\t  W_train_loss:0.3529636\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:87.0192308\t  w_top5:87.0192308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:20:57 PM |\t  weight:tensor([1.0019, 0.9972, 1.0431, 0.9774, 1.0015, 1.0064, 0.9902, 0.9824],\n",
      "       device='cuda:0')\n",
      "06/29 02:20:57 PM |\t  noise input weight:0.9971643090248108\n",
      "06/29 02:21:04 PM |\t   91.0%:\t  W_train_loss:0.3629256\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:87.2596154\t  w_top5:87.2596154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:04 PM |\t  weight:tensor([1.0465, 1.0130, 0.9900, 0.9904, 1.0035, 0.9912, 0.9822, 0.9831],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:04 PM |\t  noise input weight:1.0129849910736084\n",
      "06/29 02:21:11 PM |\t   99.4%:\t  W_train_loss:0.3610739\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:86.6586538\t  w_top5:86.6586538\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:11 PM |\t  weight:tensor([0.9739, 1.0088, 1.0049, 1.0088, 1.0010, 0.9813, 1.0008, 1.0205],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:11 PM |\t  noise input weight:1.0087783336639404\n",
      "06/29 02:21:12 PM |\t  teacher test loss : 0.643093\n",
      "06/29 02:21:12 PM |\t  teacher top1 : 72.247704\n",
      "06/29 02:21:12 PM |\t  teacher top5 : 72.247704\n",
      "06/29 02:21:12 PM |\t  teacher test loss : 0.643093\n",
      "06/29 02:21:12 PM |\t  student test loss : 0.697274\n",
      "06/29 02:21:12 PM |\t  student top1 : 49.082568\n",
      "06/29 02:21:12 PM |\t  student top5 : 49.082568\n",
      "06/29 02:21:12 PM |\t  student test loss : 0.697274\n",
      "06/29 02:21:12 PM |\t  w_train_loss:486.92933712899685\n",
      "06/29 02:21:12 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:21:12 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:21:19 PM |\t   7.61%:\t  W_train_loss:0.3528809\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:88.0208333\t  w_top5:88.0208333\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:19 PM |\t  weight:tensor([1.0179, 1.0137, 1.0099, 1.0029, 0.9805, 0.9787, 1.0005, 0.9960],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:19 PM |\t  noise input weight:0.9959623217582703\n",
      "06/29 02:21:26 PM |\t   15.9%:\t  W_train_loss:0.3298117\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:89.0625000\t  w_top5:89.0625000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:26 PM |\t  weight:tensor([1.0049, 0.9923, 1.0144, 1.0078, 0.9905, 0.9834, 1.0015, 1.0053],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:26 PM |\t  noise input weight:1.0053154230117798\n",
      "06/29 02:21:33 PM |\t   24.3%:\t  W_train_loss:0.2888892\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.1442308\t  w_top5:90.1442308\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:33 PM |\t  weight:tensor([1.0105, 1.0290, 1.0107, 1.0028, 0.9851, 1.0263, 0.9687, 0.9668],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:33 PM |\t  noise input weight:0.9668339490890503\n",
      "06/29 02:21:40 PM |\t   32.6%:\t  W_train_loss:0.2805084\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.8653846\t  w_top5:90.8653846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:40 PM |\t  weight:tensor([0.9726, 1.0166, 0.9989, 1.0372, 0.9556, 0.9975, 1.0035, 1.0182],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:40 PM |\t  noise input weight:1.0181630849838257\n",
      "06/29 02:21:48 PM |\t   40.9%:\t  W_train_loss:0.3384203\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:87.8605769\t  w_top5:87.8605769\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:48 PM |\t  weight:tensor([0.9917, 1.0224, 1.0173, 0.9938, 0.9668, 0.9823, 1.0036, 1.0222],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:48 PM |\t  noise input weight:1.022160291671753\n",
      "06/29 02:21:55 PM |\t   49.2%:\t  W_train_loss:0.2997622\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.2644231\t  w_top5:90.2644231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:21:55 PM |\t  weight:tensor([0.9811, 0.9927, 1.0192, 1.0184, 1.0003, 0.9758, 1.0271, 0.9855],\n",
      "       device='cuda:0')\n",
      "06/29 02:21:55 PM |\t  noise input weight:0.9854884147644043\n",
      "06/29 02:22:02 PM |\t   57.6%:\t  W_train_loss:0.2384600\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:92.1875000\t  w_top5:92.1875000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:02 PM |\t  weight:tensor([0.9961, 0.9860, 1.0313, 0.9826, 1.0064, 1.0028, 1.0151, 0.9797],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:02 PM |\t  noise input weight:0.9796752333641052\n",
      "06/29 02:22:09 PM |\t   65.9%:\t  W_train_loss:0.3033162\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.2644231\t  w_top5:90.2644231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:09 PM |\t  weight:tensor([0.9457, 0.9809, 1.0541, 0.9945, 1.0055, 1.0245, 0.9917, 1.0031],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:09 PM |\t  noise input weight:1.003089189529419\n",
      "06/29 02:22:16 PM |\t   74.2%:\t  W_train_loss:0.2954139\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.6250000\t  w_top5:90.6250000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:16 PM |\t  weight:tensor([1.0258, 0.9652, 1.0324, 0.9823, 1.0012, 0.9786, 1.0114, 1.0031],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:16 PM |\t  noise input weight:1.0031017065048218\n",
      "06/29 02:22:23 PM |\t   82.5%:\t  W_train_loss:0.2794790\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.9855769\t  w_top5:90.9855769\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:23 PM |\t  weight:tensor([1.0097, 0.9965, 0.9688, 1.0330, 1.0235, 0.9600, 1.0149, 0.9935],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:23 PM |\t  noise input weight:0.9935200214385986\n",
      "06/29 02:22:30 PM |\t   90.9%:\t  W_train_loss:0.2915694\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.8653846\t  w_top5:90.8653846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:30 PM |\t  weight:tensor([1.0033, 1.0082, 0.9716, 0.9696, 1.0199, 1.0239, 0.9934, 1.0102],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:30 PM |\t  noise input weight:1.0102418661117554\n",
      "06/29 02:22:37 PM |\t   99.2%:\t  W_train_loss:0.2956673\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:89.9038462\t  w_top5:89.9038462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:37 PM |\t  weight:tensor([1.0128, 0.9817, 1.0253, 0.9951, 0.9923, 1.0111, 1.0026, 0.9790],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:37 PM |\t  noise input weight:0.9790131449699402\n",
      "06/29 02:22:38 PM |\t  teacher test loss : 0.643162\n",
      "06/29 02:22:38 PM |\t  teacher top1 : 73.394493\n",
      "06/29 02:22:38 PM |\t  teacher top5 : 73.394493\n",
      "06/29 02:22:38 PM |\t  teacher test loss : 0.643162\n",
      "06/29 02:22:39 PM |\t  student test loss : 0.697274\n",
      "06/29 02:22:39 PM |\t  student top1 : 49.082568\n",
      "06/29 02:22:39 PM |\t  student top5 : 49.082568\n",
      "06/29 02:22:39 PM |\t  student test loss : 0.697274\n",
      "06/29 02:22:39 PM |\t  w_train_loss:373.77220479026437\n",
      "06/29 02:22:39 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:22:39 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:22:45 PM |\t   7.45%:\t  W_train_loss:0.2925661\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:90.0265957\t  w_top5:90.0265957\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:46 PM |\t  weight:tensor([0.9907, 0.9815, 1.0297, 0.9818, 1.0383, 0.9633, 0.9911, 1.0237],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:46 PM |\t  noise input weight:0.9632652401924133\n",
      "06/29 02:22:53 PM |\t   15.8%:\t  W_train_loss:0.2729518\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:89.9038462\t  w_top5:89.9038462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:22:53 PM |\t  weight:tensor([1.0075, 0.9741, 0.9814, 1.0045, 0.9956, 1.0042, 1.0329, 0.9998],\n",
      "       device='cuda:0')\n",
      "06/29 02:22:53 PM |\t  noise input weight:1.0042099952697754\n",
      "06/29 02:23:00 PM |\t   24.1%:\t  W_train_loss:0.2503261\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:91.1057692\t  w_top5:91.1057692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:00 PM |\t  weight:tensor([0.9989, 1.0178, 0.9910, 0.9901, 1.0305, 1.0010, 0.9714, 0.9994],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:00 PM |\t  noise input weight:1.0009548664093018\n",
      "06/29 02:23:08 PM |\t   32.4%:\t  W_train_loss:0.1970933\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:94.1105769\t  w_top5:94.1105769\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:08 PM |\t  weight:tensor([1.0116, 0.9802, 0.9786, 0.9984, 1.0076, 1.0005, 1.0157, 1.0074],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:08 PM |\t  noise input weight:1.0004931688308716\n",
      "06/29 02:23:15 PM |\t   40.8%:\t  W_train_loss:0.2461267\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:92.3076923\t  w_top5:92.3076923\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:15 PM |\t  weight:tensor([0.9904, 1.0012, 0.9978, 1.0137, 0.9851, 0.9903, 1.0014, 1.0201],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:15 PM |\t  noise input weight:0.9903203845024109\n",
      "06/29 02:23:22 PM |\t   49.1%:\t  W_train_loss:0.2455365\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:92.5480769\t  w_top5:92.5480769\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:22 PM |\t  weight:tensor([0.9850, 1.0158, 0.9926, 1.0042, 0.9993, 0.9896, 0.9965, 1.0170],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:22 PM |\t  noise input weight:0.9895837306976318\n",
      "06/29 02:23:29 PM |\t   57.4%:\t  W_train_loss:0.1870806\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:94.5913462\t  w_top5:94.5913462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:29 PM |\t  weight:tensor([0.9998, 1.0096, 1.0038, 1.0104, 1.0081, 0.9770, 0.9822, 1.0090],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:29 PM |\t  noise input weight:0.9770020246505737\n",
      "06/29 02:23:36 PM |\t   65.7%:\t  W_train_loss:0.2298752\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:92.9086538\t  w_top5:92.9086538\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:36 PM |\t  weight:tensor([0.9596, 1.0352, 0.9796, 0.9982, 1.0588, 1.0207, 0.9662, 0.9816],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:36 PM |\t  noise input weight:1.0207180976867676\n",
      "06/29 02:23:43 PM |\t   74.1%:\t  W_train_loss:0.2222959\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.3894231\t  w_top5:93.3894231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:43 PM |\t  weight:tensor([1.0107, 0.9652, 0.9830, 1.0206, 0.9972, 0.9860, 1.0058, 1.0315],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:43 PM |\t  noise input weight:0.9860300421714783\n",
      "06/29 02:23:51 PM |\t   82.4%:\t  W_train_loss:0.1946832\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.7500000\t  w_top5:93.7500000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:51 PM |\t  weight:tensor([0.9977, 0.9745, 0.9933, 1.0203, 1.0338, 1.0147, 0.9575, 1.0082],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:51 PM |\t  noise input weight:1.0147366523742676\n",
      "06/29 02:23:58 PM |\t   90.7%:\t  W_train_loss:0.1982267\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.5096154\t  w_top5:93.5096154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:23:58 PM |\t  weight:tensor([1.0209, 0.9823, 0.9836, 1.0315, 1.0036, 0.9964, 0.9846, 0.9971],\n",
      "       device='cuda:0')\n",
      "06/29 02:23:58 PM |\t  noise input weight:0.9963659048080444\n",
      "06/29 02:24:05 PM |\t   99.0%:\t  W_train_loss:0.2267353\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.5096154\t  w_top5:93.5096154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:05 PM |\t  weight:tensor([0.9642, 0.9729, 0.9805, 1.0156, 1.0131, 1.0209, 1.0225, 1.0102],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:05 PM |\t  noise input weight:1.0209428071975708\n",
      "06/29 02:24:06 PM |\t  teacher test loss : 0.692229\n",
      "06/29 02:24:06 PM |\t  teacher top1 : 72.362382\n",
      "06/29 02:24:06 PM |\t  teacher top5 : 72.362382\n",
      "06/29 02:24:06 PM |\t  teacher test loss : 0.692229\n",
      "06/29 02:24:07 PM |\t  student test loss : 0.697274\n",
      "06/29 02:24:07 PM |\t  student top1 : 49.082568\n",
      "06/29 02:24:07 PM |\t  student top5 : 49.082568\n",
      "06/29 02:24:07 PM |\t  student test loss : 0.697274\n",
      "06/29 02:24:07 PM |\t  w_train_loss:286.8522201217711\n",
      "06/29 02:24:07 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:24:07 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:24:13 PM |\t   7.29%:\t  W_train_loss:0.2021322\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:94.1576087\t  w_top5:94.1576087\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:13 PM |\t  weight:tensor([0.9868, 1.0268, 1.0132, 0.9848, 1.0018, 1.0098, 1.0089, 0.9679],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:13 PM |\t  noise input weight:0.984798014163971\n",
      "06/29 02:24:20 PM |\t   15.6%:\t  W_train_loss:0.1937836\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.9903846\t  w_top5:93.9903846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:20 PM |\t  weight:tensor([0.9723, 1.0317, 1.0162, 0.9705, 0.9517, 1.0309, 1.0154, 1.0113],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:20 PM |\t  noise input weight:0.9704935550689697\n",
      "06/29 02:24:27 PM |\t   23.9%:\t  W_train_loss:0.1752653\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.0721154\t  w_top5:95.0721154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:27 PM |\t  weight:tensor([0.9950, 1.0018, 1.0023, 0.9872, 0.9999, 1.0081, 0.9931, 1.0124],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:27 PM |\t  noise input weight:0.9872127771377563\n",
      "06/29 02:24:34 PM |\t   32.3%:\t  W_train_loss:0.1384955\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.7932692\t  w_top5:95.7932692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:34 PM |\t  weight:tensor([0.9842, 0.9694, 1.0161, 1.0136, 0.9897, 0.9905, 1.0208, 1.0156],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:34 PM |\t  noise input weight:1.0135884284973145\n",
      "06/29 02:24:41 PM |\t   40.6%:\t  W_train_loss:0.1954741\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:93.3894231\t  w_top5:93.3894231\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:41 PM |\t  weight:tensor([1.0190, 0.9746, 0.9752, 1.0035, 0.9947, 1.0212, 0.9860, 1.0260],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:41 PM |\t  noise input weight:1.0034687519073486\n",
      "06/29 02:24:48 PM |\t   48.9%:\t  W_train_loss:0.1859051\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:94.5913462\t  w_top5:94.5913462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:48 PM |\t  weight:tensor([1.0289, 0.9960, 0.9828, 0.9999, 1.0008, 1.0409, 0.9719, 0.9787],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:48 PM |\t  noise input weight:0.9998970627784729\n",
      "06/29 02:24:55 PM |\t   57.2%:\t  W_train_loss:0.1514901\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.3125000\t  w_top5:95.3125000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:24:55 PM |\t  weight:tensor([1.0117, 0.9728, 1.0030, 1.0212, 0.9983, 1.0325, 0.9969, 0.9636],\n",
      "       device='cuda:0')\n",
      "06/29 02:24:55 PM |\t  noise input weight:1.0211641788482666\n",
      "06/29 02:25:02 PM |\t   65.6%:\t  W_train_loss:0.1596288\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.7932692\t  w_top5:95.7932692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:02 PM |\t  weight:tensor([0.9927, 1.0266, 0.9992, 1.0065, 1.0093, 0.9853, 1.0106, 0.9697],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:02 PM |\t  noise input weight:1.0065053701400757\n",
      "06/29 02:25:09 PM |\t   73.9%:\t  W_train_loss:0.1595723\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.5528846\t  w_top5:95.5528846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:09 PM |\t  weight:tensor([0.9689, 1.0150, 1.0085, 1.0200, 0.9826, 0.9870, 1.0049, 1.0130],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:09 PM |\t  noise input weight:1.0200222730636597\n",
      "06/29 02:25:16 PM |\t   82.2%:\t  W_train_loss:0.1703915\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.3125000\t  w_top5:95.3125000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:16 PM |\t  weight:tensor([0.9981, 1.0178, 1.0292, 0.9775, 1.0000, 0.9985, 0.9813, 0.9977],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:16 PM |\t  noise input weight:0.9774718284606934\n",
      "06/29 02:25:23 PM |\t   90.6%:\t  W_train_loss:0.1361932\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.7932692\t  w_top5:95.7932692\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:23 PM |\t  weight:tensor([0.9941, 0.9999, 0.9936, 0.9964, 1.0289, 1.0003, 1.0164, 0.9705],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:23 PM |\t  noise input weight:0.9963822364807129\n",
      "06/29 02:25:30 PM |\t   98.9%:\t  W_train_loss:0.1547920\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.9134615\t  w_top5:95.9134615\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:30 PM |\t  weight:tensor([0.9739, 1.0072, 0.9970, 0.9892, 1.0301, 1.0273, 0.9726, 1.0027],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:30 PM |\t  noise input weight:0.9891602993011475\n",
      "06/29 02:25:31 PM |\t  teacher test loss : 0.878745\n",
      "06/29 02:25:31 PM |\t  teacher top1 : 73.394493\n",
      "06/29 02:25:31 PM |\t  teacher top5 : 73.394493\n",
      "06/29 02:25:31 PM |\t  teacher test loss : 0.878745\n",
      "06/29 02:25:31 PM |\t  student test loss : 0.697274\n",
      "06/29 02:25:31 PM |\t  student top1 : 49.082568\n",
      "06/29 02:25:31 PM |\t  student top5 : 49.082568\n",
      "06/29 02:25:31 PM |\t  student test loss : 0.697274\n",
      "06/29 02:25:31 PM |\t  w_train_loss:210.0013848040253\n",
      "06/29 02:25:31 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:0.001,\t\tlr_v:0.001,\t\tlr_A:0----------------\n",
      "06/29 02:25:31 PM |\t  split size:[8, 8, 0, 8]\n",
      "06/29 02:25:38 PM |\t   7.13%:\t  W_train_loss:0.1710968\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.1388889\t  w_top5:95.1388889\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:38 PM |\t  weight:tensor([1.0005, 0.9896, 1.0162, 1.0212, 0.9802, 0.9895, 1.0220, 0.9808],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:38 PM |\t  noise input weight:0.9895740151405334\n",
      "06/29 02:25:45 PM |\t   15.5%:\t  W_train_loss:0.1476132\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:96.1538462\t  w_top5:96.1538462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:45 PM |\t  weight:tensor([0.9758, 1.0146, 0.9839, 1.0038, 1.0217, 1.0163, 0.9884, 0.9956],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:45 PM |\t  noise input weight:1.014577865600586\n",
      "06/29 02:25:51 PM |\t   23.8%:\t  W_train_loss:0.1442645\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:96.2740385\t  w_top5:96.2740385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:51 PM |\t  weight:tensor([1.0097, 0.9772, 0.9888, 0.9834, 0.9995, 1.0184, 1.0391, 0.9839],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:51 PM |\t  noise input weight:0.9772214889526367\n",
      "06/29 02:25:58 PM |\t   32.1%:\t  W_train_loss:0.1368445\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:96.2740385\t  w_top5:96.2740385\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:25:58 PM |\t  weight:tensor([0.9952, 0.9897, 1.0082, 0.9682, 1.0352, 1.0370, 0.9848, 0.9817],\n",
      "       device='cuda:0')\n",
      "06/29 02:25:58 PM |\t  noise input weight:0.9897059798240662\n",
      "06/29 02:26:05 PM |\t   40.4%:\t  W_train_loss:0.1687872\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.0721154\t  w_top5:95.0721154\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:26:05 PM |\t  weight:tensor([1.0015, 1.0137, 0.9756, 1.0034, 0.9929, 1.0100, 1.0073, 0.9956],\n",
      "       device='cuda:0')\n",
      "06/29 02:26:05 PM |\t  noise input weight:1.013738751411438\n",
      "06/29 02:26:13 PM |\t   48.8%:\t  W_train_loss:0.1748332\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:94.5913462\t  w_top5:94.5913462\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:26:13 PM |\t  weight:tensor([0.9929, 0.9657, 1.0166, 1.0301, 1.0175, 1.0117, 0.9658, 0.9997],\n",
      "       device='cuda:0')\n",
      "06/29 02:26:13 PM |\t  noise input weight:0.9656816124916077\n",
      "06/29 02:26:24 PM |\t   57.1%:\t  W_train_loss:0.1260611\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:96.8750000\t  w_top5:96.8750000\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:26:24 PM |\t  weight:tensor([0.9947, 0.9751, 1.0050, 1.0528, 0.9831, 1.0234, 0.9795, 0.9864],\n",
      "       device='cuda:0')\n",
      "06/29 02:26:24 PM |\t  noise input weight:0.9751486778259277\n",
      "06/29 02:26:32 PM |\t   65.4%:\t  W_train_loss:0.1300870\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:96.7548077\t  w_top5:96.7548077\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:26:32 PM |\t  weight:tensor([0.9932, 0.9984, 1.0024, 0.9908, 1.0178, 0.9976, 1.0023, 0.9976],\n",
      "       device='cuda:0')\n",
      "06/29 02:26:32 PM |\t  noise input weight:0.998394787311554\n",
      "06/29 02:26:40 PM |\t   73.7%:\t  W_train_loss:0.1612271\tV_train_syn_loss:0.0000000\tV_train_loss:0.0000000\t  V_star_val_loss:0.0000000\t w_top1:95.5528846\t  w_top5:95.5528846\t v_top1:0.0000000\t v_top5:0.0000000\t \n",
      "06/29 02:26:40 PM |\t  weight:tensor([1.0265, 1.0107, 0.9706, 0.9996, 1.0227, 1.0220, 0.9752, 0.9727],\n",
      "       device='cuda:0')\n",
      "06/29 02:26:40 PM |\t  noise input weight:1.0107203722000122\n"
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(test_dataloader,model_w,-1) #before train\n",
    "    my_test(test_dataloader,model_v,-1)  \n",
    "\n",
    "tot_iter = [0]\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss =  my_train(epoch, train_dataloader, test_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v,tot_iter)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "    architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss}\")\n",
    "    # wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('./model/opt1.pt')\n",
    "b = torch.load('./model/opt2.pt')\n",
    "# compare_model(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
