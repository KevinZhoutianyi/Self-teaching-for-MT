{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from model import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import  AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = -1, help='validation data number')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default = 400, help='train data number')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default = 2000, help='train data number')\n",
    "parser.add_argument('--unlabel_num_points', type=int,           default = 2000, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = -1, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=32,     help='Batch size for test and validation')\n",
    "\n",
    "parser.add_argument('--w_bs', type=int,                         default=16,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--syn_bs', type=int,                       default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "# parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--A_bs', type=int,                         default=8,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--model_name_de2en', type=str,             default='roberta-base',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='yelp',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=-1,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=-1,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=10,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "# parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=2e-6,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=2e-6,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=100 ,   help='learning rate for A')\n",
    "# parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "# parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.999,    help='momentum')\n",
    "# parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=10,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=1,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "parser.add_argument('--freeze', type=int,                       default=0,    help='whether freeze the pretrained encoder')\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=1 ,     help='whether train A')\n",
    "parser.add_argument('--attack', type=int,                       default=0 ,     help='whether att')\n",
    "parser.add_argument('--clean_A_data', type=int,                 default=1 ,     help='whether att')\n",
    "\n",
    "# parser.add_argument('--embedding_dim', type=int,                default=300 ,     help='whether train A')\n",
    "parser.add_argument('--out_dim', type=int,                      default=2 ,     help='whether train A')\n",
    "# parser.add_argument('--hidden_size', type=int,                  default=64 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "\n",
    "args.test_num = args.test_num//args.w_bs * args.w_bs\n",
    "args.train_w_num_points= args.train_w_num_points//args.w_bs * args.w_bs\n",
    "args.train_A_num_points= args.train_A_num_points//args.A_bs * args.A_bs\n",
    "args.unlabel_num_points= args.unlabel_num_points//args.syn_bs * args.syn_bs\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size\n",
    "\n",
    "args.test_num = args.train_w_num_points #TODO: test each epoch\n",
    "args.rep_num = (args.train_w_num_points//4)//args.w_bs * args.w_bs#TODO: test each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tianyi-vol/Self-teaching-for-machine-translation/BERT/wandb/run-20220706_213211-8xi0yceu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/8xi0yceu\" target=\"_blank\">yelp</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/8xi0yceu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f113c3b5520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY'] = 'a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME'] = args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "wandb.init(project=\"Selftraining\", config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:32:20 PM |\t  Using custom data configuration default-89b305f2de5c2a24\n",
      "07/06 09:32:20 PM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-89b305f2de5c2a24/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 729.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:32:20 PM |\t  Using custom data configuration default-9e3205a4e9940313\n",
      "07/06 09:32:20 PM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-9e3205a4e9940313/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 983.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:32:21 PM |\t  Using custom data configuration default-9c4aa945ceecc93d\n",
      "07/06 09:32:21 PM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-9c4aa945ceecc93d/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 787.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:32:21 PM |\t  Using custom data configuration default-99489eb87fbc339f\n",
      "07/06 09:32:21 PM |\t  Reusing dataset json (/root/.cache/huggingface/datasets/json/default-99489eb87fbc339f/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 907.27it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "l = ['dev','test','train','unlabeled']\n",
    "dev = load_dataset('json', data_files='/tianyi-vol/yelp/dev_data.json', field='data')\n",
    "test = load_dataset('json', data_files='/tianyi-vol/yelp/test_data.json', field='data')\n",
    "train = load_dataset('json', data_files='/tianyi-vol/yelp/train_data.json', field='data')\n",
    "unlabeled = load_dataset('json', data_files='/tianyi-vol/yelp/unlabeled_data.json', field='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 25165\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 5235\n",
      "    })\n",
      "}) DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'major', 'len'],\n",
      "        num_rows: 3800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train,dev,unlabeled,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:32:37 PM |\t  modelsize:124.697433MB\n",
      "07/06 09:32:51 PM |\t  modelsize:124.697433MB\n",
      "07/06 09:33:04 PM |\t  modelsize:124.697433MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "modelname = args.model_name_teacher\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_de2en\n",
    "pretrained = AutoModelForMaskedLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/', '')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt') == False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained, pathname+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:33:04 PM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-9c4aa945ceecc93d/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-a565eec58ed7e408.arrow\n",
      "07/06 09:33:04 PM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-89b305f2de5c2a24/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-5967255ee27fdb41.arrow\n",
      "07/06 09:33:04 PM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-99489eb87fbc339f/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-5706d4fed9149c74.arrow\n",
      "07/06 09:33:04 PM |\t  Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-9e3205a4e9940313/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-7e682076f91c470f.arrow\n",
      "07/06 09:33:04 PM |\t  train len: 2400\n",
      "07/06 09:33:04 PM |\t  train_w_num_points_len and train_v_num_points_len: 400\n",
      "07/06 09:33:04 PM |\t  train_v_synthetic_num_points_len: 2000\n",
      "07/06 09:33:04 PM |\t  train_A_num_points_len: 2000\n",
      "07/06 09:33:05 PM |\t  valid len: 3800\n",
      "07/06 09:33:05 PM |\t  test len: 3800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train =train[\"train\"].shuffle(seed=seed_).select(range(args.train_w_num_points+args.train_A_num_points)) # A and W)\n",
    "valid = dev[\"train\"].shuffle(seed=seed_)#.select(range( r(args.valid_num_points, args.batch_size))) # dev\n",
    "unlabeled = unlabeled[\"train\"].shuffle(seed=seed_).select(range( args.unlabel_num_points) )# dev\n",
    "test = test[\"train\"].shuffle(seed=seed_)#.select(range( r(args.valid_num_points, args.batch_size))) # dev # test\n",
    "\n",
    "logging.info(\"train len: %d\", len(train))\n",
    "\n",
    "train_w_num_points_len = args.train_w_num_points\n",
    "\n",
    "\n",
    "\n",
    "train_v_synthetic_num_points_len = args.unlabel_num_points\n",
    "train_A_num_points_len =  args.train_A_num_points\n",
    "\n",
    "logging.info(\"train_w_num_points_len and train_v_num_points_len: %d\", train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",\n",
    "             train_v_synthetic_num_points_len)\n",
    "# logging.info(\"train_v_num_points_len: %d\", train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\", train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\", len(valid))\n",
    "logging.info(\"test len: %d\", len(test))\n",
    "# logging.info(test[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:33:07 PM |\t  train w data size:batchsize:16\t numofbatch:25\t totoal:400\n",
      "07/06 09:33:07 PM |\t  train syn data size:batchsize:8\t numofbatch:250\t totoal:2000\n",
      "07/06 09:33:07 PM |\t  train A data size:batchsize:8\t numofbatch:250\t totoal:2000\n",
      "07/06 09:33:08 PM |\t  validation data size:batchsize:32\t numofbatch:119\t totoal:3808\n",
      "07/06 09:33:09 PM |\t  test data size:batchsize:32\t numofbatch:119\t totoal:3808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_w_data = get_data_idx(train[:train_w_num_points_len], tokenizer,train_w_num_points_len)\n",
    "train_A_data = get_data_A(train[train_w_num_points_len:], tokenizer,args.clean_A_data)#TODO:use label now\n",
    "train_syn_data = get_syn_data(unlabeled, tokenizer)\n",
    "\n",
    "# indices = list(range(len(train)-train_w_num_points_len))\n",
    "\n",
    "train_w_dataloader = DataLoader(train_w_data, sampler=SequentialSampler(train_w_data),\n",
    "                              batch_size=args.w_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train w data size:{get_dataloader_size(train_w_dataloader)}')\n",
    "\n",
    "\n",
    "train_syn_dataloader = DataLoader(train_syn_data, sampler=RandomSampler(train_syn_data),\n",
    "                              batch_size=args.syn_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train syn data size:{get_dataloader_size(train_syn_dataloader)}')\n",
    "\n",
    "\n",
    "train_A_dataloader = DataLoader(train_A_data,  sampler=RandomSampler(train_A_data),\n",
    "                              batch_size=args.A_bs, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'train A data size:{get_dataloader_size(train_A_dataloader)}')\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "valid_data = get_data(valid, tokenizer)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data),\n",
    "                              batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)\n",
    "logging.info(f'validation data size:{get_dataloader_size(valid_dataloader)}')\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "test_data = get_data(test, tokenizer)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                             batch_size=args.batch_size, pin_memory=args.num_workers > 0, num_workers=args.num_workers)  # , sampler=RandomSampler(test_data)\n",
    "logging.info(f'test data size:{get_dataloader_size(test_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(tokenizer, args, train_w_num_points_len)  # half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = Model(tokenizer, args, 'teacher')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.AdamW(model_w.parameters(\n",
    "),  lr=args.w_lr,  betas=(args.beta1, args.beta2), eps=1e-8,weight_decay=1e-4)\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "\n",
    "scheduler_w = get_linear_schedule_with_warmup(w_optimizer, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "model_v = Model(tokenizer, args, 'student')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.AdamW(model_v.parameters(\n",
    "),  lr=args.v_lr,  betas=(args.beta1, args.beta2), eps=1e-8,weight_decay=1e-4)\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "\n",
    "scheduler_v = get_linear_schedule_with_warmup(v_optimizer, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)\n",
    "#  scheduler_v = StepLR(\n",
    "    # v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n",
    "architect.scheduler_A = get_linear_schedule_with_warmup(architect.optimizer_A, num_warmup_steps=args.epochs, num_training_steps=len(train_w_dataloader) * args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    objs_top1 = AvgrageMeter()\n",
    "    objs_top5 = AvgrageMeter()\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        logits,ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,model)\n",
    "        n = test_dataloaderx.shape[0]\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        prec1, prec5 = accuracy(logits, test_dataloadery, topk=(1, 1))\n",
    "                \n",
    "        objs_top1.update(prec1.item(), n)\n",
    "        \n",
    "        objs_top5.update(prec5.item(), n)\n",
    "    acc = objs_top1.avg\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    logging.info('%s top1 : %f',model.name,objs_top1.avg)\n",
    "    objs_top1.reset()\n",
    "    logging.info('%s top5 : %f',model.name,objs_top5.avg)\n",
    "    objs_top5.reset()\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    model.train()\n",
    "    return acc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_train(epoch, wdataloader,syndataloader,Adataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer,  scheduler_w, scheduler_v, tot_iter, past_v_accu):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    objs_w_top1 = AvgrageMeter()\n",
    "    objs_w_top5 = AvgrageMeter()\n",
    "    objs_v_top1 = AvgrageMeter()\n",
    "    objs_v_top5 = AvgrageMeter()\n",
    "    objs_weight = AvgrageMeter_tensor()\n",
    "    objs_cor_weight = AvgrageMeter()\n",
    "    objs_incor_weight = AvgrageMeter()\n",
    "    improvementacc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.w_bs\n",
    "    synsize = args.syn_bs\n",
    "    vsize = -1\n",
    "    Asize = args.A_bs\n",
    "    loader_len = len(wdataloader)\n",
    "    w_model.train()\n",
    "    v_model.train()\n",
    "\n",
    "    for step, w_batch in enumerate(wdataloader):\n",
    "        scheduler_w.step()\n",
    "        scheduler_v.step()\n",
    "        architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "        input_w = Variable(w_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_w_attn = Variable(w_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        output_w = Variable(w_batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        attn_idx = Variable(w_batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        real = Variable(w_batch[4], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        \n",
    "\n",
    "        syn_batch = next(iter(syndataloader))\n",
    "        input_syn = Variable(syn_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_syn_attn = Variable(syn_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "\n",
    "        A_batch = next(iter(Adataloader))\n",
    "        input_A_v = Variable(A_batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        input_A_v_attn = Variable(A_batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        output_A_v = Variable(A_batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "\n",
    "\n",
    "\n",
    "        tot_iter[0] += input_w.shape[0]\n",
    "        \n",
    "        \n",
    "        if(True):  # let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "        v_star_val_loss=0\n",
    "        if (args.train_A == 1 and epoch>=args.pre_epochs):\n",
    "            epsilon_w = scheduler_w.get_lr()[0]\n",
    "            epsilon_v  = scheduler_v.get_lr()[0]\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, attn_idx,v_optimizer,\n",
    "                                             epsilon_w, epsilon_v, args.grad_clip)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():    \n",
    "            sampleweight = A(input_w, input_w_attn, attn_idx).data\n",
    "            iscor = real==output_w\n",
    "            cor_mean = torch.mean(sampleweight[iscor])#correct label mean weight\n",
    "            incor_mean = torch.mean(sampleweight[~iscor])#incorrect label mean weight\n",
    "            objs_weight.update(sampleweight)\n",
    "            objs_cor_weight.update(cor_mean,torch.sum(iscor))\n",
    "            objs_incor_weight.update(incor_mean,torch.sum(~iscor))\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        logits, loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                                  A,attn_idx, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "        prec1, prec5 = accuracy(logits, output_w, topk=(1, 1))\n",
    "        objs_w_top1.update(prec1.item(), wsize)\n",
    "        objs_w_top5.update(prec5.item(), wsize)\n",
    "\n",
    "        if(epoch >= args.pre_epochs):\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(\n",
    "                input_syn, input_syn_attn, w_model, v_model)\n",
    "            logits, loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                                    v_model)\n",
    "            v_loss = (args.traindata_loss_ratio*loss +\n",
    "                      loss_aug*args.syndata_loss_ratio)\n",
    "            v_loss.backward()\n",
    "            objs_v_syn.update(loss_aug.item(), synsize)\n",
    "            objs_v_train.update(loss.item(), vsize)\n",
    "            v_optimizer.step()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "            prec1, prec5 = accuracy(logits, output_v, topk=(1, 1))\n",
    "            objs_v_top1.update(prec1.item(), vsize)\n",
    "            objs_v_top5.update(prec5.item(), vsize)\n",
    "\n",
    "\n",
    "        with torch.no_grad():#new V validation \n",
    "            _,new_v_loss = my_loss2(\n",
    "            input_A_v, input_A_v_attn,  output_A_v,model_v)\n",
    "            improvementacc+=v_star_val_loss-new_v_loss.item()\n",
    "            objs_v_val.update(new_v_loss.item(), Asize)\n",
    "\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "\n",
    "        \n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info('\\n')\n",
    "            logging.info(f\"{progress:5.3}%:||W_train_loss:{objs_w.avg:^.7f}|V_train_syn_loss:{objs_v_syn.avg:^.7f}|V_train_loss:{objs_v_train.avg:^.7f}|V_val_loss:{objs_v_val.avg:^.7f}|V_star_val_loss:{objs_v_star_val.avg:^.7f}|improvement:{objs_v_star_val.avg-objs_v_val.avg:^.7f}|w_top1:{objs_w_top1.avg:^.7f}|w_top5:{objs_w_top5.avg:^.7f}|v_top1:{objs_v_top1.avg:^.7f}|v_top5:{objs_v_top5.avg:^.7f}|\")\n",
    "            temp = objs_weight.avg\n",
    "            logging.info(f\"avg weight:{temp}\")\n",
    "            logging.info(f\"current alpha:{A.alpha[attn_idx].data}\")\n",
    "            logging.info(f\"current weight:{A(input_w, input_w_attn, attn_idx)}\")\n",
    "            logging.info(f'noise:{torch.mean(temp[5:8]) if args.attack else None} mean:{torch.mean(temp)} max: {torch.max(temp)} min: {torch.min(temp)}')\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'W_accuracy': objs_w_top1.avg})\n",
    "            wandb.log({'v_accuracy': objs_v_top1.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_weight.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "            objs_w_top1.reset()\n",
    "            objs_w_top5.reset()\n",
    "\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            w_accu = my_test(validdataloader, model_w, epoch)\n",
    "            v_accu = my_test(validdataloader, model_v, epoch)\n",
    "            wandb.log({'W_test_accuracy': w_accu})\n",
    "            wandb.log({'v_test_accuracy':v_accu})\n",
    "            wandb.log({'correct_label_mean_weight': objs_cor_weight.avg})\n",
    "            wandb.log({'wrong_label_mean_weight':objs_incor_weight.avg})\n",
    "            logging.info(f'correct label mean weight: {objs_cor_weight.avg}, wrong label mean weight: {objs_incor_weight.avg}')\n",
    "            objs_cor_weight.reset()\n",
    "            objs_incor_weight.reset()\n",
    "            if(v_accu>past_v_accu):\n",
    "                past_v_accu = v_accu\n",
    "                logging.info('find a better model')\n",
    "                torch.save(model_w, './model/'+'model_w.pt')  # +now+\n",
    "                torch.save(model_v, './model/'+'model_v.pt')\n",
    "                torch.save(model_w.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_w.pt\"))\n",
    "                torch.save(model_v.state_dict(), os.path.join(\n",
    "                    wandb.run.dir, \"model_v.pt\"))\n",
    "                torch.save(A.state_dict(), os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "                wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "            \n",
    "            logging.info(f'current best accuracy:{past_v_accu}')\n",
    "    logging.info(f'improvment:{improvementacc}')\n",
    "    return w_trainloss_acc,past_v_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/06 09:42:26 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:1.6666666666666667e-06,\t\tlr_v:1.6666666666666667e-06,\t\tlr_A:83.33333333333334----------------\n",
      "07/06 09:42:54 PM |\t  \n",
      "\n",
      "07/06 09:42:54 PM |\t   20.8%:||W_train_loss:0.6820486|V_train_syn_loss:0.6729795|V_train_loss:0.6142504|V_val_loss:0.7106711|V_star_val_loss:0.7069828|improvement:-0.0036884|w_top1:73.9583333|w_top5:73.9583333|v_top1:73.9583333|v_top5:73.9583333|\n",
      "07/06 09:42:54 PM |\t  avg weight:tensor([1.2565, 1.1686, 0.9190, 0.9621, 1.0213, 0.9528, 0.6168, 0.7292, 1.3356,\n",
      "        1.2850, 1.0750, 0.3786, 0.9679, 1.0642, 1.2851, 0.9820],\n",
      "       device='cuda:0')\n",
      "07/06 09:42:54 PM |\t  current alpha:tensor([ 2.5631,  1.1953, -1.1620, -0.9184,  2.7035, -0.5905, -2.1379,  1.6737,\n",
      "        -1.5654, -4.2933, -0.2504, -2.4065, -1.6034,  3.3695,  3.0623, -3.3464],\n",
      "       device='cuda:0')\n",
      "07/06 09:42:54 PM |\t  current weight:tensor([2.0374, 1.6846, 0.5229, 0.6260, 2.0567, 0.7823, 0.2314, 1.8478, 0.3794,\n",
      "        0.0296, 0.9605, 0.1814, 0.3676, 2.1214, 2.0963, 0.0746],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:42:54 PM |\t  noise:None mean:1.0 max: 1.3356356620788574 min: 0.37857288122177124\n",
      "07/06 09:43:22 PM |\t  \n",
      "\n",
      "07/06 09:43:22 PM |\t   45.8%:||W_train_loss:0.5846679|V_train_syn_loss:0.6696590|V_train_loss:0.6090518|V_val_loss:0.7208353|V_star_val_loss:0.7143272|improvement:-0.0065081|w_top1:79.1666667|w_top5:79.1666667|v_top1:76.5625000|v_top5:76.5625000|\n",
      "07/06 09:43:22 PM |\t  avg weight:tensor([1.1166, 0.8227, 1.1062, 0.9196, 0.7689, 0.8987, 1.2451, 1.3466, 1.4092,\n",
      "        0.8433, 1.0033, 1.0675, 0.6270, 0.7433, 1.3680, 0.7142],\n",
      "       device='cuda:0')\n",
      "07/06 09:43:22 PM |\t  current alpha:tensor([ 0.3355, -1.3520, -1.9914, -0.4459, -1.4348, -1.8993, -0.3743, -0.9367,\n",
      "         0.9420, -0.4818, -0.4218,  3.7143, -0.6840, -0.4525,  3.4812, -0.8992],\n",
      "       device='cuda:0')\n",
      "07/06 09:43:22 PM |\t  current weight:tensor([1.3785, 0.4859, 0.2839, 0.9228, 0.4547, 0.3078, 0.9634, 0.6657, 1.7010,\n",
      "        0.9027, 0.9364, 2.3079, 0.7928, 0.9191, 2.2936, 0.6838],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:43:22 PM |\t  noise:None mean:1.0000001192092896 max: 1.4091829061508179 min: 0.6270023584365845\n",
      "07/06 09:43:50 PM |\t  \n",
      "\n",
      "07/06 09:43:50 PM |\t   70.8%:||W_train_loss:0.7552465|V_train_syn_loss:0.6688701|V_train_loss:0.5984745|V_val_loss:0.7106647|V_star_val_loss:0.7243612|improvement:0.0136965|w_top1:78.1250000|w_top5:78.1250000|v_top1:77.0833333|v_top5:77.0833333|\n",
      "07/06 09:43:50 PM |\t  avg weight:tensor([1.0611, 1.1818, 1.6744, 1.3155, 0.7547, 1.1381, 0.9750, 0.3955, 1.0960,\n",
      "        1.1606, 0.4289, 0.8505, 0.7491, 1.1925, 0.9862, 1.0401],\n",
      "       device='cuda:0')\n",
      "07/06 09:43:50 PM |\t  current alpha:tensor([-1.2837, -0.7818, -0.2121, -2.0932,  2.7520, -1.3572, -0.9732, -2.1453,\n",
      "        -0.7662,  3.6098, -0.8775, -1.9247,  1.3402,  0.0043,  4.1846, -0.4361],\n",
      "       device='cuda:0')\n",
      "07/06 09:43:50 PM |\t  current weight:tensor([0.4962, 0.7181, 1.0229, 0.2511, 2.1502, 0.4682, 0.6273, 0.2397, 0.7258,\n",
      "        2.2272, 0.6718, 0.2913, 1.8128, 1.1461, 2.2531, 0.8982],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:43:50 PM |\t  noise:None mean:1.0 max: 1.6743922233581543 min: 0.3955381512641907\n",
      "07/06 09:44:18 PM |\t  \n",
      "\n",
      "07/06 09:44:18 PM |\t   95.8%:||W_train_loss:0.6734592|V_train_syn_loss:0.6703784|V_train_loss:0.6371609|V_val_loss:0.6991068|V_star_val_loss:0.6870081|improvement:-0.0120988|w_top1:70.8333333|w_top5:70.8333333|v_top1:75.5208333|v_top5:75.5208333|\n",
      "07/06 09:44:18 PM |\t  avg weight:tensor([0.7091, 1.0101, 1.0161, 1.3971, 0.9471, 1.3837, 0.6792, 1.1303, 0.8335,\n",
      "        0.9473, 0.9332, 0.9576, 1.1836, 1.1139, 1.1464, 0.6119],\n",
      "       device='cuda:0')\n",
      "07/06 09:44:18 PM |\t  current alpha:tensor([-2.7771, -0.8600, -1.4120,  2.5755, -1.8192,  1.9069, -0.7497, -3.3082,\n",
      "        -2.9992,  1.6902, -2.2477, -1.4490,  2.4974, -2.4579, 12.1066, -1.4284],\n",
      "       device='cuda:0')\n",
      "07/06 09:44:18 PM |\t  current weight:tensor([0.1506, 0.7648, 0.5039, 2.3900, 0.3589, 2.2393, 0.8253, 0.0908, 0.1221,\n",
      "        2.1713, 0.2457, 0.4890, 2.3763, 0.2028, 2.5719, 0.4973],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:44:18 PM |\t  noise:None mean:1.0 max: 1.3971076011657715 min: 0.6118544340133667\n",
      "07/06 09:44:38 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:44:38 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:44:38 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:44:38 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:44:53 PM |\t  student test loss : 0.426139\n",
      "07/06 09:44:53 PM |\t  student top1 : 50.710526\n",
      "07/06 09:44:53 PM |\t  student top5 : 50.710526\n",
      "07/06 09:44:53 PM |\t  student test loss : 0.426139\n",
      "07/06 09:44:54 PM |\t  weight_accuracy:0.5174999833106995\n",
      "07/06 09:44:54 PM |\t  find a better model\n",
      "07/06 09:44:58 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:44:58 PM |\t  improvment:-0.08216536045074463\n",
      "07/06 09:44:58 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:1.4583333333333333e-06,\t\tlr_v:1.4583333333333333e-06,\t\tlr_A:72.91666666666666----------------\n",
      "07/06 09:45:21 PM |\t  \n",
      "\n",
      "07/06 09:45:21 PM |\t   16.7%:||W_train_loss:0.6646979|V_train_syn_loss:0.6722317|V_train_loss:0.6243336|V_val_loss:0.6851927|V_star_val_loss:0.6845567|improvement:-0.0006359|w_top1:72.5000000|w_top5:72.5000000|v_top1:72.5000000|v_top5:72.5000000|\n",
      "07/06 09:45:21 PM |\t  avg weight:tensor([1.1008, 1.2623, 0.9647, 0.8134, 0.7486, 1.0138, 0.6493, 0.4607, 1.7554,\n",
      "        1.6855, 1.2402, 0.3434, 0.6111, 0.7841, 1.2529, 1.3136],\n",
      "       device='cuda:0')\n",
      "07/06 09:45:21 PM |\t  current alpha:tensor([-5.3197e-01, -8.2800e-01, -2.5630e-01,  3.8375e+00, -4.3935e-01,\n",
      "         2.6577e+00, -3.9324e-05, -4.3555e-01,  1.9770e+00,  3.0214e-01,\n",
      "         1.2859e+00, -3.0111e-01, -1.1371e+00, -3.7920e+00,  1.4478e+00,\n",
      "        -1.2669e+00], device='cuda:0')\n",
      "07/06 09:45:21 PM |\t  current weight:tensor([0.7164, 0.5886, 0.8446, 1.8951, 0.7587, 1.8091, 0.9679, 0.7604, 1.7004,\n",
      "        1.1131, 1.5167, 0.8233, 0.4701, 0.0427, 1.5674, 0.4255],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:45:21 PM |\t  noise:None mean:1.0 max: 1.7554094791412354 min: 0.34341904520988464\n",
      "07/06 09:45:49 PM |\t  \n",
      "\n",
      "07/06 09:45:49 PM |\t   41.7%:||W_train_loss:0.6159293|V_train_syn_loss:0.6713856|V_train_loss:0.5981570|V_val_loss:0.7363862|V_star_val_loss:0.7114738|improvement:-0.0249124|w_top1:78.1250000|w_top5:78.1250000|v_top1:75.5681818|v_top5:75.5681818|\n",
      "07/06 09:45:49 PM |\t  avg weight:tensor([1.1686, 0.9622, 1.0521, 0.8897, 1.0765, 0.9813, 1.1318, 1.6725, 1.2443,\n",
      "        0.9038, 0.9500, 0.6909, 0.5397, 0.8065, 1.4416, 0.4885],\n",
      "       device='cuda:0')\n",
      "07/06 09:45:49 PM |\t  current alpha:tensor([  1.6784,   4.6205,   8.0277,   2.8183,  -3.3085,   7.2146,  -2.1458,\n",
      "          6.9471,  -1.2629, -14.5761,   7.9188,   3.6534, -26.7954,   7.7384,\n",
      "         -5.7420,   3.5865], device='cuda:0')\n",
      "07/06 09:45:49 PM |\t  current weight:tensor([1.3369e+00, 1.5710e+00, 1.5860e+00, 1.4971e+00, 5.5970e-02, 1.5853e+00,\n",
      "        1.6614e-01, 1.5850e+00, 3.4978e-01, 7.4149e-07, 1.5859e+00, 1.5464e+00,\n",
      "        3.6587e-12, 1.5858e+00, 5.0736e-03, 1.5437e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:45:49 PM |\t  noise:None mean:1.0 max: 1.6725170612335205 min: 0.488450288772583\n",
      "07/06 09:46:16 PM |\t  \n",
      "\n",
      "07/06 09:46:16 PM |\t   66.7%:||W_train_loss:0.7356734|V_train_syn_loss:0.6714968|V_train_loss:0.5885528|V_val_loss:0.7468633|V_star_val_loss:0.7429883|improvement:-0.0038750|w_top1:79.1666667|w_top5:79.1666667|v_top1:76.8382353|v_top5:76.8382353|\n",
      "07/06 09:46:16 PM |\t  avg weight:tensor([1.1465, 1.0615, 1.5950, 1.4056, 0.6027, 1.1050, 0.9353, 0.3454, 1.3022,\n",
      "        0.7795, 0.5362, 1.0826, 0.4592, 1.2278, 1.4010, 1.0144],\n",
      "       device='cuda:0')\n",
      "07/06 09:46:16 PM |\t  current alpha:tensor([-0.8090, -2.7948,  5.1405,  5.5870, -4.4254,  3.0120,  1.9978, -3.2698,\n",
      "        -2.5800, -3.6566, -2.9936, -2.0252, -2.9393, -4.7072,  4.6771, -2.9798],\n",
      "       device='cuda:0')\n",
      "07/06 09:46:16 PM |\t  current weight:tensor([0.8809, 0.1647, 2.8423, 2.8482, 0.0338, 2.7249, 2.5175, 0.1047, 0.2014,\n",
      "        0.0720, 0.1364, 0.3333, 0.1436, 0.0256, 2.8326, 0.1382],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:46:16 PM |\t  noise:None mean:1.0 max: 1.5950002670288086 min: 0.34535688161849976\n",
      "07/06 09:46:46 PM |\t  \n",
      "\n",
      "07/06 09:46:46 PM |\t   91.7%:||W_train_loss:0.6437775|V_train_syn_loss:0.6739480|V_train_loss:0.6322739|V_val_loss:0.6911820|V_star_val_loss:0.6574660|improvement:-0.0337160|w_top1:73.9583333|w_top5:73.9583333|v_top1:76.0869565|v_top5:76.0869565|\n",
      "07/06 09:46:46 PM |\t  avg weight:tensor([0.6642, 1.0016, 1.1732, 0.9968, 1.3472, 1.1239, 0.6100, 1.2310, 0.9554,\n",
      "        1.0778, 1.0278, 0.7727, 0.9284, 1.2777, 1.0892, 0.7230],\n",
      "       device='cuda:0')\n",
      "07/06 09:46:46 PM |\t  current alpha:tensor([ 1.6881e+00,  3.2979e-01,  2.3469e+00,  4.6460e+00,  7.6451e-01,\n",
      "         2.0467e+00, -2.7633e+00, -2.8561e-03, -7.5819e-01, -9.6573e-01,\n",
      "         2.8090e-01, -3.3589e+00, -2.0502e+00, -5.6754e-02,  3.9717e-01,\n",
      "        -1.4460e+00], device='cuda:0')\n",
      "07/06 09:46:46 PM |\t  current weight:tensor([1.6791, 1.1573, 1.8158, 1.9706, 1.3575, 1.7620, 0.1181, 0.9934, 0.6347,\n",
      "        0.5486, 1.1336, 0.0669, 0.2269, 0.9666, 1.1898, 0.3792],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:46:46 PM |\t  noise:None mean:1.0 max: 1.3472216129302979 min: 0.6099790930747986\n",
      "07/06 09:47:10 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:47:10 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:47:10 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:47:10 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:47:25 PM |\t  student test loss : 0.426139\n",
      "07/06 09:47:25 PM |\t  student top1 : 50.710526\n",
      "07/06 09:47:25 PM |\t  student top5 : 50.710526\n",
      "07/06 09:47:25 PM |\t  student test loss : 0.426139\n",
      "07/06 09:47:25 PM |\t  weight_accuracy:0.5174999833106995\n",
      "07/06 09:47:25 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:47:25 PM |\t  improvment:-0.2745177149772644\n",
      "07/06 09:47:25 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:1.2499999999999999e-06,\t\tlr_v:1.2499999999999999e-06,\t\tlr_A:62.5----------------\n",
      "07/06 09:47:44 PM |\t  \n",
      "\n",
      "07/06 09:47:44 PM |\t   12.5%:||W_train_loss:0.6569107|V_train_syn_loss:0.6747758|V_train_loss:0.6435011|V_val_loss:0.6889487|V_star_val_loss:0.7064321|improvement:0.0174834|w_top1:71.8750000|w_top5:71.8750000|v_top1:71.8750000|v_top5:71.8750000|\n",
      "07/06 09:47:44 PM |\t  avg weight:tensor([1.2103, 1.4981, 0.9624, 0.4822, 0.7322, 0.7489, 0.5075, 0.3742, 1.7680,\n",
      "        1.8500, 1.2182, 0.2160, 0.6065, 0.9913, 1.2299, 1.6041],\n",
      "       device='cuda:0')\n",
      "07/06 09:47:44 PM |\t  current alpha:tensor([ 2.0467, -0.7927,  4.1563, -1.1278,  0.1970,  1.1318, -3.4235,  0.1180,\n",
      "         1.6246, -1.8824,  2.1855, -5.7989,  2.7217,  1.7688,  3.5879,  1.0811],\n",
      "       device='cuda:0')\n",
      "07/06 09:47:44 PM |\t  current weight:tensor([1.4647, 0.5153, 1.6283, 0.4045, 0.9081, 1.2506, 0.0522, 0.8756, 1.3817,\n",
      "        0.2185, 1.4867, 0.0050, 1.5518, 1.4129, 1.6093, 1.2349],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:47:44 PM |\t  noise:None mean:1.0 max: 1.8500117063522339 min: 0.21595700085163116\n",
      "07/06 09:48:12 PM |\t  \n",
      "\n",
      "07/06 09:48:12 PM |\t   37.5%:||W_train_loss:0.6328539|V_train_syn_loss:0.6761212|V_train_loss:0.6099321|V_val_loss:0.7131650|V_star_val_loss:0.6893620|improvement:-0.0238031|w_top1:78.1250000|w_top5:78.1250000|v_top1:75.6250000|v_top5:75.6250000|\n",
      "07/06 09:48:12 PM |\t  avg weight:tensor([1.0490, 0.6193, 0.9346, 1.1012, 1.2238, 1.0109, 1.3236, 1.5283, 1.5550,\n",
      "        1.1577, 0.9230, 0.4669, 0.6580, 0.4642, 1.7790, 0.2055],\n",
      "       device='cuda:0')\n",
      "07/06 09:48:12 PM |\t  current alpha:tensor([  9.6641,  -1.4212,   1.6409,   6.2550,  -4.1659,   0.0274,   5.6129,\n",
      "         -0.7302,   2.9041,  -0.7669,  -1.0662,  -4.8227,  -4.2076, -11.1053,\n",
      "         -0.9596,  -5.4454], device='cuda:0')\n",
      "07/06 09:48:12 PM |\t  current weight:tensor([2.3883e+00, 4.6449e-01, 2.0007e+00, 2.3838e+00, 3.6490e-02, 1.2106e+00,\n",
      "        2.3797e+00, 7.7659e-01, 2.2643e+00, 7.5747e-01, 6.1173e-01, 1.9062e-02,\n",
      "        3.5022e-02, 3.5905e-05, 6.6147e-01, 1.0264e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:48:12 PM |\t  noise:None mean:1.0 max: 1.7789649963378906 min: 0.20553873479366302\n",
      "07/06 09:48:40 PM |\t  \n",
      "\n",
      "07/06 09:48:40 PM |\t   62.5%:||W_train_loss:0.7010170|V_train_syn_loss:0.6748468|V_train_loss:0.5917287|V_val_loss:0.7294656|V_star_val_loss:0.7244456|improvement:-0.0050201|w_top1:80.2083333|w_top5:80.2083333|v_top1:77.3437500|v_top5:77.3437500|\n",
      "07/06 09:48:40 PM |\t  avg weight:tensor([1.1662, 1.2261, 1.3074, 1.1811, 0.7995, 1.1186, 0.4765, 0.5786, 1.2982,\n",
      "        0.7320, 0.9113, 1.2048, 0.4057, 1.5229, 0.9622, 1.1087],\n",
      "       device='cuda:0')\n",
      "07/06 09:48:40 PM |\t  current alpha:tensor([ 9.3938, -1.7411,  2.8468, -3.0110, -4.5886, -2.8671, -2.1844, -2.0859,\n",
      "        -2.4915, -2.7071, -3.1916, -2.5465, -3.8909,  1.9002, -2.6695, -2.6189],\n",
      "       device='cuda:0')\n",
      "07/06 09:48:40 PM |\t  current weight:tensor([4.3351, 0.6467, 4.0977, 0.2035, 0.0436, 0.2333, 0.4386, 0.4790, 0.3315,\n",
      "        0.2712, 0.1712, 0.3150, 0.0868, 3.7715, 0.2809, 0.2945],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:48:40 PM |\t  noise:None mean:1.0 max: 1.5228846073150635 min: 0.4057387113571167\n",
      "07/06 09:49:12 PM |\t  \n",
      "\n",
      "07/06 09:49:12 PM |\t   87.5%:||W_train_loss:0.7033441|V_train_syn_loss:0.6758017|V_train_loss:0.6317768|V_val_loss:0.7245068|V_star_val_loss:0.7598947|improvement:0.0353879|w_top1:72.9166667|w_top5:72.9166667|v_top1:76.1363636|v_top5:76.1363636|\n",
      "07/06 09:49:12 PM |\t  avg weight:tensor([0.4216, 0.6774, 1.3302, 1.1700, 1.3232, 1.3245, 1.0382, 1.0706, 0.7644,\n",
      "        1.1199, 0.9341, 0.7004, 0.9176, 0.9638, 1.5330, 0.7109],\n",
      "       device='cuda:0')\n",
      "07/06 09:49:12 PM |\t  current alpha:tensor([-4.0663, -0.1162,  1.1464, -4.4244,  1.9882, -0.6837,  3.5849, -0.4234,\n",
      "         1.4540,  5.3172,  1.2604, -1.7582, -3.2386, -5.3869, -0.0700, -2.9351],\n",
      "       device='cuda:0')\n",
      "07/06 09:49:12 PM |\t  current weight:tensor([0.0377, 1.0541, 1.6983, 0.0265, 1.9684, 0.7507, 2.1776, 0.8856, 1.8141,\n",
      "        2.2270, 1.7436, 0.3290, 0.0845, 0.0102, 1.0798, 0.1129],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:49:12 PM |\t  noise:None mean:1.0 max: 1.533044695854187 min: 0.421612024307251\n",
      "07/06 09:49:42 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:49:42 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:49:42 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:49:42 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:49:57 PM |\t  student test loss : 0.426139\n",
      "07/06 09:49:57 PM |\t  student top1 : 50.710526\n",
      "07/06 09:49:57 PM |\t  student top5 : 50.710526\n",
      "07/06 09:49:57 PM |\t  student test loss : 0.426139\n",
      "07/06 09:49:57 PM |\t  weight_accuracy:0.5149999856948853\n",
      "07/06 09:49:57 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:49:57 PM |\t  improvment:0.11835765838623047\n",
      "07/06 09:49:57 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:1.0416666666666667e-06,\t\tlr_v:1.0416666666666667e-06,\t\tlr_A:52.083333333333336----------------\n",
      "07/06 09:50:11 PM |\t  \n",
      "\n",
      "07/06 09:50:11 PM |\t   8.33%:||W_train_loss:0.6800271|V_train_syn_loss:0.6766441|V_train_loss:0.6413379|V_val_loss:0.6658806|V_star_val_loss:0.6524447|improvement:-0.0134358|w_top1:68.7500000|w_top5:68.7500000|v_top1:68.7500000|v_top5:68.7500000|\n",
      "07/06 09:50:11 PM |\t  avg weight:tensor([1.1297, 1.8153, 0.7264, 0.4900, 0.6599, 0.5037, 0.6186, 0.2125, 1.9055,\n",
      "        2.4264, 1.1367, 0.3000, 0.3425, 0.8693, 1.1339, 1.7295],\n",
      "       device='cuda:0')\n",
      "07/06 09:50:11 PM |\t  current alpha:tensor([-6.3868, 13.8914, -6.6460, -6.2162, -4.7686, -5.6336, -5.5738, -5.6817,\n",
      "        11.4163,  9.3937, -7.8865, -6.5635, -9.2513, -5.6429, -6.3194, 11.2825],\n",
      "       device='cuda:0')\n",
      "07/06 09:50:11 PM |\t  current weight:tensor([6.6713e-03, 3.9690e+00, 5.1497e-03, 7.9098e-03, 3.3421e-02, 1.4141e-02,\n",
      "        1.5009e-02, 1.3479e-02, 3.9690e+00, 3.9687e+00, 1.4909e-03, 5.5922e-03,\n",
      "        3.8095e-04, 1.4011e-02, 7.1351e-03, 3.9690e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:50:11 PM |\t  noise:None mean:1.0 max: 2.426448345184326 min: 0.2124823033809662\n",
      "07/06 09:50:39 PM |\t  \n",
      "\n",
      "07/06 09:50:39 PM |\t   33.3%:||W_train_loss:0.6457146|V_train_syn_loss:0.6783021|V_train_loss:0.6002600|V_val_loss:0.6926490|V_star_val_loss:0.6800288|improvement:-0.0126202|w_top1:82.2916667|w_top5:82.2916667|v_top1:77.7777778|v_top5:77.7777778|\n",
      "07/06 09:50:39 PM |\t  avg weight:tensor([0.8482, 0.6039, 0.8542, 0.7644, 1.3600, 1.0618, 0.9322, 1.6586, 1.3416,\n",
      "        1.0846, 1.0672, 0.3818, 0.9093, 0.6682, 2.0667, 0.3972],\n",
      "       device='cuda:0')\n",
      "07/06 09:50:39 PM |\t  current alpha:tensor([-8.6323e-02, -1.3514e+00,  5.6875e+00, -2.6332e+00, -4.2980e-01,\n",
      "         9.8852e-01,  1.2200e+00, -2.1117e+00, -1.8405e-03,  2.4577e-01,\n",
      "        -2.4462e-01, -6.8788e-01, -5.9406e-02, -1.0679e+00, -5.3338e-01,\n",
      "        -1.4556e+00], device='cuda:0')\n",
      "07/06 09:50:39 PM |\t  current weight:tensor([1.1118, 0.4779, 2.3161, 0.1558, 0.9160, 1.6937, 1.7942, 0.2509, 1.1609,\n",
      "        1.3040, 1.0206, 0.7774, 1.1275, 0.5945, 0.8592, 0.4396],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:50:39 PM |\t  noise:None mean:1.0 max: 2.066737651824951 min: 0.38178563117980957\n",
      "07/06 09:51:06 PM |\t  \n",
      "\n",
      "07/06 09:51:06 PM |\t   58.3%:||W_train_loss:0.6533321|V_train_syn_loss:0.6789332|V_train_loss:0.6236278|V_val_loss:0.6969179|V_star_val_loss:0.6913116|improvement:-0.0056063|w_top1:75.0000000|w_top5:75.0000000|v_top1:76.6666667|v_top5:76.6666667|\n",
      "07/06 09:51:06 PM |\t  avg weight:tensor([0.8203, 1.1232, 0.9068, 1.5800, 0.8788, 1.3892, 0.8204, 0.6096, 1.6234,\n",
      "        0.7740, 1.0195, 1.1474, 0.3590, 0.9104, 1.0207, 1.0173],\n",
      "       device='cuda:0')\n",
      "07/06 09:51:06 PM |\t  current alpha:tensor([-6.6892,  5.8185, -1.8368,  3.3800, -9.4528, -4.0507, -2.8210, -4.4487,\n",
      "        16.7287,  6.6616, -4.7205, -2.9772, -3.4596, -4.0540, -1.3027,  3.6528],\n",
      "       device='cuda:0')\n",
      "07/06 09:51:06 PM |\t  current weight:tensor([3.6288e-03, 2.9112e+00, 4.0127e-01, 2.8237e+00, 2.2911e-04, 4.9963e-02,\n",
      "        1.6409e-01, 3.3749e-02, 2.9198e+00, 2.9161e+00, 2.5789e-02, 1.4151e-01,\n",
      "        8.9007e-02, 4.9804e-02, 6.2401e-01, 2.8461e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:51:06 PM |\t  noise:None mean:1.0 max: 1.623420238494873 min: 0.3589692711830139\n",
      "07/06 09:51:31 PM |\t  \n",
      "\n",
      "07/06 09:51:31 PM |\t   83.3%:||W_train_loss:0.7003353|V_train_syn_loss:0.6789678|V_train_loss:0.6088606|V_val_loss:0.6901887|V_star_val_loss:0.6915155|improvement:0.0013268|w_top1:75.0000000|w_top5:75.0000000|v_top1:76.1904762|v_top5:76.1904762|\n",
      "07/06 09:51:31 PM |\t  avg weight:tensor([1.1292, 0.5291, 1.7828, 1.2184, 0.9807, 1.2327, 0.8231, 1.0246, 0.4731,\n",
      "        0.7563, 0.6631, 0.6567, 0.9434, 1.6075, 1.3760, 0.8033],\n",
      "       device='cuda:0')\n",
      "07/06 09:51:31 PM |\t  current alpha:tensor([-2.8725,  1.1835,  1.3514,  2.6572, -7.3851, -0.8669, -2.2792,  1.7992,\n",
      "        -0.5918, -0.9082,  4.4490,  0.2253,  0.9923,  2.5348,  0.2312,  1.8966],\n",
      "       device='cuda:0')\n",
      "07/06 09:51:31 PM |\t  current weight:tensor([9.4463e-02, 1.3510e+00, 1.4018e+00, 1.6491e+00, 1.0942e-03, 5.2218e-01,\n",
      "        1.6387e-01, 1.5142e+00, 6.2861e-01, 5.0711e-01, 1.7443e+00, 9.8133e-01,\n",
      "        1.2874e+00, 1.6351e+00, 9.8390e-01, 1.5344e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:51:31 PM |\t  noise:None mean:1.0 max: 1.7827571630477905 min: 0.47308585047721863\n",
      "07/06 09:52:05 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:52:05 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:52:05 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:52:05 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:52:20 PM |\t  student test loss : 0.426139\n",
      "07/06 09:52:20 PM |\t  student top1 : 50.710526\n",
      "07/06 09:52:20 PM |\t  student top5 : 50.710526\n",
      "07/06 09:52:20 PM |\t  student test loss : 0.426139\n",
      "07/06 09:52:20 PM |\t  weight_accuracy:0.5074999928474426\n",
      "07/06 09:52:20 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:52:20 PM |\t  improvment:-0.03255438804626465\n",
      "07/06 09:52:20 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:8.333333333333333e-07,\t\tlr_v:8.333333333333333e-07,\t\tlr_A:41.66666666666667----------------\n",
      "07/06 09:52:29 PM |\t  \n",
      "\n",
      "07/06 09:52:29 PM |\t   4.17%:||W_train_loss:0.5844117|V_train_syn_loss:0.6812770|V_train_loss:0.6422273|V_val_loss:0.6476559|V_star_val_loss:0.6361403|improvement:-0.0115156|w_top1:68.7500000|w_top5:68.7500000|v_top1:68.7500000|v_top5:68.7500000|\n",
      "07/06 09:52:29 PM |\t  avg weight:tensor([1.6963, 0.7113, 1.0836, 0.7104, 0.9683, 0.6881, 0.8825, 0.3062, 0.8998,\n",
      "        1.6776, 1.7191, 0.4625, 0.5504, 1.3098, 1.7230, 0.6112],\n",
      "       device='cuda:0')\n",
      "07/06 09:52:29 PM |\t  current alpha:tensor([ 2.7564, -7.6575,  6.4292, -6.0434, -2.4899, -0.4527, -1.1414, -0.9779,\n",
      "         0.3380,  2.4679,  1.6432, -0.4707, -4.9715,  1.8991,  2.1260,  0.4384],\n",
      "       device='cuda:0')\n",
      "07/06 09:52:29 PM |\t  current weight:tensor([1.8740e+00, 9.4121e-04, 1.9898e+00, 4.7190e-03, 1.5261e-01, 7.7472e-01,\n",
      "        4.8243e-01, 5.4470e-01, 1.1633e+00, 1.8373e+00, 1.6701e+00, 7.6621e-01,\n",
      "        1.3722e-02, 1.7335e+00, 1.7805e+00, 1.2115e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:52:29 PM |\t  noise:None mean:1.0 max: 1.7230041027069092 min: 0.30620676279067993\n",
      "07/06 09:52:58 PM |\t  \n",
      "\n",
      "07/06 09:52:58 PM |\t   29.2%:||W_train_loss:0.6879347|V_train_syn_loss:0.6816072|V_train_loss:0.6243204|V_val_loss:0.7172664|V_star_val_loss:0.7184533|improvement:0.0011869|w_top1:79.1666667|w_top5:79.1666667|v_top1:75.7812500|v_top5:75.7812500|\n",
      "07/06 09:52:58 PM |\t  avg weight:tensor([0.6383, 1.1736, 0.4708, 0.7610, 1.2174, 0.8054, 0.6303, 1.5949, 1.8468,\n",
      "        1.5817, 0.9055, 0.2175, 0.7047, 0.5643, 1.9295, 0.9583],\n",
      "       device='cuda:0')\n",
      "07/06 09:52:58 PM |\t  current alpha:tensor([-5.0320, -2.7352, -4.9296, -5.0735,  1.9248, -3.8250,  5.4964, 11.7957,\n",
      "         4.1195,  1.1679, -2.6790, -2.8777, -0.0311, -5.0544,  3.4249, -3.9405],\n",
      "       device='cuda:0')\n",
      "07/06 09:52:58 PM |\t  current weight:tensor([0.0164, 0.1542, 0.0182, 0.0157, 2.2089, 0.0540, 2.5209, 2.5312, 2.4907,\n",
      "        1.9307, 0.1626, 0.1348, 1.2459, 0.0160, 2.4514, 0.0483],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:52:58 PM |\t  noise:None mean:1.0 max: 1.9294519424438477 min: 0.21750807762145996\n",
      "07/06 09:53:25 PM |\t  \n",
      "\n",
      "07/06 09:53:25 PM |\t   54.2%:||W_train_loss:0.6304537|V_train_syn_loss:0.6792107|V_train_loss:0.6158376|V_val_loss:0.6788448|V_star_val_loss:0.6951553|improvement:0.0163105|w_top1:78.1250000|w_top5:78.1250000|v_top1:76.3392857|v_top5:76.3392857|\n",
      "07/06 09:53:25 PM |\t  avg weight:tensor([0.9782, 0.6735, 1.2012, 1.1372, 1.0755, 1.7512, 1.0883, 0.6463, 1.2937,\n",
      "        0.5155, 1.2139, 1.2536, 0.5085, 1.0156, 1.0520, 0.5958],\n",
      "       device='cuda:0')\n",
      "07/06 09:53:25 PM |\t  current alpha:tensor([-3.8679, -0.4250, -0.8466,  1.2530,  1.7766,  3.6203, -1.7385, -2.3598,\n",
      "        -0.7000, -2.0020, -2.3148,  3.8754, -0.9471,  1.6105, -2.9763, -1.7491],\n",
      "       device='cuda:0')\n",
      "07/06 09:53:25 PM |\t  current weight:tensor([0.0513, 0.9900, 0.7517, 1.9479, 2.1420, 2.4391, 0.3744, 0.2161, 0.8310,\n",
      "        0.2980, 0.2251, 2.4535, 0.6999, 2.0874, 0.1215, 0.3711],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:53:25 PM |\t  noise:None mean:1.0 max: 1.7512331008911133 min: 0.5085302591323853\n",
      "07/06 09:53:52 PM |\t  \n",
      "\n",
      "07/06 09:53:52 PM |\t   79.2%:||W_train_loss:0.7162321|V_train_syn_loss:0.6795392|V_train_loss:0.6230594|V_val_loss:0.7159523|V_star_val_loss:0.7030739|improvement:-0.0128784|w_top1:77.0833333|w_top5:77.0833333|v_top1:76.5625000|v_top5:76.5625000|\n",
      "07/06 09:53:52 PM |\t  avg weight:tensor([1.0922, 0.7638, 1.6133, 1.4194, 0.9871, 1.1575, 0.8417, 0.7825, 0.8399,\n",
      "        1.1566, 0.3723, 0.5065, 0.7482, 1.3508, 1.3511, 1.0170],\n",
      "       device='cuda:0')\n",
      "07/06 09:53:52 PM |\t  current alpha:tensor([ 1.2496, -0.4943, -2.0566, -1.4896, -1.6524,  3.8266, -1.3452,  1.3591,\n",
      "        -1.0728, -1.2661, -2.3705,  4.9787,  1.4083,  0.8623, -0.8827,  1.2026],\n",
      "       device='cuda:0')\n",
      "07/06 09:53:52 PM |\t  current weight:tensor([1.6115, 0.7856, 0.2351, 0.3815, 0.3334, 2.0292, 0.4285, 1.6496, 0.5284,\n",
      "        0.4560, 0.1772, 2.0592, 1.6660, 1.4579, 0.6067, 1.5944],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:53:52 PM |\t  noise:None mean:1.0 max: 1.613300085067749 min: 0.37230154871940613\n",
      "07/06 09:54:28 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:54:28 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:54:28 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:54:28 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:54:43 PM |\t  student test loss : 0.426139\n",
      "07/06 09:54:43 PM |\t  student top1 : 50.710526\n",
      "07/06 09:54:43 PM |\t  student top5 : 50.710526\n",
      "07/06 09:54:43 PM |\t  student test loss : 0.426139\n",
      "07/06 09:54:43 PM |\t  weight_accuracy:0.512499988079071\n",
      "07/06 09:54:43 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:54:43 PM |\t  improvment:0.013157784938812256\n",
      "07/06 09:54:43 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:6.249999999999999e-07,\t\tlr_v:6.249999999999999e-07,\t\tlr_A:31.25----------------\n",
      "07/06 09:54:48 PM |\t  \n",
      "\n",
      "07/06 09:54:48 PM |\t    0.0%:||W_train_loss:0.6099830|V_train_syn_loss:0.6836225|V_train_loss:0.6246132|V_val_loss:0.5887828|V_star_val_loss:0.5799229|improvement:-0.0088600|w_top1:75.0000000|w_top5:75.0000000|v_top1:75.0000000|v_top5:75.0000000|\n",
      "07/06 09:54:48 PM |\t  avg weight:tensor([1.5142, 1.3745, 0.1700, 1.3656, 1.7840, 0.5806, 1.3077, 0.0646, 0.6556,\n",
      "        1.5224, 1.7676, 0.1621, 1.1281, 0.9175, 1.6747, 0.0107],\n",
      "       device='cuda:0')\n",
      "07/06 09:54:48 PM |\t  current alpha:tensor([ 1.5087,  1.0635, -2.2902,  1.0383,  3.3112, -0.7816,  0.8819, -3.3182,\n",
      "        -0.5991,  1.5392,  3.0762, -2.3425,  0.4477, -0.0152,  2.2621, -5.1466],\n",
      "       device='cuda:0')\n",
      "07/06 09:54:48 PM |\t  current weight:tensor([1.5142, 1.3745, 0.1700, 1.3656, 1.7840, 0.5806, 1.3077, 0.0646, 0.6556,\n",
      "        1.5224, 1.7676, 0.1621, 1.1281, 0.9175, 1.6747, 0.0107],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:54:48 PM |\t  noise:None mean:1.0 max: 1.7840276956558228 min: 0.010697496123611927\n",
      "07/06 09:55:16 PM |\t  \n",
      "\n",
      "07/06 09:55:16 PM |\t   25.0%:||W_train_loss:0.6743504|V_train_syn_loss:0.6812924|V_train_loss:0.6268076|V_val_loss:0.7236228|V_star_val_loss:0.7193024|improvement:-0.0043204|w_top1:75.0000000|w_top5:75.0000000|v_top1:75.0000000|v_top5:75.0000000|\n",
      "07/06 09:55:16 PM |\t  avg weight:tensor([0.9404, 1.1445, 0.7995, 0.7634, 0.8494, 0.9318, 0.2766, 1.2789, 1.6344,\n",
      "        1.5731, 1.1546, 0.3086, 0.4954, 0.8516, 1.8524, 1.1453],\n",
      "       device='cuda:0')\n",
      "07/06 09:55:16 PM |\t  current alpha:tensor([-4.1423, -3.0423, -3.7481, -1.7225, -0.7464, -2.5056, -3.2088,  5.9192,\n",
      "        -1.0230,  0.2166, -1.7124, -2.1963, -2.6364, -3.4710,  4.6420, -2.8898],\n",
      "       device='cuda:0')\n",
      "07/06 09:55:16 PM |\t  current weight:tensor([0.0645, 0.1878, 0.0949, 0.6249, 1.3260, 0.3112, 0.1601, 4.1122, 1.0904,\n",
      "        2.2841, 0.6303, 0.4127, 0.2756, 0.1243, 4.0839, 0.2171],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:55:16 PM |\t  noise:None mean:1.0 max: 1.852410078048706 min: 0.2766008973121643\n",
      "07/06 09:55:44 PM |\t  \n",
      "\n",
      "07/06 09:55:44 PM |\t   50.0%:||W_train_loss:0.6497958|V_train_syn_loss:0.6810720|V_train_loss:0.6316408|V_val_loss:0.6709316|V_star_val_loss:0.6990913|improvement:0.0281597|w_top1:76.0416667|w_top5:76.0416667|v_top1:75.9615385|v_top5:75.9615385|\n",
      "07/06 09:55:44 PM |\t  avg weight:tensor([0.9613, 0.5267, 1.0829, 0.8109, 1.0917, 1.3738, 1.4560, 1.0363, 1.5710,\n",
      "        0.7849, 1.2077, 0.8632, 0.5873, 0.6709, 1.4401, 0.5351],\n",
      "       device='cuda:0')\n",
      "07/06 09:55:44 PM |\t  current alpha:tensor([-2.9095e+00, -2.0050e+00, -2.6198e+00, -4.3269e+00,  6.5123e+00,\n",
      "         5.5034e+00, -3.5905e+00, -1.3426e+00,  4.3838e-03, -3.4287e+00,\n",
      "         4.8866e+00, -1.2218e+00, -3.3915e+00, -2.6135e+00,  5.0113e+00,\n",
      "        -2.9073e+00], device='cuda:0')\n",
      "07/06 09:55:44 PM |\t  current weight:tensor([0.1538, 0.3531, 0.2019, 0.0388, 2.9705, 2.9628, 0.0799, 0.6161, 1.4907,\n",
      "        0.0934, 2.9526, 0.6772, 0.0969, 0.2031, 2.9552, 0.1541],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:55:44 PM |\t  noise:None mean:1.0 max: 1.5710004568099976 min: 0.5266917943954468\n",
      "07/06 09:56:12 PM |\t  \n",
      "\n",
      "07/06 09:56:12 PM |\t   75.0%:||W_train_loss:0.7172248|V_train_syn_loss:0.6813044|V_train_loss:0.6222873|V_val_loss:0.6802538|V_star_val_loss:0.6987577|improvement:0.0185039|w_top1:78.1250000|w_top5:78.1250000|v_top1:76.6447368|v_top5:76.6447368|\n",
      "07/06 09:56:12 PM |\t  avg weight:tensor([0.8218, 0.7774, 1.6884, 1.6844, 1.3041, 1.2402, 0.8394, 0.5460, 0.8737,\n",
      "        1.1310, 0.3829, 0.5814, 0.5774, 1.4620, 1.2855, 0.8043],\n",
      "       device='cuda:0')\n",
      "07/06 09:56:12 PM |\t  current alpha:tensor([-2.6353, -2.0682, -0.3827,  0.9510,  9.6427,  0.3813, -2.7788,  2.2967,\n",
      "        -0.0527, -2.4494,  0.8944, -1.8188, -2.1040,  5.1739, -2.9124, -3.1199],\n",
      "       device='cuda:0')\n",
      "07/06 09:56:12 PM |\t  current weight:tensor([0.1652, 0.2771, 1.0012, 1.7811, 2.4691, 1.4672, 0.1444, 2.2436, 1.2021,\n",
      "        0.1963, 1.7526, 0.3446, 0.2684, 2.4553, 0.1273, 0.1044],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:56:12 PM |\t  noise:None mean:1.0 max: 1.6884033679962158 min: 0.3829441964626312\n",
      "07/06 09:56:40 PM |\t  \n",
      "\n",
      "07/06 09:56:40 PM |\t  1e+02%:||W_train_loss:0.6804230|V_train_syn_loss:0.6823110|V_train_loss:0.6431581|V_val_loss:0.6796425|V_star_val_loss:0.6819135|improvement:0.0022709|w_top1:71.8750000|w_top5:71.8750000|v_top1:75.5000000|v_top5:75.5000000|\n",
      "07/06 09:56:40 PM |\t  avg weight:tensor([0.5829, 1.0673, 1.3098, 1.5736, 1.0883, 1.3576, 0.5982, 1.2329, 0.6244,\n",
      "        1.1612, 0.9040, 0.9539, 0.9955, 0.6775, 1.1735, 0.6995],\n",
      "       device='cuda:0')\n",
      "07/06 09:56:40 PM |\t  current alpha:tensor([-4.0188,  4.5844,  2.7704,  3.0125,  3.5657, -3.9425, -3.3025,  7.6745,\n",
      "        -1.8716, -2.8648, -1.7538,  2.4736, -6.3349, -3.6295, -3.1312, -1.1271],\n",
      "       device='cuda:0')\n",
      "07/06 09:56:40 PM |\t  current weight:tensor([0.0435, 2.4368, 2.3166, 2.3463, 2.3940, 0.0468, 0.0874, 2.4606, 0.3283,\n",
      "        0.1327, 0.3633, 2.2704, 0.0044, 0.0636, 0.1030, 0.6024],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:56:40 PM |\t  noise:None mean:1.0 max: 1.5736401081085205 min: 0.5828990936279297\n",
      "07/06 09:56:55 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:56:55 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:56:55 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:56:55 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:57:10 PM |\t  student test loss : 0.426139\n",
      "07/06 09:57:10 PM |\t  student top1 : 50.710526\n",
      "07/06 09:57:10 PM |\t  student top5 : 50.710526\n",
      "07/06 09:57:10 PM |\t  student test loss : 0.426139\n",
      "07/06 09:57:10 PM |\t  weight_accuracy:0.5149999856948853\n",
      "07/06 09:57:10 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:57:10 PM |\t  improvment:0.2588247060775757\n",
      "07/06 09:57:10 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:4.1666666666666667e-07,\t\tlr_v:4.1666666666666667e-07,\t\tlr_A:20.833333333333336----------------\n",
      "07/06 09:57:38 PM |\t  \n",
      "\n",
      "07/06 09:57:38 PM |\t   20.8%:||W_train_loss:0.6518797|V_train_syn_loss:0.6814857|V_train_loss:0.6442908|V_val_loss:0.6825466|V_star_val_loss:0.6929958|improvement:0.0104493|w_top1:73.9583333|w_top5:73.9583333|v_top1:73.9583333|v_top5:73.9583333|\n",
      "07/06 09:57:38 PM |\t  avg weight:tensor([1.1795, 1.3390, 0.8112, 0.8882, 0.9232, 0.9791, 0.4645, 0.6020, 1.5680,\n",
      "        1.4452, 1.3443, 0.2651, 0.6386, 0.9859, 1.4559, 1.1103],\n",
      "       device='cuda:0')\n",
      "07/06 09:57:38 PM |\t  current alpha:tensor([ 4.0766,  3.5250, -1.9519,  1.6735,  4.3720,  0.3583, -2.9230,  3.3884,\n",
      "        -2.3218, -7.7551,  1.6474, -3.7352, -0.1962,  6.8498,  4.2360, -6.1061],\n",
      "       device='cuda:0')\n",
      "07/06 09:57:38 PM |\t  current weight:tensor([1.7667e+00, 1.7453e+00, 2.2342e-01, 1.5129e+00, 1.7743e+00, 1.0576e+00,\n",
      "        9.1682e-02, 1.7380e+00, 1.6051e-01, 7.6963e-04, 1.5066e+00, 4.1884e-02,\n",
      "        8.1051e-01, 1.7948e+00, 1.7711e+00, 3.9962e-03], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 09:57:38 PM |\t  noise:None mean:1.0 max: 1.5679854154586792 min: 0.26507094502449036\n",
      "07/06 09:58:04 PM |\t  \n",
      "\n",
      "07/06 09:58:04 PM |\t   45.8%:||W_train_loss:0.6287993|V_train_syn_loss:0.6810023|V_train_loss:0.6148722|V_val_loss:0.6816760|V_star_val_loss:0.6828218|improvement:0.0011458|w_top1:79.1666667|w_top5:79.1666667|v_top1:76.5625000|v_top5:76.5625000|\n",
      "07/06 09:58:04 PM |\t  avg weight:tensor([0.9432, 0.4973, 1.0663, 0.9039, 0.8079, 0.9360, 1.4718, 1.6337, 1.4998,\n",
      "        1.1512, 0.8185, 0.8155, 0.6140, 0.6585, 1.6391, 0.5433],\n",
      "       device='cuda:0')\n",
      "07/06 09:58:04 PM |\t  current alpha:tensor([-3.0290e-01, -1.4349e+00, -2.2845e+00, -7.9452e-01, -1.6025e+00,\n",
      "        -1.9582e+00,  7.7523e-01, -1.1920e+00,  1.3314e+00, -8.4882e-01,\n",
      "        -1.4172e-01,  4.1225e+00, -2.0043e-02,  1.2338e+00,  5.7780e+00,\n",
      "         2.3453e-04], device='cuda:0')\n",
      "07/06 09:58:04 PM |\t  current weight:tensor([0.9021, 0.4084, 0.1962, 0.6608, 0.3559, 0.2626, 1.4537, 0.4945, 1.6797,\n",
      "        0.6363, 0.9865, 2.0894, 1.0510, 1.6444, 2.1167, 1.0618],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:58:04 PM |\t  noise:None mean:1.0 max: 1.6390798091888428 min: 0.4972935914993286\n",
      "07/06 09:58:30 PM |\t  \n",
      "\n",
      "07/06 09:58:30 PM |\t   70.8%:||W_train_loss:0.7180141|V_train_syn_loss:0.6817772|V_train_loss:0.6268546|V_val_loss:0.7175763|V_star_val_loss:0.7275531|improvement:0.0099768|w_top1:79.1666667|w_top5:79.1666667|v_top1:77.0833333|v_top5:77.0833333|\n",
      "07/06 09:58:30 PM |\t  avg weight:tensor([0.8178, 0.7842, 1.5505, 1.3959, 1.3938, 1.4960, 0.8264, 0.2735, 0.9183,\n",
      "        1.1143, 0.5838, 0.6387, 0.5463, 1.0880, 1.7635, 0.8090],\n",
      "       device='cuda:0')\n",
      "07/06 09:58:30 PM |\t  current alpha:tensor([-4.2529, -2.0765, -1.3511, -3.3605,  3.6146, -3.0032, -0.9447, -2.5381,\n",
      "        -4.8780,  4.2949, -2.8910, -2.9581,  1.0018, -3.6693,  6.8795, -0.3132],\n",
      "       device='cuda:0')\n",
      "07/06 09:58:30 PM |\t  current weight:tensor([0.0448, 0.3556, 0.6565, 0.1071, 3.1083, 0.1509, 0.8936, 0.2337, 0.0241,\n",
      "        3.1491, 0.1679, 0.1575, 2.3346, 0.0794, 3.1887, 1.3481],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:58:30 PM |\t  noise:None mean:1.0 max: 1.7634538412094116 min: 0.27354666590690613\n",
      "07/06 09:58:57 PM |\t  \n",
      "\n",
      "07/06 09:58:57 PM |\t   95.8%:||W_train_loss:0.6727719|V_train_syn_loss:0.6813217|V_train_loss:0.6451852|V_val_loss:0.7215035|V_star_val_loss:0.7268977|improvement:0.0053942|w_top1:70.8333333|w_top5:70.8333333|v_top1:75.5208333|v_top5:75.5208333|\n",
      "07/06 09:58:57 PM |\t  avg weight:tensor([0.6032, 0.7047, 1.0930, 1.4799, 1.1025, 1.5948, 0.6037, 1.1972, 0.7656,\n",
      "        1.1732, 1.1392, 0.6314, 1.0397, 1.0768, 1.1782, 0.6170],\n",
      "       device='cuda:0')\n",
      "07/06 09:58:57 PM |\t  current alpha:tensor([-4.9780, -4.6647, -2.5254,  4.5744, -3.3231,  4.1719, -1.3361, -5.3634,\n",
      "        -4.2216,  3.4422, -3.8767, -2.8213,  2.6278, -3.8813, 15.4090, -4.0197],\n",
      "       device='cuda:0')\n",
      "07/06 09:58:57 PM |\t  current weight:tensor([0.0205, 0.0280, 0.2219, 2.9641, 0.1042, 2.9491, 0.6233, 0.0140, 0.0433,\n",
      "        2.9018, 0.0608, 0.1682, 2.7929, 0.0605, 2.9946, 0.0528],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:58:57 PM |\t  noise:None mean:1.0 max: 1.594846248626709 min: 0.6031661629676819\n",
      "07/06 09:59:16 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:59:16 PM |\t  teacher top1 : 50.710526\n",
      "07/06 09:59:16 PM |\t  teacher top5 : 50.710526\n",
      "07/06 09:59:16 PM |\t  teacher test loss : 0.426139\n",
      "07/06 09:59:31 PM |\t  student test loss : 0.426139\n",
      "07/06 09:59:31 PM |\t  student top1 : 50.710526\n",
      "07/06 09:59:31 PM |\t  student top5 : 50.710526\n",
      "07/06 09:59:31 PM |\t  student test loss : 0.426139\n",
      "07/06 09:59:31 PM |\t  weight_accuracy:0.5149999856948853\n",
      "07/06 09:59:31 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 09:59:31 PM |\t  improvment:0.17748987674713135\n",
      "07/06 09:59:31 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:2.0833333333333333e-07,\t\tlr_v:2.0833333333333333e-07,\t\tlr_A:10.416666666666668----------------\n",
      "07/06 09:59:53 PM |\t  \n",
      "\n",
      "07/06 09:59:53 PM |\t   16.7%:||W_train_loss:0.6594195|V_train_syn_loss:0.6815611|V_train_loss:0.6256843|V_val_loss:0.6951565|V_star_val_loss:0.7080508|improvement:0.0128943|w_top1:72.5000000|w_top5:72.5000000|v_top1:72.5000000|v_top5:72.5000000|\n",
      "07/06 09:59:53 PM |\t  avg weight:tensor([1.0612, 1.2566, 0.9283, 0.7635, 0.7521, 0.9644, 0.5379, 0.3740, 1.8518,\n",
      "        1.7336, 1.3120, 0.3091, 0.6048, 0.8248, 1.3945, 1.3314],\n",
      "       device='cuda:0')\n",
      "07/06 09:59:53 PM |\t  current alpha:tensor([-1.3808, -1.5884, -0.4482,  4.5209, -0.5534,  3.4862, -0.3950, -1.5517,\n",
      "         2.1520,  0.1121,  0.8833, -1.0299, -1.7727, -4.2250,  1.7088, -2.3464],\n",
      "       device='cuda:0')\n",
      "07/06 09:59:53 PM |\t  current weight:tensor([0.4495, 0.3795, 0.8722, 2.2136, 0.8169, 2.1712, 0.9007, 0.3912, 2.0046,\n",
      "        1.1815, 1.5831, 0.5887, 0.3249, 0.0323, 1.8946, 0.1955],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 09:59:53 PM |\t  noise:None mean:1.0 max: 1.8517874479293823 min: 0.3091394603252411\n",
      "07/06 10:00:21 PM |\t  \n",
      "\n",
      "07/06 10:00:21 PM |\t   41.7%:||W_train_loss:0.6328148|V_train_syn_loss:0.6823238|V_train_loss:0.6075377|V_val_loss:0.6962591|V_star_val_loss:0.6744117|improvement:-0.0218474|w_top1:77.0833333|w_top5:77.0833333|v_top1:75.5681818|v_top5:75.5681818|\n",
      "07/06 10:00:21 PM |\t  avg weight:tensor([1.0869, 0.7198, 1.0710, 1.0453, 1.0423, 1.0693, 1.2450, 1.8439, 1.2456,\n",
      "        1.0459, 0.9046, 0.4735, 0.5732, 0.6835, 1.5836, 0.3667],\n",
      "       device='cuda:0')\n",
      "07/06 10:00:21 PM |\t  current alpha:tensor([  0.9594,   5.5141,   8.9389,   3.5098,  -3.5717,   8.0006,  -1.1545,\n",
      "          7.7000,  -1.5153, -16.1942,   8.7756,   4.4523, -29.8337,   8.5829,\n",
      "         -6.6573,   4.0642], device='cuda:0')\n",
      "07/06 10:00:21 PM |\t  current weight:tensor([1.1444e+00, 1.5764e+00, 1.5826e+00, 1.5368e+00, 4.3273e-02, 1.5823e+00,\n",
      "        3.7936e-01, 1.5821e+00, 2.8514e-01, 1.4668e-07, 1.5825e+00, 1.5646e+00,\n",
      "        1.7490e-13, 1.5825e+00, 2.0306e-03, 1.5561e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 10:00:21 PM |\t  noise:None mean:1.0 max: 1.8439373970031738 min: 0.3666636347770691\n",
      "07/06 10:00:49 PM |\t  \n",
      "\n",
      "07/06 10:00:49 PM |\t   66.7%:||W_train_loss:0.7286319|V_train_syn_loss:0.6809108|V_train_loss:0.6141976|V_val_loss:0.7140972|V_star_val_loss:0.7222896|improvement:0.0081924|w_top1:79.1666667|w_top5:79.1666667|v_top1:76.8382353|v_top5:76.8382353|\n",
      "07/06 10:00:49 PM |\t  avg weight:tensor([0.9603, 0.7922, 1.4733, 1.4884, 0.9358, 1.5156, 0.9196, 0.3168, 1.1938,\n",
      "        0.6952, 0.7205, 0.9611, 0.3318, 1.3493, 1.5856, 0.7608],\n",
      "       device='cuda:0')\n",
      "07/06 10:00:49 PM |\t  current alpha:tensor([-2.1586, -3.9563,  5.6382,  6.7649, -5.5804,  4.0103,  1.1882, -4.4280,\n",
      "        -3.8626, -4.8825, -4.4736, -3.3037, -3.9770, -5.6414,  7.8025, -4.0775],\n",
      "       device='cuda:0')\n",
      "07/06 10:00:49 PM |\t  current weight:tensor([0.3316, 0.0601, 3.1920, 3.1996, 0.0120, 3.1463, 2.4551, 0.0378, 0.0659,\n",
      "        0.0241, 0.0361, 0.1135, 0.0589, 0.0113, 3.2020, 0.0534],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:00:49 PM |\t  noise:None mean:1.0 max: 1.5855563879013062 min: 0.3168354034423828\n",
      "07/06 10:01:12 PM |\t  \n",
      "\n",
      "07/06 10:01:12 PM |\t   91.7%:||W_train_loss:0.6564056|V_train_syn_loss:0.6809138|V_train_loss:0.6281306|V_val_loss:0.6963051|V_star_val_loss:0.7149198|improvement:0.0186147|w_top1:75.0000000|w_top5:75.0000000|v_top1:76.0869565|v_top5:76.0869565|\n",
      "07/06 10:01:12 PM |\t  avg weight:tensor([0.6072, 0.7591, 1.1655, 1.0036, 1.6035, 1.1284, 0.6486, 1.2339, 0.7620,\n",
      "        1.2146, 1.1573, 0.6295, 0.9633, 1.0800, 1.2107, 0.8328],\n",
      "       device='cuda:0')\n",
      "07/06 10:01:12 PM |\t  current alpha:tensor([ 1.5332,  0.0478,  2.5221,  5.6508,  1.1734,  2.2011, -3.3291, -0.1610,\n",
      "        -1.4790, -0.5881,  0.3732, -3.4654, -2.3754, -0.4916, -0.0538, -1.8927],\n",
      "       device='cuda:0')\n",
      "07/06 10:01:12 PM |\t  current weight:tensor([1.7174, 1.0690, 1.9329, 2.0808, 1.5948, 1.8800, 0.0722, 0.9602, 0.3875,\n",
      "        0.7456, 1.2366, 0.0633, 0.1776, 0.7925, 1.0160, 0.2734],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:01:12 PM |\t  noise:None mean:1.0 max: 1.6034597158432007 min: 0.6071873903274536\n",
      "07/06 10:01:36 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:01:36 PM |\t  teacher top1 : 50.710526\n",
      "07/06 10:01:36 PM |\t  teacher top5 : 50.710526\n",
      "07/06 10:01:36 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:01:51 PM |\t  student test loss : 0.426139\n",
      "07/06 10:01:51 PM |\t  student top1 : 50.710526\n",
      "07/06 10:01:51 PM |\t  student top5 : 50.710526\n",
      "07/06 10:01:51 PM |\t  student test loss : 0.426139\n",
      "07/06 10:01:51 PM |\t  weight_accuracy:0.5174999833106995\n",
      "07/06 10:01:51 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 10:01:51 PM |\t  improvment:0.05045562982559204\n",
      "07/06 10:01:51 PM |\t  \n",
      "\n",
      "  ----------------epoch:8,\t\tlr_w:0.0,\t\tlr_v:0.0,\t\tlr_A:0.0----------------\n",
      "07/06 10:02:08 PM |\t  \n",
      "\n",
      "07/06 10:02:08 PM |\t   12.5%:||W_train_loss:0.6405956|V_train_syn_loss:0.6815959|V_train_loss:0.6468275|V_val_loss:0.6687380|V_star_val_loss:0.6682618|improvement:-0.0004762|w_top1:70.3125000|w_top5:70.3125000|v_top1:71.8750000|v_top5:71.8750000|\n",
      "07/06 10:02:08 PM |\t  avg weight:tensor([1.2141, 1.4758, 0.9423, 0.4010, 0.7359, 0.6627, 0.4472, 0.3697, 1.8136,\n",
      "        1.8717, 1.2442, 0.2392, 0.6747, 1.0229, 1.2695, 1.6154],\n",
      "       device='cuda:0')\n",
      "07/06 10:02:08 PM |\t  current alpha:tensor([ 2.1836, -0.6099,  4.5730, -1.7138,  0.4512,  1.4618, -3.7054,  0.1287,\n",
      "         1.9904, -2.3541,  2.7501, -6.5284,  2.9529,  1.9025,  4.2602,  1.2333],\n",
      "       device='cuda:0')\n",
      "07/06 10:02:08 PM |\t  current weight:tensor([1.4583, 0.5713, 1.6060, 0.2477, 0.9912, 1.3172, 0.0389, 0.8634, 1.4275,\n",
      "        0.1407, 1.5251, 0.0024, 1.5421, 1.4119, 1.5999, 1.2565],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:02:08 PM |\t  noise:None mean:1.0 max: 1.8716598749160767 min: 0.23923881351947784\n",
      "07/06 10:02:47 PM |\t  \n",
      "\n",
      "07/06 10:02:47 PM |\t   37.5%:||W_train_loss:0.6536573|V_train_syn_loss:0.6817626|V_train_loss:0.6135903|V_val_loss:0.6876303|V_star_val_loss:0.6792557|improvement:-0.0083745|w_top1:76.0416667|w_top5:76.0416667|v_top1:75.6250000|v_top5:75.6250000|\n",
      "07/06 10:02:47 PM |\t  avg weight:tensor([0.9711, 0.5203, 0.9526, 1.1580, 1.1713, 1.1674, 1.3319, 1.6455, 1.5322,\n",
      "        1.2428, 0.9047, 0.3108, 0.6274, 0.4252, 1.8990, 0.1399],\n",
      "       device='cuda:0')\n",
      "07/06 10:02:47 PM |\t  current alpha:tensor([ 10.1133,  -2.3587,   1.7526,   6.5102,  -4.3230,   0.5795,   5.9337,\n",
      "         -0.8414,   3.0893,  -0.7824,  -1.2327,  -5.0336,  -4.4087, -11.5746,\n",
      "         -2.1548,  -5.6502], device='cuda:0')\n",
      "07/06 10:02:47 PM |\t  current weight:tensor([2.4570e+00, 2.1224e-01, 2.0942e+00, 2.4535e+00, 3.2156e-02, 1.5749e+00,\n",
      "        2.4506e+00, 7.4020e-01, 2.3501e+00, 7.7106e-01, 5.5459e-01, 1.5905e-02,\n",
      "        2.9545e-02, 2.3102e-05, 2.5526e-01, 8.6112e-03], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 10:02:47 PM |\t  noise:None mean:1.0 max: 1.8990232944488525 min: 0.1398964822292328\n",
      "07/06 10:03:15 PM |\t  \n",
      "\n",
      "07/06 10:03:15 PM |\t   62.5%:||W_train_loss:0.6713768|V_train_syn_loss:0.6825836|V_train_loss:0.6158922|V_val_loss:0.6747651|V_star_val_loss:0.6731484|improvement:-0.0016167|w_top1:78.1250000|w_top5:78.1250000|v_top1:77.3437500|v_top5:77.3437500|\n",
      "07/06 10:03:15 PM |\t  avg weight:tensor([1.0957, 1.0449, 1.2050, 1.2112, 0.9410, 1.2549, 0.5737, 0.5742, 1.2303,\n",
      "        0.6912, 0.9782, 1.2029, 0.3220, 1.6111, 1.0522, 1.0112],\n",
      "       device='cuda:0')\n",
      "07/06 10:03:15 PM |\t  current alpha:tensor([ 9.7013, -3.2541,  3.2435, -2.7310, -4.7847, -3.6162, -1.1648, -2.0117,\n",
      "        -2.6850, -2.7989, -3.8952, -3.0501, -3.9992,  2.6899, -2.5334, -3.3449],\n",
      "       device='cuda:0')\n",
      "07/06 10:03:15 PM |\t  current weight:tensor([4.3248, 0.1608, 4.1626, 0.2646, 0.0358, 0.1132, 1.0285, 0.5103, 0.2762,\n",
      "        0.2482, 0.0862, 0.1956, 0.0779, 4.0501, 0.3181, 0.1473],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:03:15 PM |\t  noise:None mean:1.0 max: 1.611120581626892 min: 0.32202741503715515\n",
      "07/06 10:03:35 PM |\t  \n",
      "\n",
      "07/06 10:03:35 PM |\t   87.5%:||W_train_loss:0.6699495|V_train_syn_loss:0.6816360|V_train_loss:0.6345579|V_val_loss:0.7004910|V_star_val_loss:0.7127283|improvement:0.0122373|w_top1:72.9166667|w_top5:72.9166667|v_top1:76.1363636|v_top5:76.1363636|\n",
      "07/06 10:03:35 PM |\t  avg weight:tensor([0.3762, 0.5909, 1.3754, 1.1901, 1.3397, 1.3394, 1.0458, 1.0802, 0.7084,\n",
      "        1.0943, 0.9572, 0.6379, 0.9436, 0.9498, 1.5751, 0.7961],\n",
      "       device='cuda:0')\n",
      "07/06 10:03:35 PM |\t  current alpha:tensor([-4.7628, -0.7523,  1.2219, -5.1467,  2.6548, -0.7752,  3.6965, -0.6796,\n",
      "         1.5645,  5.9012,  1.4718, -2.3908, -3.8377, -5.5449,  0.4378, -2.9900],\n",
      "       device='cuda:0')\n",
      "07/06 10:03:35 PM |\t  current weight:tensor([0.0192, 0.7248, 1.7478, 0.0131, 2.1142, 0.7136, 2.2080, 0.7611, 1.8713,\n",
      "        2.2566, 1.8404, 0.1898, 0.0477, 0.0088, 1.3752, 0.1083],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:03:35 PM |\t  noise:None mean:1.0 max: 1.575081706047058 min: 0.37621909379959106\n",
      "07/06 10:04:05 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:04:05 PM |\t  teacher top1 : 50.710526\n",
      "07/06 10:04:05 PM |\t  teacher top5 : 50.710526\n",
      "07/06 10:04:05 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:04:20 PM |\t  student test loss : 0.426139\n",
      "07/06 10:04:20 PM |\t  student top1 : 50.710526\n",
      "07/06 10:04:20 PM |\t  student top5 : 50.710526\n",
      "07/06 10:04:20 PM |\t  student test loss : 0.426139\n",
      "07/06 10:04:20 PM |\t  weight_accuracy:0.5174999833106995\n",
      "07/06 10:04:20 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 10:04:20 PM |\t  improvment:0.08465802669525146\n",
      "07/06 10:04:20 PM |\t  \n",
      "\n",
      "  ----------------epoch:9,\t\tlr_w:0.0,\t\tlr_v:0.0,\t\tlr_A:0.0----------------\n",
      "07/06 10:04:35 PM |\t  \n",
      "\n",
      "07/06 10:04:35 PM |\t   8.33%:||W_train_loss:0.6676438|V_train_syn_loss:0.6824616|V_train_loss:0.6491800|V_val_loss:0.7095595|V_star_val_loss:0.7332777|improvement:0.0237182|w_top1:68.7500000|w_top5:68.7500000|v_top1:68.7500000|v_top5:68.7500000|\n",
      "07/06 10:04:35 PM |\t  avg weight:tensor([1.1327, 1.7774, 0.7210, 0.4522, 0.6507, 0.4445, 0.5833, 0.2052, 1.9423,\n",
      "        2.4486, 1.1506, 0.3182, 0.3856, 0.8933, 1.1594, 1.7351],\n",
      "       device='cuda:0')\n",
      "07/06 10:04:35 PM |\t  current alpha:tensor([-6.6153, 14.3522, -6.8424, -6.4996, -5.0065, -5.8999, -5.8646, -5.8604,\n",
      "        11.6272,  9.5640, -8.2892, -6.8503, -9.9912, -5.9586, -6.5930, 11.6613],\n",
      "       device='cuda:0')\n",
      "07/06 10:04:35 PM |\t  current weight:tensor([5.3196e-03, 3.9760e+00, 4.2400e-03, 5.9711e-03, 2.6440e-02, 1.0863e-02,\n",
      "        1.1253e-02, 1.1300e-02, 3.9759e+00, 3.9757e+00, 9.9856e-04, 4.2065e-03,\n",
      "        1.8210e-04, 1.0246e-02, 5.4392e-03, 3.9759e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 10:04:35 PM |\t  noise:None mean:1.0 max: 2.4486327171325684 min: 0.20518022775650024\n",
      "07/06 10:05:03 PM |\t  \n",
      "\n",
      "07/06 10:05:03 PM |\t   33.3%:||W_train_loss:0.6458168|V_train_syn_loss:0.6826149|V_train_loss:0.6013387|V_val_loss:0.6776007|V_star_val_loss:0.6861295|improvement:0.0085288|w_top1:82.2916667|w_top5:82.2916667|v_top1:77.7777778|v_top5:77.7777778|\n",
      "07/06 10:05:03 PM |\t  avg weight:tensor([0.8046, 0.5801, 0.8712, 0.7904, 1.3311, 1.1245, 0.9299, 1.6660, 1.3784,\n",
      "        1.1377, 1.0664, 0.3086, 0.8795, 0.6605, 2.1231, 0.3479],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:03 PM |\t  current alpha:tensor([-0.2288, -1.4548,  5.8119, -2.7220, -0.4598,  1.1686,  1.1338, -2.2576,\n",
      "        -0.2066,  0.1541, -0.3202, -0.8898, -0.0558, -1.1443, -0.6982, -1.6966],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:03 PM |\t  current weight:tensor([1.0731, 0.4584, 2.4149, 0.1494, 0.9375, 1.8479, 1.8325, 0.2294, 1.0864,\n",
      "        1.3042, 1.0188, 0.7052, 1.1773, 0.5850, 0.8047, 0.3752],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "07/06 10:05:03 PM |\t  noise:None mean:1.0000001192092896 max: 2.1231374740600586 min: 0.30858391523361206\n",
      "07/06 10:05:31 PM |\t  \n",
      "\n",
      "07/06 10:05:31 PM |\t   58.3%:||W_train_loss:0.6607363|V_train_syn_loss:0.6819831|V_train_loss:0.6284727|V_val_loss:0.6953475|V_star_val_loss:0.6919873|improvement:-0.0033603|w_top1:75.0000000|w_top5:75.0000000|v_top1:76.6666667|v_top5:76.6666667|\n",
      "07/06 10:05:31 PM |\t  avg weight:tensor([0.7845, 1.0535, 0.8603, 1.5761, 0.9404, 1.4985, 0.8107, 0.6125, 1.5760,\n",
      "        0.7783, 1.0563, 1.1730, 0.3140, 0.9361, 1.0417, 0.9881],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:31 PM |\t  current alpha:tensor([-6.8964,  5.8697, -1.7545,  3.3873, -9.7375, -4.1034, -2.8529, -4.5649,\n",
      "        16.7287,  6.8544, -4.7740, -3.0009, -3.4821, -4.1626, -0.9664,  3.7095],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:31 PM |\t  current weight:tensor([2.9150e-03, 2.8770e+00, 4.2549e-01, 2.7908e+00, 1.7029e-04, 4.6876e-02,\n",
      "        1.5732e-01, 2.9729e-02, 2.8851e+00, 2.8821e+00, 2.4165e-02, 1.3671e-01,\n",
      "        8.6053e-02, 4.4225e-02, 7.9515e-01, 2.8162e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 10:05:31 PM |\t  noise:None mean:1.0 max: 1.5760712623596191 min: 0.31397587060928345\n",
      "07/06 10:05:59 PM |\t  \n",
      "\n",
      "07/06 10:05:59 PM |\t   83.3%:||W_train_loss:0.6933918|V_train_syn_loss:0.6809942|V_train_loss:0.6284616|V_val_loss:0.7134989|V_star_val_loss:0.7164926|improvement:0.0029936|w_top1:75.0000000|w_top5:75.0000000|v_top1:76.1904762|v_top5:76.1904762|\n",
      "07/06 10:05:59 PM |\t  avg weight:tensor([1.0938, 0.4969, 1.7778, 1.2320, 0.9933, 1.2394, 0.8492, 1.0384, 0.4425,\n",
      "        0.7596, 0.6648, 0.6389, 0.9486, 1.6233, 1.3989, 0.8026],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:59 PM |\t  current alpha:tensor([-2.9291,  1.1418,  1.3101,  2.7443, -7.4912, -0.8708, -2.3299,  1.8773,\n",
      "        -0.7082, -0.9679,  4.5040,  0.1506,  0.9502,  2.6202,  0.1590,  1.9544],\n",
      "       device='cuda:0')\n",
      "07/06 10:05:59 PM |\t  current weight:tensor([9.0310e-02, 1.3493e+00, 1.4019e+00, 1.6726e+00, 9.9265e-04, 5.2527e-01,\n",
      "        1.5786e-01, 1.5439e+00, 5.8741e-01, 4.9006e-01, 1.7606e+00, 9.5694e-01,\n",
      "        1.2837e+00, 1.6593e+00, 9.6064e-01, 1.5592e+00], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "07/06 10:05:59 PM |\t  noise:None mean:1.0 max: 1.7778451442718506 min: 0.4425015151500702\n",
      "07/06 10:06:32 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:06:32 PM |\t  teacher top1 : 50.710526\n",
      "07/06 10:06:32 PM |\t  teacher top5 : 50.710526\n",
      "07/06 10:06:32 PM |\t  teacher test loss : 0.426139\n",
      "07/06 10:06:48 PM |\t  student test loss : 0.426139\n",
      "07/06 10:06:48 PM |\t  student top1 : 50.710526\n",
      "07/06 10:06:48 PM |\t  student top5 : 50.710526\n",
      "07/06 10:06:48 PM |\t  student test loss : 0.426139\n",
      "07/06 10:06:48 PM |\t  weight_accuracy:0.5174999833106995\n",
      "07/06 10:06:48 PM |\t  current best accuracy:50.71052631578947\n",
      "07/06 10:06:48 PM |\t  improvment:0.17267906665802002\n",
      "07/06 10:07:26 PM |\t  teacher test loss : 0.438744\n",
      "07/06 10:07:26 PM |\t  teacher top1 : 52.210526\n",
      "07/06 10:07:26 PM |\t  teacher top5 : 52.210526\n",
      "07/06 10:07:26 PM |\t  teacher test loss : 0.438744\n",
      "07/06 10:08:37 PM |\t  student test loss : 0.438744\n",
      "07/06 10:08:37 PM |\t  student top1 : 52.210526\n",
      "07/06 10:08:37 PM |\t  student top5 : 52.210526\n",
      "07/06 10:08:37 PM |\t  student test loss : 0.438744\n",
      "07/06 10:08:37 PM |\t  best w on test:52.21052629972759 accuracy; best v on test:52.21052629972759 accuracy\n"
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin == 1):\n",
    "#     my_test(valid_dataloader, model_w, -1)  # before train\n",
    "#     my_test(valid_dataloader, model_v, -1)\n",
    "\n",
    "tot_iter = [0]\n",
    "v_accu = 0\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(\n",
    "        f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss,v_accu = my_train(epoch, train_w_dataloader,train_syn_dataloader,train_A_dataloader, valid_dataloader, model_w,\n",
    "                            model_v,  architect, A, w_optimizer, v_optimizer, scheduler_w, scheduler_v, tot_iter,v_accu)\n",
    "\n",
    "    # scheduler_w.step()\n",
    "    # scheduler_v.step()\n",
    "    # architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "\n",
    "w_accu = my_test(test_dataloader, torch.load('./model/'+'model_w.pt'), -2)\n",
    "v_accu = my_test(test_dataloader, torch.load('./model/'+'model_v.pt'), -2)\n",
    "logging.info(f'best w on test:{w_accu} accuracy; best v on test:{v_accu} accuracy')\n",
    "\n",
    "wandb.log({'v_testdata_accuracy': v_accu})\n",
    "wandb.log({'w_testdata_accuracy': w_accu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/05 07:00:38 AM |\t  teacher test loss : 0.793897\n",
      "07/05 07:00:38 AM |\t  teacher top1 : 94.473684\n",
      "07/05 07:00:38 AM |\t  teacher top5 : 94.473684\n",
      "07/05 07:00:38 AM |\t  teacher test loss : 0.793897\n",
      "07/05 07:00:47 AM |\t  student test loss : 0.782397\n",
      "07/05 07:00:47 AM |\t  student top1 : 93.105263\n",
      "07/05 07:00:47 AM |\t  student top5 : 93.105263\n",
      "07/05 07:00:47 AM |\t  student test loss : 0.782397\n",
      "07/05 07:00:47 AM |\t  best w on test:94.47368421052632 accuracy; best v on test:93.10526315789474 accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_accu = my_test(test_dataloader, torch.load('./model/'+'model_w.pt'), -2)\n",
    "v_accu = my_test(test_dataloader, torch.load('./model/'+'model_v.pt'), -2)\n",
    "\n",
    "logging.info(f'best w on test:{w_accu} accuracy; best v on test:{v_accu} accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
