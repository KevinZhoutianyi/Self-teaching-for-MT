{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer,AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length,target_language\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 10, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 200, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = 50, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=16,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=4,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='google/t5-small-lm-adapt',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='google/t5-small-lm-adapt',      help='model_name')\n",
    "parser.add_argument('--model_name_de2en', type=str,             default='Onlydrinkwater/t5-small-de-en-mt',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='T5spec',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=50,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=80,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=500,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-3,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.98,    help='momentum')\n",
    "parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=1,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=0.7,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.5,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.5,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=1 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "args.test_num = args.test_num//args.batch_size * args.batch_size\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5_newA\\wandb\\run-20220618_210442-3qv6roro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/3qv6roro\" target=\"_blank\">T5spec</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/3qv6roro?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x179e0ff2e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "wandb.init(project=\"Selftraining\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18 09:04:45 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18 09:04:46 PM |\t  Namespace(A_lr=0.001, batch_size=16, beta1=0.9, beta2=0.98, decay=0.001, decay_lr=0.7, epochs=500, exp_name='T5spec', gpu=0, grad_acc_count=-1, grad_clip=1, learning_rate_min=1e-08, model_name_de2en='Onlydrinkwater/t5-small-de-en-mt', model_name_student='google/t5-small-lm-adapt', model_name_teacher='google/t5-small-lm-adapt', num_step_lr=1, num_workers=0, pre_epochs=0, rep_num=48, syndata_loss_ratio=0.5, test_num=80, test_num_points=50, train_A=1, train_A_num_points=4, train_num_points=200, train_v_num_points=0, train_v_synthetic_num_points=8, train_w_num_points=4, traindata_loss_ratio=0.5, unrolled_v_lr=0.001, unrolled_w_lr=0.001, v_lr=0.001, valid_begin=1, valid_num_points=10, w_lr=0.001, warm=10)\n",
      "06/18 09:04:46 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "06/18 09:04:46 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18 09:04:47 PM |\t  modelsize:76.961152MB\n",
      "06/18 09:04:48 PM |\t  modelsize:76.961152MB\n",
      "06/18 09:04:50 PM |\t  modelsize:76.961152MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name_teacher\n",
    "pretrained  =  AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "\n",
    "if(exists(pathname+'.pt')==False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained,pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained  =  AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt')==False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained,pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_de2en\n",
    "pretrained  =  AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt')==False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained,pathname+'.pt')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18 09:04:52 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-fcff064badad2159.arrow\n",
      "06/18 09:04:53 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-ef861152e003e0c7.arrow\n",
      "06/18 09:04:53 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-848b25adc701ab2b.arrow\n",
      "06/18 09:04:53 PM |\t  train len: 192\n",
      "06/18 09:04:53 PM |\t  train_w_num_points_len: 48\n",
      "06/18 09:04:53 PM |\t  train_v_synthetic_num_points_len: 96\n",
      "06/18 09:04:53 PM |\t  train_v_num_points_len: 0\n",
      "06/18 09:04:53 PM |\t  train_A_num_points_len: 48\n",
      "06/18 09:04:53 PM |\t  valid len: 10\n",
      "06/18 09:04:53 PM |\t  test len: 50\n",
      "06/18 09:04:53 PM |\t  {'de': 'Dank unseres Personals und Geräteparks sind wir täglich rund um die Uhr in der Lage, uns allen erdenklichen Herausforderungen zu stellen.', 'en': 'translate English to German: Our staff and equipment stands ready to answer any challenges 24/7.'}\n",
      "06/18 09:04:53 PM |\t  {'de': 'Diese Entscheidung rief in der Öffentlichkeit eine lebhafte Diskussion hervor.', 'en': 'translate English to German: The resolution caused lively public debate.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess the data, make a dataloader\n",
    "import random\n",
    "modelname = args.model_name_teacher\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#teacher shouldn't have label smoothing, especially when student got same size.\n",
    "criterion_v = torch.nn.CrossEntropyLoss( reduction='none')#,label_smoothing=args.smoothing) #without LS, V may be too confident to that syn data, and LS do well for real data also.\n",
    "\n",
    "\n",
    "\n",
    "train = dataset['train'].shuffle(seed=seed_).select(range(args.train_num_points))\n",
    "valid = dataset['validation'].shuffle(seed=seed_).select(range(args.valid_num_points))\n",
    "test = dataset['test'].shuffle(seed=seed_).select(range(args.test_num_points))#[L_t+L_v:L_t+L_v+L_test]\n",
    "train = train['translation']\n",
    "valid = valid['translation']\n",
    "test = test['translation']\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en']  #needed for T5\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "#TODO: Syn_input should be monolingual data, should try en-fo's en. cuz wmt may align\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "\n",
    "'''\n",
    "each mini batch consist of : \n",
    "1. data to train W\n",
    "2. monolingual data to generate parallel data\n",
    "3. data to train V\n",
    "4. data to train A\n",
    "'''\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "# logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "06/18 09:04:53 PM |\t  train data get\n",
      "06/18 09:04:53 PM |\t  train data loader get\n",
      "06/18 09:04:53 PM |\t  valid data loader get\n",
      "06/18 09:04:53 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0266,  0.0244,  0.0231,  ...,  0.0285, -0.0229, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0100], device='cuda:0', requires_grad=True)\n",
      "! [Parameter containing:\n",
      "tensor([[-0.0266,  0.0244,  0.0231,  ...,  0.0285, -0.0229, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0100], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = attention_params(args)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(),  lr= args.w_lr ,  betas=(args.beta1, args.beta2) ,eps=1e-9 )\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_w  =   StepLR(w_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion_v, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(),  lr= args.v_lr ,  betas=(args.beta1,args.beta2) ,eps=1e-9  )\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_v  =   StepLR(v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    # metric_bleu =  load_metric('bleu')\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        \n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        pre = model.generate(test_dataloaderx)\n",
    "        x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "        pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "        label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "        \n",
    "        pred_str = [x  for x in pred_decoded]\n",
    "        label_str = [[x] for x in label_decoded]\n",
    "        # pred_list = [x.split()  for x in pred_decoded]\n",
    "        # label_list = [[x.split()] for x in label_decoded]\n",
    "        metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "        # metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "        if  step==0:\n",
    "            logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "            logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "            logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            \n",
    "            \n",
    "    logging.info('computing score...') \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    # bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    # logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    # del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter):\n",
    "\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.train_w_num_points\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points\n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size = [wsize, synsize, vsize, Asize]\n",
    "    bs = args.batch_size\n",
    "    w_model.eval()\n",
    "    v_model.eval()\n",
    "\n",
    "    logging.info(f\"split size:{split_size}\")\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        tot_iter[0] += bs\n",
    "        \n",
    "\n",
    "        # logging.info(f\"GPU mem :{getGPUMem(device)}%\")\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        (input_w, input_syn, input_v, input_A_v) = torch.split(train_x, split_size)\n",
    "        (input_w_attn, input_syn_attn, input_v_attn,\n",
    "         input_A_v_attn) = torch.split(train_x_attn, split_size)\n",
    "        (output_w, _, output_v, output_A_v) = torch.split(train_y, split_size)\n",
    "        (output_w_attn, _, output_v_attn, output_A_v_attn) = torch.split(\n",
    "            train_y_attn, split_size)\n",
    "        # attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "        if(True):# let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            output_v_attn = output_w_attn\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (args.train_A == 1):\n",
    "            epsilon_w = args.unrolled_w_lr\n",
    "            epsilon_v  = args.unrolled_v_lr\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, output_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, output_v_attn, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, output_A_v_attn, v_optimizer,\n",
    "                                             epsilon_w, epsilon_v)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "                            \n",
    "        w_optimizer.zero_grad()\n",
    "        loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                          output_w_attn, A, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "\n",
    "\n",
    "        v_optimizer.zero_grad()\n",
    "        loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "\n",
    "        loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                        output_v_attn, v_model)\n",
    "        v_loss = (args.traindata_loss_ratio*loss +\n",
    "                  loss_aug*args.syndata_loss_ratio)\n",
    "        v_trainloss_acc += v_loss.item()\n",
    "        v_loss.backward()\n",
    "        objs_v_syn.update(loss_aug.item(), synsize)\n",
    "        objs_v_train.update(loss.item(), vsize)\n",
    "        v_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valloss = my_loss2(input_A_v, input_A_v_attn,  output_A_v, output_A_v_attn,v_model)\n",
    "            objs_v_val.update(valloss.item(), Asize)\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            my_test(validdataloader, model_w, epoch)\n",
    "            my_test(validdataloader, model_v, epoch)\n",
    "            # logging.info(str((\"Attention Weights A : \", A.ReLU(A.alpha))))\n",
    "            torch.save(model_w,'./model/'+'model_w.pt')#+now+\n",
    "            torch.save(model_v,'./model/'+'model_v.pt')\n",
    "            torch.save(A,'./model/'+'A.pt')\n",
    "            torch.save(model_w,os.path.join(wandb.run.dir, \"model_w.pt\"))\n",
    "            torch.save(model_v,os.path.join(wandb.run.dir, \"model_v.pt\"))\n",
    "            torch.save(A,os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "            wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "\n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info(f\"{progress:5.3}%:\\t  W_train_loss:{objs_w.avg:^.7f}\\tV_train_syn_loss:{objs_v_syn.avg:^.7f}\\tV_train_loss:{objs_v_train.avg:^.7f}\\t  V_star_val_loss:{objs_v_star_val.avg:^.7f}\\t  V_val_loss:{objs_v_val.avg:^.7f}\")\n",
    "            logging.info(f\"{A(input_w, input_w_attn, output_w,output_w_attn)}\")\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_val.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "\n",
    "    return w_trainloss_acc, v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/18 09:34:45 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:7.979226629761193e-07,\t\tlr_v:7.979226629761193e-07,\t\tlr_A:7.979226629761193e-07----------------\n",
      "06/18 09:34:45 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/18 09:35:01 PM |\t   18.2%:\t  W_train_loss:0.0143297\tV_train_syn_loss:1.0447528\tV_train_loss:0.1957237\t  V_star_val_loss:7.0860985\t  V_val_loss:6.9723271\n",
      "06/18 09:35:01 PM |\t  tensor([0.2221, 0.4002, 0.3639, 0.2220], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19544/877182758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19544/1674366172.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_iter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_num\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtot_iter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvaliddataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvaliddataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# logging.info(str((\"Attention Weights A : \", A.ReLU(A.alpha))))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19544/2931650421.py\u001b[0m in \u001b[0;36mmy_test\u001b[1;34m(_dataloader, model, epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0macc\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaderx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mx_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaderx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpred_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5_newA\\T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m## sampling with top_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1313\u001b[0m             )\n\u001b[0;32m   1314\u001b[0m             \u001b[1;31m# 12. run beam search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1316\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   2159\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[1;31m# Decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1639\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 )\n\u001b[0;32m   1032\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1034\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;31m# Apply Feed Forward layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mhidden_gelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\activations.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.044715\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin==1):\n",
    "#     my_test(valid_dataloader,model_w,-1) #before train\n",
    "#     my_test(valid_dataloader,model_v,-1)  \n",
    "\n",
    "tot_iter = [0]\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v,tot_iter)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "    architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    # wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6276, 0.9817, 0.8305], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.ReLU(A.alpha[[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en2de = (torch.load(args.model_name_teacher.replace('/','')+'.pt')).encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.2656e+00, -5.7422e-01, -1.9336e-01,  ...,  3.6133e-01,\n",
      "          3.7500e-01,  3.3594e+00],\n",
      "        [-1.3562e+01,  8.8750e+00, -9.8145e-02,  ...,  1.1250e+01,\n",
      "          5.5312e+00,  9.2000e+01],\n",
      "        [ 9.8125e+00,  5.1875e+00, -4.1875e+00,  ..., -7.5000e+00,\n",
      "          1.2812e+01,  1.8875e+01],\n",
      "        ...,\n",
      "        [-4.5117e-01, -3.3594e-01, -3.8867e-01,  ..., -2.0996e-01,\n",
      "         -2.0000e+00, -9.1406e-01],\n",
      "        [-1.0234e+00, -8.0859e-01,  4.3555e-01,  ..., -5.9326e-02,\n",
      "         -9.2188e-01, -9.2969e-01],\n",
      "        [ 1.0078e+00,  1.5234e-01, -2.4902e-01,  ..., -1.8555e-01,\n",
      "         -2.7148e-01,  1.7969e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0102,  0.0608, -0.0903,  ..., -0.0889, -0.0732,  0.0432],\n",
      "        [ 0.0898,  0.0156,  0.0618,  ..., -0.0175,  0.0140, -0.1162],\n",
      "        [-0.0776, -0.0332, -0.0967,  ...,  0.1523,  0.0613,  0.0449],\n",
      "        ...,\n",
      "        [-0.0110, -0.0140, -0.1235,  ...,  0.0278,  0.0461, -0.0192],\n",
      "        [ 0.0669, -0.0437, -0.0181,  ...,  0.0581,  0.0294,  0.0298],\n",
      "        [ 0.0713,  0.0050, -0.0840,  ..., -0.0366, -0.0603, -0.0874]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1729,  0.3848,  0.4531,  ..., -0.5039,  0.4902,  0.0962],\n",
      "        [-0.4629, -1.0703, -0.1011,  ..., -1.7109, -0.2373, -1.2188],\n",
      "        [-0.1348, -0.8438, -0.2412,  ...,  0.3945, -0.2832,  0.0889],\n",
      "        ...,\n",
      "        [-0.5078, -0.2363,  0.0153,  ..., -0.4609, -0.4844, -0.1152],\n",
      "        [-0.2441,  0.2041, -0.4375,  ..., -0.0437, -0.1670,  0.2773],\n",
      "        [ 0.3711, -0.0991, -0.1514,  ..., -0.1221, -0.1660, -0.0344]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6719,  0.3008, -0.0053,  ...,  0.2852,  0.4102, -0.0398],\n",
      "        [ 0.1016, -0.0217,  0.5234,  ...,  0.9336,  0.3379, -0.3848],\n",
      "        [-0.1836, -0.4062,  0.1953,  ...,  0.0245,  0.1924,  0.0476],\n",
      "        ...,\n",
      "        [ 0.6406, -0.6680,  0.1387,  ...,  0.3047,  0.3359,  0.1216],\n",
      "        [ 0.5391,  0.3340, -0.1592,  ..., -0.2275,  0.0187,  0.2637],\n",
      "        [-0.0457, -0.4551, -0.2441,  ...,  0.1040, -0.0674, -0.2754]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5195, -0.1904,  0.4258,  ..., -0.1797,  0.2949,  0.6250],\n",
      "        [-0.6172, -0.0085,  0.5078,  ..., -0.1504, -0.5156,  0.1895],\n",
      "        [ 0.2344, -0.1689, -1.0703,  ..., -0.2041, -0.7031, -0.1445],\n",
      "        ...,\n",
      "        [ 0.2393, -0.8242,  0.0427,  ...,  0.1045, -0.2139, -0.2988],\n",
      "        [-0.4922,  1.1094, -0.2969,  ..., -0.2500,  0.4258, -0.5039],\n",
      "        [ 0.3613,  0.1670,  0.1045,  ..., -0.0491, -0.2500,  0.8672]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[  5.2500,  -8.1250,   2.7344,  -9.5000,   4.4688, -17.8750],\n",
      "        [  7.6875,   8.1875,   0.7891,   3.3125,   7.4062,   5.5312],\n",
      "        [  5.7812,   6.4375,   1.1172,   3.5000,   7.4375,   5.7500],\n",
      "        [  4.8125,   5.3750,   1.1016,   3.6406,   7.0938,   5.6875],\n",
      "        [  4.2812,   4.6562,   1.1562,   3.7500,   6.7812,   5.6250],\n",
      "        [  3.9375,   4.1250,   1.1172,   3.7812,   6.5312,   5.5312],\n",
      "        [  3.6406,   3.6562,   1.1172,   3.8438,   6.2500,   5.4688],\n",
      "        [  3.4062,   3.3125,   1.1719,   3.8750,   6.0625,   5.4062],\n",
      "        [  2.9062,   2.4844,   1.0859,   3.9688,   5.5938,   5.2500],\n",
      "        [  2.3750,   1.6406,   1.1172,   3.9844,   4.9688,   5.0625],\n",
      "        [  1.9141,   0.8633,   0.9453,   4.0000,   4.3750,   4.8438],\n",
      "        [  1.5078,   0.1553,   0.7812,   4.0625,   3.6094,   4.5625],\n",
      "        [  1.1328,  -0.2217,   0.5625,   4.0938,   2.8438,   4.2500],\n",
      "        [  0.6875,  -0.9688,   0.2490,   4.1250,   2.0938,   3.8750],\n",
      "        [  0.3965,  -1.8203,   0.0786,   4.1250,   1.5000,   3.5781],\n",
      "        [ -1.3984,  -0.9883,  -1.6797,   4.1562,  -0.7383,   2.2812],\n",
      "        [  0.0801,  -0.0801,   0.3340,  -0.3594,   0.3145,  -0.2793],\n",
      "        [  3.5469,   0.1250,   8.5625,   4.2188,   1.1875, -16.8750],\n",
      "        [  3.2500,   0.2578,   7.4062,   4.4062,   1.3594,   2.5312],\n",
      "        [  3.1250,   0.3418,   6.7188,   4.5312,   1.4922,   2.5938],\n",
      "        [  3.0469,   0.3867,   6.2812,   4.5625,   1.4531,   2.5938],\n",
      "        [  2.8906,   0.2471,   5.9375,   4.6250,   1.2188,   2.5781],\n",
      "        [  2.8281,   0.3887,   5.6562,   4.6250,   1.5078,   2.6250],\n",
      "        [  2.8594,   0.3203,   5.4375,   4.6562,   1.3750,   2.5469],\n",
      "        [  2.7500,   0.3984,   5.0000,   4.6875,   1.4219,   2.5156],\n",
      "        [  2.6406,   0.3516,   4.4375,   4.7188,   1.3750,   2.4531],\n",
      "        [  2.4375,   0.3828,   3.8906,   4.7188,   1.2969,   2.3125],\n",
      "        [  2.1562,   0.3535,   3.2812,   4.7188,   1.2031,   2.2188],\n",
      "        [  1.7188,   0.2715,   2.7344,   4.7188,   1.0469,   2.1250],\n",
      "        [  1.1094,   0.2090,   2.1094,   4.6875,   0.9531,   1.9766],\n",
      "        [  0.5039,   0.1260,   1.7109,   4.6562,   0.8438,   1.8906],\n",
      "        [ -4.6562,   0.2188,   0.3398,   4.7500,   0.1406,   1.5859]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0620, 0.0830, 0.0679, 0.0835, 0.0757, 0.0767, 0.1079, 0.0811, 0.0613,\n",
      "        0.1025, 0.0781, 0.0771, 0.0786, 0.0752, 0.0723, 0.0942, 0.0723, 0.0625,\n",
      "        0.1055, 0.0635, 0.0854, 0.0635, 0.1011, 0.0913, 0.0645, 0.0674, 0.0625,\n",
      "        0.0903, 0.0771, 0.0815, 0.0776, 0.0938, 0.1177, 0.0801, 0.0664, 0.0786,\n",
      "        0.0913, 0.0767, 0.0737, 0.0554, 0.0830, 0.0674, 0.0874, 0.0552, 0.0698,\n",
      "        0.1357, 0.1064, 0.0698, 0.0718, 0.0623, 0.0618, 0.0767, 0.0771, 0.0703,\n",
      "        0.0981, 0.0728, 0.0613, 0.0947, 0.0737, 0.0723, 0.0771, 0.0801, 0.0850,\n",
      "        0.0791, 0.0967, 0.0786, 0.0640, 0.0654, 0.1187, 0.0869, 0.0913, 0.0591,\n",
      "        0.0708, 0.0757, 0.1025, 0.0825, 0.0679, 0.0713, 0.0620, 0.0530, 0.0898,\n",
      "        0.0723, 0.0840, 0.0903, 0.0649, 0.0835, 0.0815, 0.0762, 0.0776, 0.1055,\n",
      "        0.0645, 0.0571, 0.0869, 0.0767, 0.0713, 0.0713, 0.0669, 0.0791, 0.0752,\n",
      "        0.0708, 0.0796, 0.0752, 0.0698, 0.0635, 0.0703, 0.0752, 0.0535, 0.0776,\n",
      "        0.0598, 0.0830, 0.0762, 0.0815, 0.1118, 0.0598, 0.0742, 0.0723, 0.0525,\n",
      "        0.0728, 0.0752, 0.0776, 0.0815, 0.1069, 0.0840, 0.0850, 0.0713, 0.0767,\n",
      "        0.0894, 0.0845, 0.0562, 0.0654, 0.0811, 0.0815, 0.0654, 0.0674, 0.0781,\n",
      "        0.0762, 0.1523, 0.0586, 0.0708, 0.0898, 0.0728, 0.1118, 0.0776, 0.0869,\n",
      "        0.0898, 0.0938, 0.0737, 0.0791, 0.0869, 0.0610, 0.0918, 0.1104, 0.0732,\n",
      "        0.0830, 0.0654, 0.0913, 0.0786, 0.1035, 0.0923, 0.0869, 0.0698, 0.0938,\n",
      "        0.0879, 0.0503, 0.0732, 0.0903, 0.0747, 0.0820, 0.0776, 0.1045, 0.0688,\n",
      "        0.0786, 0.0737, 0.0747, 0.0747, 0.0781, 0.0869, 0.0491, 0.0718, 0.0737,\n",
      "        0.0781, 0.0659, 0.0952, 0.0737, 0.0776, 0.0781, 0.0913, 0.1016, 0.0801,\n",
      "        0.0796, 0.0645, 0.1089, 0.0684, 0.1069, 0.0791, 0.0522, 0.0854, 0.0864,\n",
      "        0.0825, 0.0708, 0.0645, 0.0840, 0.0649, 0.0806, 0.0542, 0.0747, 0.0762,\n",
      "        0.0703, 0.0684, 0.0913, 0.1089, 0.0698, 0.0864, 0.0640, 0.0708, 0.0879,\n",
      "        0.0894, 0.0752, 0.0825, 0.1147, 0.0728, 0.0552, 0.0835, 0.0898, 0.0811,\n",
      "        0.0540, 0.0742, 0.0732, 0.0796, 0.0864, 0.0645, 0.0603, 0.0601, 0.0703,\n",
      "        0.0811, 0.0659, 0.0718, 0.0933, 0.0859, 0.0845, 0.0854, 0.0771, 0.0811,\n",
      "        0.0679, 0.0820, 0.0830, 0.0913, 0.0840, 0.0679, 0.0801, 0.0718, 0.0791,\n",
      "        0.0708, 0.0723, 0.0674, 0.0649, 0.0791, 0.0830, 0.0776, 0.0889, 0.0884,\n",
      "        0.0679, 0.0669, 0.0703, 0.1050, 0.0815, 0.0757, 0.0845, 0.0654, 0.0669,\n",
      "        0.0742, 0.0728, 0.0786, 0.0718, 0.0996, 0.0688, 0.0786, 0.0942, 0.0737,\n",
      "        0.0864, 0.0688, 0.0757, 0.0786, 0.1084, 0.0684, 0.0688, 0.0908, 0.0613,\n",
      "        0.1123, 0.0684, 0.0742, 0.0544, 0.0801, 0.0649, 0.0786, 0.0737, 0.0564,\n",
      "        0.0510, 0.0879, 0.0630, 0.1069, 0.0806, 0.0703, 0.1040, 0.0601, 0.1055,\n",
      "        0.0942, 0.0625, 0.0527, 0.0693, 0.0684, 0.0879, 0.0889, 0.0850, 0.0874,\n",
      "        0.1099, 0.0806, 0.0786, 0.0996, 0.0713, 0.1055, 0.0645, 0.0659, 0.1147,\n",
      "        0.0596, 0.0645, 0.0781, 0.0806, 0.0791, 0.0830, 0.0806, 0.0825, 0.0889,\n",
      "        0.0396, 0.0723, 0.0574, 0.0752, 0.0898, 0.0781, 0.0630, 0.0669, 0.0728,\n",
      "        0.0776, 0.0752, 0.0674, 0.0747, 0.0972, 0.0781, 0.0684, 0.0654, 0.0757,\n",
      "        0.0825, 0.0625, 0.0825, 0.0854, 0.0806, 0.0845, 0.0771, 0.0991, 0.0723,\n",
      "        0.0737, 0.0737, 0.0864, 0.0771, 0.0659, 0.0820, 0.0771, 0.0256, 0.0952,\n",
      "        0.0825, 0.0923, 0.0845, 0.1108, 0.0820, 0.0645, 0.0703, 0.0928, 0.0664,\n",
      "        0.0654, 0.0732, 0.0879, 0.0698, 0.0718, 0.0693, 0.0693, 0.0840, 0.1328,\n",
      "        0.0942, 0.0728, 0.0552, 0.0747, 0.0791, 0.0830, 0.0747, 0.1025, 0.0684,\n",
      "        0.0898, 0.0728, 0.0703, 0.0654, 0.0579, 0.0688, 0.0874, 0.0713, 0.0708,\n",
      "        0.0854, 0.0869, 0.0850, 0.1025, 0.0688, 0.0708, 0.0723, 0.0518, 0.0664,\n",
      "        0.0283, 0.0864, 0.0698, 0.0688, 0.0593, 0.0908, 0.0742, 0.0864, 0.0767,\n",
      "        0.0598, 0.0527, 0.0806, 0.0879, 0.0811, 0.0613, 0.0664, 0.0564, 0.0898,\n",
      "        0.0767, 0.0791, 0.0854, 0.0908, 0.0762, 0.0757, 0.0762, 0.0894, 0.0708,\n",
      "        0.0591, 0.0669, 0.0698, 0.0713, 0.0820, 0.0586, 0.1177, 0.0947, 0.0669,\n",
      "        0.0649, 0.0840, 0.0664, 0.0703, 0.1050, 0.0669, 0.0574, 0.0620, 0.0752,\n",
      "        0.0610, 0.0986, 0.0806, 0.0791, 0.0559, 0.0659, 0.0835, 0.0664, 0.0669,\n",
      "        0.0835, 0.0869, 0.0693, 0.0830, 0.0898, 0.0635, 0.0698, 0.1147, 0.0830,\n",
      "        0.0723, 0.0491, 0.0674, 0.0801, 0.0752, 0.0806, 0.0977, 0.0747, 0.0806,\n",
      "        0.1167, 0.0889, 0.1099, 0.0815, 0.0718, 0.0649, 0.0708, 0.1045, 0.0928,\n",
      "        0.0635, 0.0781, 0.0991, 0.0820, 0.0957, 0.0903, 0.1099, 0.0884, 0.0952,\n",
      "        0.0840, 0.0620, 0.0752, 0.0767, 0.0645, 0.0811, 0.0718, 0.0938],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1895,  0.3398,  0.5000,  ..., -0.6758, -0.3027,  0.9961],\n",
      "        [ 0.1357, -0.0674, -0.0557,  ..., -1.0312, -0.6992, -0.0649],\n",
      "        [-0.4727,  0.7773, -0.7891,  ..., -0.6641, -0.0859, -0.3066],\n",
      "        ...,\n",
      "        [-0.9062, -0.9180, -0.8164,  ...,  0.4258, -0.0120,  0.0684],\n",
      "        [-0.1250, -0.1787,  0.2559,  ..., -0.8984, -0.0337, -0.3691],\n",
      "        [-0.4297,  0.1064, -0.1128,  ..., -0.3184,  0.1719, -0.4863]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2891, -0.5391,  0.0737,  ..., -0.1494, -0.4141, -0.1699],\n",
      "        [ 0.2637, -0.5469,  1.1875,  ..., -0.7852,  0.7617,  0.2471],\n",
      "        [-0.0342,  1.0781,  0.4375,  ...,  0.6172,  0.0791, -0.1582],\n",
      "        ...,\n",
      "        [-0.0293, -0.0332,  0.4668,  ...,  0.3223,  0.4336,  0.0417],\n",
      "        [-0.9141, -0.4492, -0.5352,  ...,  0.6914, -1.2031,  0.1094],\n",
      "        [ 0.3496,  0.5938, -0.0449,  ...,  0.9570,  0.2148, -0.2100]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0153, -0.0022,  0.2578,  ...,  0.3086,  0.2207, -0.1660],\n",
      "        [-0.1768,  0.4375, -0.0184,  ..., -0.2754,  0.2949,  0.1709],\n",
      "        [ 0.3027, -0.1748, -0.3418,  ..., -0.1226, -0.0126, -0.1572],\n",
      "        ...,\n",
      "        [ 0.0464, -0.2119, -0.0737,  ..., -0.0128, -0.1748, -0.0430],\n",
      "        [ 0.2412,  0.0576,  0.4473,  ...,  0.1602,  0.3672,  0.3301],\n",
      "        [ 1.1016,  0.0786, -0.3457,  ..., -0.2949,  0.2617,  0.0630]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1055, 0.0947, 0.0505, 0.0903, 0.0752, 0.0728, 0.1118, 0.0825, 0.0608,\n",
      "        0.0889, 0.0835, 0.0479, 0.0762, 0.0771, 0.0630, 0.1021, 0.0786, 0.0603,\n",
      "        0.1055, 0.0618, 0.0947, 0.0688, 0.0986, 0.0859, 0.0645, 0.0505, 0.0596,\n",
      "        0.0815, 0.0737, 0.0659, 0.0737, 0.0981, 0.1387, 0.0762, 0.0605, 0.0737,\n",
      "        0.0957, 0.0588, 0.0825, 0.0535, 0.0913, 0.0698, 0.0859, 0.0586, 0.0747,\n",
      "        0.1270, 0.1021, 0.0669, 0.0635, 0.0618, 0.0601, 0.0835, 0.0625, 0.0693,\n",
      "        0.0840, 0.0688, 0.0649, 0.1016, 0.0713, 0.0737, 0.0796, 0.0757, 0.0781,\n",
      "        0.0830, 0.1235, 0.0728, 0.0576, 0.0613, 0.1084, 0.0669, 0.1055, 0.0608,\n",
      "        0.0957, 0.0698, 0.0986, 0.0830, 0.0623, 0.0669, 0.0359, 0.0549, 0.0791,\n",
      "        0.0718, 0.0884, 0.0850, 0.0569, 0.0781, 0.0811, 0.0728, 0.0640, 0.1030,\n",
      "        0.0654, 0.0581, 0.0884, 0.0737, 0.0825, 0.0620, 0.0664, 0.0776, 0.0610,\n",
      "        0.0703, 0.0771, 0.0718, 0.0615, 0.0649, 0.0801, 0.0693, 0.0659, 0.0688,\n",
      "        0.0552, 0.0767, 0.0757, 0.0723, 0.1167, 0.0552, 0.0659, 0.0630, 0.0427,\n",
      "        0.0659, 0.0603, 0.0654, 0.0713, 0.0942, 0.0874, 0.0811, 0.0669, 0.0747,\n",
      "        0.0762, 0.0791, 0.0571, 0.0659, 0.0908, 0.0854, 0.0757, 0.0723, 0.0615,\n",
      "        0.0791, 0.1914, 0.0615, 0.0737, 0.0967, 0.0613, 0.1006, 0.0723, 0.0713,\n",
      "        0.0737, 0.0898, 0.0659, 0.0649, 0.0723, 0.0576, 0.0898, 0.0972, 0.0718,\n",
      "        0.0874, 0.0623, 0.0742, 0.0820, 0.1006, 0.0850, 0.0903, 0.0791, 0.0947,\n",
      "        0.0952, 0.0317, 0.0659, 0.0811, 0.0615, 0.0923, 0.0737, 0.0986, 0.0684,\n",
      "        0.0708, 0.0723, 0.0703, 0.0874, 0.0698, 0.0762, 0.0483, 0.0796, 0.0845,\n",
      "        0.0742, 0.0530, 0.0996, 0.0669, 0.0635, 0.0791, 0.0991, 0.0894, 0.0767,\n",
      "        0.0781, 0.0601, 0.1079, 0.0649, 0.1118, 0.0781, 0.0471, 0.0767, 0.0776,\n",
      "        0.0835, 0.0791, 0.0623, 0.0820, 0.0728, 0.0820, 0.0825, 0.0674, 0.0771,\n",
      "        0.0593, 0.0825, 0.0933, 0.1040, 0.0742, 0.0845, 0.0635, 0.0791, 0.0786,\n",
      "        0.0703, 0.0752, 0.0693, 0.1001, 0.0708, 0.0557, 0.0796, 0.0820, 0.0684,\n",
      "        0.0684, 0.0693, 0.0796, 0.0757, 0.0908, 0.0537, 0.0630, 0.0645, 0.0718,\n",
      "        0.0952, 0.0713, 0.0708, 0.1040, 0.0879, 0.0703, 0.0698, 0.0874, 0.0752,\n",
      "        0.0586, 0.0752, 0.0732, 0.0830, 0.0752, 0.0752, 0.0713, 0.0957, 0.0815,\n",
      "        0.0679, 0.0625, 0.0591, 0.0659, 0.0684, 0.0786, 0.0825, 0.0762, 0.0952,\n",
      "        0.0579, 0.0649, 0.0776, 0.0967, 0.0801, 0.0762, 0.0864, 0.0820, 0.0615,\n",
      "        0.0728, 0.0635, 0.0684, 0.0767, 0.0845, 0.0603, 0.0752, 0.0894, 0.0747,\n",
      "        0.0659, 0.0615, 0.0659, 0.0703, 0.1162, 0.0664, 0.0649, 0.0869, 0.0566,\n",
      "        0.0967, 0.0703, 0.0664, 0.0518, 0.0845, 0.0684, 0.0791, 0.0693, 0.0728,\n",
      "        0.0464, 0.0908, 0.0603, 0.1089, 0.0649, 0.0649, 0.0938, 0.0625, 0.0967,\n",
      "        0.0967, 0.0574, 0.0508, 0.0649, 0.0674, 0.0854, 0.0894, 0.0796, 0.0806,\n",
      "        0.0962, 0.0747, 0.0742, 0.1040, 0.0737, 0.1001, 0.0645, 0.0625, 0.1138,\n",
      "        0.0664, 0.0674, 0.0723, 0.0698, 0.0771, 0.0811, 0.0835, 0.0845, 0.0986,\n",
      "        0.0449, 0.0796, 0.0623, 0.0718, 0.0742, 0.0713, 0.1167, 0.0767, 0.0669,\n",
      "        0.0698, 0.0613, 0.0649, 0.0791, 0.0918, 0.0649, 0.0654, 0.0664, 0.0801,\n",
      "        0.0776, 0.0928, 0.0684, 0.0737, 0.0688, 0.0679, 0.0713, 0.0952, 0.0620,\n",
      "        0.0801, 0.0767, 0.0898, 0.0684, 0.0581, 0.0859, 0.0723, 0.0347, 0.0898,\n",
      "        0.0820, 0.1001, 0.0752, 0.1196, 0.0811, 0.0703, 0.0742, 0.0864, 0.0708,\n",
      "        0.0654, 0.0713, 0.0879, 0.0747, 0.0742, 0.0688, 0.0664, 0.0781, 0.1167,\n",
      "        0.0918, 0.0654, 0.0554, 0.0742, 0.0693, 0.0835, 0.0796, 0.1030, 0.0737,\n",
      "        0.0952, 0.0723, 0.0664, 0.0752, 0.0664, 0.0623, 0.0791, 0.0684, 0.0723,\n",
      "        0.0742, 0.0747, 0.0757, 0.0986, 0.0742, 0.0688, 0.0659, 0.0645, 0.0547,\n",
      "        0.0305, 0.0801, 0.0649, 0.0757, 0.0591, 0.0991, 0.0605, 0.0840, 0.0698,\n",
      "        0.0513, 0.0413, 0.0776, 0.0742, 0.0781, 0.0693, 0.0723, 0.0544, 0.0879,\n",
      "        0.0649, 0.0718, 0.0825, 0.0830, 0.0776, 0.0801, 0.0752, 0.0737, 0.0684,\n",
      "        0.0503, 0.0608, 0.0693, 0.0786, 0.0845, 0.0557, 0.1104, 0.0742, 0.0562,\n",
      "        0.0688, 0.0786, 0.0635, 0.0654, 0.0991, 0.0630, 0.0771, 0.0623, 0.0649,\n",
      "        0.0635, 0.1030, 0.0869, 0.0703, 0.0520, 0.0732, 0.0835, 0.0664, 0.0457,\n",
      "        0.0806, 0.0933, 0.0659, 0.0874, 0.0732, 0.0703, 0.0679, 0.0981, 0.0879,\n",
      "        0.0693, 0.0464, 0.0728, 0.0762, 0.0791, 0.0708, 0.0913, 0.0732, 0.0806,\n",
      "        0.1128, 0.0850, 0.0933, 0.0854, 0.0664, 0.0591, 0.0588, 0.0835, 0.0854,\n",
      "        0.0562, 0.0791, 0.0957, 0.0781, 0.0889, 0.0796, 0.0986, 0.0664, 0.0874,\n",
      "        0.0752, 0.0613, 0.0688, 0.0835, 0.0615, 0.0728, 0.0684, 0.1562],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0164, -0.0016, -0.0664,  ...,  0.0776, -0.0918, -0.1484],\n",
      "        [ 0.0574, -0.0187, -0.0286,  ...,  0.0435,  0.0068, -0.0952],\n",
      "        [-0.0525,  0.0977, -0.0625,  ...,  0.0811, -0.0261, -0.0300],\n",
      "        ...,\n",
      "        [-0.0400, -0.1030, -0.0378,  ...,  0.0160, -0.0161,  0.0396],\n",
      "        [ 0.0483,  0.0569, -0.0342,  ..., -0.0342,  0.0417,  0.0544],\n",
      "        [ 0.0640,  0.0038,  0.0757,  ...,  0.0413,  0.0208,  0.0135]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8164, -0.3750,  0.0371,  ..., -0.4336,  0.5391,  0.0073],\n",
      "        [ 1.0547, -0.4961,  0.0025,  ..., -0.8984,  0.0850, -0.6133],\n",
      "        [ 0.2129, -0.4062,  0.2793,  ...,  0.5625, -0.3789,  0.0198],\n",
      "        ...,\n",
      "        [ 0.2393,  0.0425, -0.3008,  ...,  0.2832,  0.1328,  0.0669],\n",
      "        [ 0.2070, -0.0167, -0.4180,  ...,  0.1963, -0.2949,  0.3906],\n",
      "        [-0.2051,  0.2754,  0.5352,  ...,  0.5312,  0.5859, -0.1289]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6797, -0.2969,  0.1406,  ...,  0.0067,  0.0640, -0.0344],\n",
      "        [-0.2891, -0.1768,  0.9023,  ...,  0.8281, -0.2812,  0.0267],\n",
      "        [-0.4297, -0.0752,  0.2217,  ...,  0.5391,  0.5039,  0.1064],\n",
      "        ...,\n",
      "        [ 0.2314,  0.0155,  0.2412,  ...,  0.1157,  0.2949, -0.1494],\n",
      "        [-0.0815, -0.5547,  0.5078,  ...,  0.2412, -0.0205, -0.1650],\n",
      "        [-0.0540, -0.4668, -0.3984,  ..., -0.1924, -0.2295,  0.0496]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3301, -0.0148,  0.3359,  ..., -0.2324,  0.7734,  0.1348],\n",
      "        [ 0.6055,  0.2207, -0.2285,  ..., -0.4160, -0.8984,  0.6016],\n",
      "        [ 0.3398, -0.3320, -0.0559,  ...,  0.0422,  0.1279,  0.0425],\n",
      "        ...,\n",
      "        [-0.2422,  0.1699,  0.5078,  ...,  1.1406, -0.7266, -0.2236],\n",
      "        [ 1.0078,  0.9102,  0.6250,  ..., -0.2754,  0.3008,  0.0048],\n",
      "        [ 0.4512, -0.1582, -0.0530,  ..., -0.0723, -0.1035,  0.3672]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1060, 0.1206, 0.0981, 0.1270, 0.1035, 0.1084, 0.0713, 0.1357, 0.0879,\n",
      "        0.1504, 0.1211, 0.0483, 0.1191, 0.1187, 0.1069, 0.1494, 0.1006, 0.1001,\n",
      "        0.1318, 0.0977, 0.1069, 0.0830, 0.1367, 0.1128, 0.0972, 0.0845, 0.0889,\n",
      "        0.1084, 0.1196, 0.1040, 0.1245, 0.1465, 0.1108, 0.1182, 0.1094, 0.1157,\n",
      "        0.1348, 0.1040, 0.1187, 0.0952, 0.1196, 0.1055, 0.1406, 0.0801, 0.1123,\n",
      "        0.1689, 0.1299, 0.1094, 0.1113, 0.1055, 0.0962, 0.1230, 0.1079, 0.1152,\n",
      "        0.1357, 0.1094, 0.1021, 0.1328, 0.1055, 0.1045, 0.1191, 0.1348, 0.1221,\n",
      "        0.1143, 0.1475, 0.1270, 0.0801, 0.1182, 0.1504, 0.1060, 0.1279, 0.0874,\n",
      "        0.1157, 0.1113, 0.1230, 0.1216, 0.1040, 0.1260, 0.0308, 0.0942, 0.1328,\n",
      "        0.1089, 0.1235, 0.1260, 0.1040, 0.1338, 0.1196, 0.1260, 0.1079, 0.1582,\n",
      "        0.0986, 0.0913, 0.1211, 0.1230, 0.1157, 0.1099, 0.1021, 0.1260, 0.1001,\n",
      "        0.0991, 0.1064, 0.1118, 0.1099, 0.1094, 0.0908, 0.1123, 0.0908, 0.1147,\n",
      "        0.0869, 0.1172, 0.1021, 0.1094, 0.1309, 0.0894, 0.1006, 0.1021, 0.0776,\n",
      "        0.1069, 0.1108, 0.1230, 0.1133, 0.1338, 0.1318, 0.1406, 0.1006, 0.1206,\n",
      "        0.1191, 0.1172, 0.0894, 0.0884, 0.1167, 0.1216, 0.0972, 0.0864, 0.1011,\n",
      "        0.1177, 0.1216, 0.0957, 0.0879, 0.1406, 0.0938, 0.1455, 0.1133, 0.1357,\n",
      "        0.1309, 0.1357, 0.1104, 0.0991, 0.1123, 0.0977, 0.1206, 0.1357, 0.1064,\n",
      "        0.1318, 0.0942, 0.1206, 0.1201, 0.1514, 0.1235, 0.1250, 0.1099, 0.1318,\n",
      "        0.1387, 0.0322, 0.1050, 0.1172, 0.1104, 0.1406, 0.1060, 0.1416, 0.1084,\n",
      "        0.1172, 0.1094, 0.1235, 0.1201, 0.1099, 0.1113, 0.0859, 0.1138, 0.1177,\n",
      "        0.1025, 0.0859, 0.1406, 0.0957, 0.1123, 0.1123, 0.1533, 0.1484, 0.1050,\n",
      "        0.1025, 0.1074, 0.1436, 0.1074, 0.1416, 0.1348, 0.0879, 0.1187, 0.1279,\n",
      "        0.1094, 0.1172, 0.1021, 0.1035, 0.1089, 0.1152, 0.0781, 0.1079, 0.1128,\n",
      "        0.0879, 0.1191, 0.1240, 0.1562, 0.1060, 0.1211, 0.1055, 0.1079, 0.1128,\n",
      "        0.1289, 0.1201, 0.1045, 0.1436, 0.0918, 0.0991, 0.1187, 0.1162, 0.1060,\n",
      "        0.0869, 0.1177, 0.1060, 0.1133, 0.1270, 0.0918, 0.1074, 0.1006, 0.1157,\n",
      "        0.1426, 0.1147, 0.1328, 0.1211, 0.1289, 0.1094, 0.1133, 0.1074, 0.1270,\n",
      "        0.1021, 0.1147, 0.1118, 0.1216, 0.1162, 0.0884, 0.1289, 0.0791, 0.1240,\n",
      "        0.0977, 0.0977, 0.0981, 0.0918, 0.1084, 0.1108, 0.1221, 0.1250, 0.1250,\n",
      "        0.1006, 0.0981, 0.1108, 0.1348, 0.1387, 0.1226, 0.1357, 0.0869, 0.0942,\n",
      "        0.1060, 0.1089, 0.1138, 0.1182, 0.1338, 0.1035, 0.1055, 0.1245, 0.1099,\n",
      "        0.1216, 0.1045, 0.1240, 0.1113, 0.1396, 0.1191, 0.1060, 0.1270, 0.0894,\n",
      "        0.1475, 0.1011, 0.1182, 0.0981, 0.1260, 0.0894, 0.0962, 0.0654, 0.1069,\n",
      "        0.0767, 0.1191, 0.1084, 0.1504, 0.1187, 0.1187, 0.1377, 0.0977, 0.1494,\n",
      "        0.1270, 0.0986, 0.0771, 0.1084, 0.1138, 0.1167, 0.1064, 0.1118, 0.1367,\n",
      "        0.1406, 0.1318, 0.1240, 0.1226, 0.1128, 0.1504, 0.1045, 0.1094, 0.1328,\n",
      "        0.1035, 0.1011, 0.1152, 0.1050, 0.1074, 0.1299, 0.1206, 0.1128, 0.1289,\n",
      "        0.0479, 0.1182, 0.0952, 0.1055, 0.1299, 0.0879, 0.1113, 0.0967, 0.1172,\n",
      "        0.1152, 0.1143, 0.1016, 0.1040, 0.1318, 0.1011, 0.1074, 0.1030, 0.1157,\n",
      "        0.1167, 0.0986, 0.1143, 0.1221, 0.1133, 0.1357, 0.1143, 0.1338, 0.0913,\n",
      "        0.1167, 0.1240, 0.1260, 0.0947, 0.0874, 0.1230, 0.1055, 0.0184, 0.1377,\n",
      "        0.1187, 0.1465, 0.0986, 0.1387, 0.1318, 0.1016, 0.1069, 0.1240, 0.0913,\n",
      "        0.1069, 0.1226, 0.1338, 0.1113, 0.1143, 0.1084, 0.1069, 0.1279, 0.1777,\n",
      "        0.1318, 0.1172, 0.0962, 0.1196, 0.1094, 0.1089, 0.1226, 0.1465, 0.1113,\n",
      "        0.1211, 0.1182, 0.1001, 0.1143, 0.0903, 0.1001, 0.1279, 0.1201, 0.1050,\n",
      "        0.1089, 0.1426, 0.1196, 0.1279, 0.1289, 0.1133, 0.1030, 0.0537, 0.0957,\n",
      "        0.0242, 0.1196, 0.1152, 0.1094, 0.0977, 0.1216, 0.1118, 0.1416, 0.1206,\n",
      "        0.0874, 0.0811, 0.1118, 0.1177, 0.1084, 0.0786, 0.1089, 0.0967, 0.1377,\n",
      "        0.1060, 0.1182, 0.1240, 0.1152, 0.1064, 0.1138, 0.1240, 0.1079, 0.1162,\n",
      "        0.0977, 0.1030, 0.1157, 0.1206, 0.1318, 0.0898, 0.1787, 0.1108, 0.0869,\n",
      "        0.0923, 0.1206, 0.1035, 0.1016, 0.1455, 0.0952, 0.0996, 0.1162, 0.0728,\n",
      "        0.1060, 0.1436, 0.1064, 0.1069, 0.0762, 0.0903, 0.1201, 0.0996, 0.0938,\n",
      "        0.1299, 0.1377, 0.1118, 0.1050, 0.0991, 0.0981, 0.1118, 0.1416, 0.1089,\n",
      "        0.1138, 0.0737, 0.1074, 0.1187, 0.1025, 0.1270, 0.1162, 0.1040, 0.1196,\n",
      "        0.1465, 0.1074, 0.1426, 0.1299, 0.1211, 0.0928, 0.1069, 0.1504, 0.1416,\n",
      "        0.1001, 0.1260, 0.1387, 0.1250, 0.1226, 0.1299, 0.1235, 0.1084, 0.1533,\n",
      "        0.1230, 0.1035, 0.1416, 0.1069, 0.0889, 0.1201, 0.1279, 0.1367],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1475,  0.0189,  0.6523,  ...,  0.1338, -0.1235, -1.1172],\n",
      "        [ 0.1436,  0.3633,  0.2422,  ..., -0.2354,  1.2500, -0.5742],\n",
      "        [-0.5938, -0.5781, -0.3906,  ...,  0.2168, -0.4570,  0.1602],\n",
      "        ...,\n",
      "        [-0.3477, -0.1904, -0.8086,  ..., -0.3652, -0.5859,  0.2236],\n",
      "        [ 0.6406,  0.4570,  0.7617,  ...,  0.8359, -0.2734,  0.7656],\n",
      "        [ 0.2285, -0.0713, -0.1943,  ...,  0.1455, -0.5312, -0.0854]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1836,  0.7148,  0.1865,  ...,  0.1494, -0.0815,  0.8828],\n",
      "        [-0.2363, -0.2715, -0.8164,  ..., -0.3633, -0.9961, -0.3672],\n",
      "        [ 0.1455, -0.5898,  0.2461,  ...,  0.1367,  0.9648,  1.0000],\n",
      "        ...,\n",
      "        [-0.5859, -0.3535, -0.2715,  ...,  0.7266, -0.3398,  0.3438],\n",
      "        [-0.0320, -0.1357,  0.5859,  ..., -0.6914,  1.0703,  0.6328],\n",
      "        [-0.0869, -0.4199, -0.0508,  ..., -0.1348,  0.6992, -0.1982]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0659, -0.5508,  0.3340,  ...,  0.4258, -0.7148,  0.3945],\n",
      "        [-0.2988, -0.1074,  0.3945,  ..., -0.1143, -0.2559,  0.0405],\n",
      "        [ 0.3496, -0.0272, -0.1797,  ...,  0.2178,  0.1445, -0.0393],\n",
      "        ...,\n",
      "        [ 0.2656, -0.2461,  0.4473,  ..., -0.0232,  0.0757, -0.4355],\n",
      "        [ 0.0491, -0.3867,  0.4023,  ..., -0.1475, -0.4941, -0.4395],\n",
      "        [ 0.1572,  0.1128, -0.1152,  ...,  0.0043, -0.2148, -0.0947]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1357, 0.1299, 0.0811, 0.1221, 0.1201, 0.1016, 0.1270, 0.1069, 0.0796,\n",
      "        0.1367, 0.1089, 0.0386, 0.0972, 0.0889, 0.0859, 0.1260, 0.1025, 0.0815,\n",
      "        0.1206, 0.0874, 0.0962, 0.0698, 0.1025, 0.1084, 0.0859, 0.0718, 0.0640,\n",
      "        0.0913, 0.1094, 0.0923, 0.1006, 0.1328, 0.2061, 0.0967, 0.0854, 0.0962,\n",
      "        0.1128, 0.0874, 0.1035, 0.0767, 0.1045, 0.0908, 0.1147, 0.0796, 0.1021,\n",
      "        0.1396, 0.1074, 0.1074, 0.0830, 0.0947, 0.0830, 0.1021, 0.0889, 0.1094,\n",
      "        0.1245, 0.1074, 0.0903, 0.1377, 0.1035, 0.0894, 0.1079, 0.0884, 0.1157,\n",
      "        0.1045, 0.1475, 0.1128, 0.0713, 0.0889, 0.1289, 0.0918, 0.1196, 0.0854,\n",
      "        0.0996, 0.0967, 0.1084, 0.1016, 0.0884, 0.1113, 0.0381, 0.0889, 0.1128,\n",
      "        0.0967, 0.0986, 0.0942, 0.0952, 0.1221, 0.1001, 0.0972, 0.1084, 0.1348,\n",
      "        0.0928, 0.0854, 0.1006, 0.1055, 0.1089, 0.0859, 0.1001, 0.1060, 0.0889,\n",
      "        0.0923, 0.0962, 0.1030, 0.0938, 0.0947, 0.0884, 0.0923, 0.0928, 0.1060,\n",
      "        0.0757, 0.1006, 0.0918, 0.0938, 0.1533, 0.0762, 0.0884, 0.0981, 0.0610,\n",
      "        0.0981, 0.0864, 0.1035, 0.1016, 0.1094, 0.1289, 0.1152, 0.1035, 0.1021,\n",
      "        0.0977, 0.0942, 0.0864, 0.0742, 0.1191, 0.0967, 0.0962, 0.0771, 0.0825,\n",
      "        0.0967, 0.3301, 0.0811, 0.0923, 0.1162, 0.0918, 0.1465, 0.0864, 0.1167,\n",
      "        0.0874, 0.1182, 0.0942, 0.1050, 0.0952, 0.0869, 0.1118, 0.1143, 0.0898,\n",
      "        0.1299, 0.0918, 0.0884, 0.1089, 0.1338, 0.1035, 0.1055, 0.0957, 0.1309,\n",
      "        0.1060, 0.0303, 0.1021, 0.0991, 0.0791, 0.1206, 0.0908, 0.1128, 0.1006,\n",
      "        0.0972, 0.0947, 0.0967, 0.1152, 0.0898, 0.1025, 0.0698, 0.1016, 0.1025,\n",
      "        0.1016, 0.0737, 0.1260, 0.0742, 0.0967, 0.1084, 0.1328, 0.1167, 0.1094,\n",
      "        0.0820, 0.0820, 0.1553, 0.0913, 0.1206, 0.1177, 0.0703, 0.1016, 0.0991,\n",
      "        0.0981, 0.1035, 0.0850, 0.0923, 0.0923, 0.0972, 0.0933, 0.1035, 0.0903,\n",
      "        0.0854, 0.1030, 0.1187, 0.1260, 0.0962, 0.1025, 0.0830, 0.0942, 0.1152,\n",
      "        0.0972, 0.1157, 0.0879, 0.1289, 0.0791, 0.0796, 0.0996, 0.1138, 0.0859,\n",
      "        0.0752, 0.0991, 0.0918, 0.0874, 0.1250, 0.0752, 0.0996, 0.0850, 0.0967,\n",
      "        0.1201, 0.0967, 0.0947, 0.1396, 0.1099, 0.1001, 0.0991, 0.1050, 0.0972,\n",
      "        0.0908, 0.1030, 0.0938, 0.1162, 0.0903, 0.0791, 0.1040, 0.0796, 0.1143,\n",
      "        0.0947, 0.0859, 0.0776, 0.0845, 0.0923, 0.1045, 0.0933, 0.1084, 0.1099,\n",
      "        0.0742, 0.0830, 0.1216, 0.1104, 0.1201, 0.1025, 0.1235, 0.0840, 0.0752,\n",
      "        0.1118, 0.0889, 0.0996, 0.0894, 0.1289, 0.0859, 0.0913, 0.1050, 0.0952,\n",
      "        0.1035, 0.0869, 0.1001, 0.0825, 0.1396, 0.1162, 0.0938, 0.1021, 0.0854,\n",
      "        0.1152, 0.0986, 0.1016, 0.0762, 0.1128, 0.0786, 0.0894, 0.0610, 0.0913,\n",
      "        0.0645, 0.1030, 0.0908, 0.1338, 0.1045, 0.0977, 0.1201, 0.0928, 0.1191,\n",
      "        0.1123, 0.0771, 0.0669, 0.0957, 0.0962, 0.0942, 0.1064, 0.1006, 0.1157,\n",
      "        0.1436, 0.0996, 0.1050, 0.1157, 0.0869, 0.1416, 0.0894, 0.0889, 0.1387,\n",
      "        0.1006, 0.0835, 0.1108, 0.0981, 0.0898, 0.1094, 0.1001, 0.1006, 0.1270,\n",
      "        0.0508, 0.0947, 0.0811, 0.0967, 0.0962, 0.0854, 0.1187, 0.1011, 0.0977,\n",
      "        0.0908, 0.0933, 0.0879, 0.1025, 0.1118, 0.1006, 0.0933, 0.0864, 0.0991,\n",
      "        0.0986, 0.1157, 0.1001, 0.1025, 0.0879, 0.1172, 0.0928, 0.1172, 0.0776,\n",
      "        0.0903, 0.0981, 0.1108, 0.0850, 0.0718, 0.1045, 0.0835, 0.0320, 0.1152,\n",
      "        0.0942, 0.1406, 0.1079, 0.1309, 0.1187, 0.0869, 0.0967, 0.1064, 0.1079,\n",
      "        0.0835, 0.1040, 0.1128, 0.1069, 0.0913, 0.0840, 0.0854, 0.1108, 0.1611,\n",
      "        0.1089, 0.0859, 0.0830, 0.1250, 0.0845, 0.1108, 0.1035, 0.1250, 0.0977,\n",
      "        0.1191, 0.1050, 0.1104, 0.1045, 0.0845, 0.0786, 0.1338, 0.0898, 0.0942,\n",
      "        0.1021, 0.1084, 0.0938, 0.1060, 0.0972, 0.1021, 0.0889, 0.0359, 0.0894,\n",
      "        0.0283, 0.0991, 0.0908, 0.0947, 0.0811, 0.1152, 0.0981, 0.1279, 0.1084,\n",
      "        0.0732, 0.0654, 0.0933, 0.1196, 0.0928, 0.0781, 0.0850, 0.0796, 0.1177,\n",
      "        0.0889, 0.1050, 0.1113, 0.0938, 0.0972, 0.0977, 0.1030, 0.0850, 0.0815,\n",
      "        0.0840, 0.0776, 0.1123, 0.1094, 0.1177, 0.0811, 0.1094, 0.0957, 0.0791,\n",
      "        0.0771, 0.1157, 0.0889, 0.0820, 0.1328, 0.0991, 0.0840, 0.0894, 0.0757,\n",
      "        0.0957, 0.1328, 0.1040, 0.0850, 0.0688, 0.0771, 0.1094, 0.0859, 0.0659,\n",
      "        0.0991, 0.1240, 0.0923, 0.1030, 0.0830, 0.0913, 0.0938, 0.1270, 0.0972,\n",
      "        0.0923, 0.0625, 0.1035, 0.0977, 0.0928, 0.1279, 0.1055, 0.0864, 0.1089,\n",
      "        0.1396, 0.1187, 0.1309, 0.1016, 0.1021, 0.0903, 0.0874, 0.1064, 0.1094,\n",
      "        0.0815, 0.0952, 0.1157, 0.1045, 0.1152, 0.1050, 0.1475, 0.1030, 0.1216,\n",
      "        0.0918, 0.0952, 0.1187, 0.1152, 0.0732, 0.1074, 0.1060, 0.1689],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0630,  0.0215,  0.0161,  ...,  0.0262, -0.0430, -0.0571],\n",
      "        [ 0.0713,  0.0413,  0.0649,  ...,  0.0708, -0.0084, -0.0128],\n",
      "        [-0.1021, -0.0108,  0.0806,  ...,  0.0069,  0.0503,  0.0547],\n",
      "        ...,\n",
      "        [ 0.0615,  0.0075, -0.0048,  ...,  0.0291, -0.0559, -0.0062],\n",
      "        [ 0.0165,  0.1035, -0.0187,  ..., -0.0037, -0.1084, -0.0422],\n",
      "        [ 0.0996, -0.0332,  0.0540,  ..., -0.0069,  0.0037,  0.0957]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1187, -0.4316,  0.5117,  ...,  0.4648,  0.5391, -0.3418],\n",
      "        [-0.2432,  0.1465, -0.3262,  ...,  0.0918, -1.0391, -0.5312],\n",
      "        [-0.1855,  0.1504, -0.3301,  ...,  0.0688, -0.5000,  0.2520],\n",
      "        ...,\n",
      "        [ 0.2578, -0.4609,  0.0557,  ..., -0.1914, -0.4453, -0.3047],\n",
      "        [ 0.2871,  0.5664, -0.7891,  ...,  0.1602, -0.5547, -0.1206],\n",
      "        [ 0.5117, -0.5664,  0.4727,  ..., -0.0491, -0.0554,  0.1670]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0576,  0.7383,  0.8711,  ...,  0.5938,  0.9062, -0.2451],\n",
      "        [-0.1855, -0.6992, -0.1533,  ..., -0.1982,  0.1050,  0.0305],\n",
      "        [-0.3359,  0.4473,  1.1406,  ...,  0.8320,  0.3555,  0.0188],\n",
      "        ...,\n",
      "        [-0.0630,  0.9219,  0.5391,  ..., -0.6250, -0.5586,  0.2451],\n",
      "        [-0.3965,  0.4062,  0.3770,  ...,  0.9805,  0.4238,  0.3066],\n",
      "        [-0.4473, -0.3574,  0.1030,  ...,  0.0530,  0.3262, -0.0461]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0272, -0.3887,  0.2617,  ..., -0.4180, -0.5078,  0.8125],\n",
      "        [-0.1631,  0.2832, -0.0977,  ..., -0.0908,  0.1641,  0.2598],\n",
      "        [ 0.2324,  0.2539, -0.8242,  ..., -0.1279,  0.0728, -0.5938],\n",
      "        ...,\n",
      "        [-0.1787, -0.3887,  0.0947,  ...,  0.2344, -0.6250, -1.2969],\n",
      "        [ 0.5977, -0.4570, -1.1406,  ...,  0.0334, -0.0850, -0.3613],\n",
      "        [ 0.5977,  0.5391, -0.5312,  ...,  0.0320,  0.5195,  0.0242]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1338, 0.1279, 0.0859, 0.1221, 0.1094, 0.1079, 0.0500, 0.1206, 0.0947,\n",
      "        0.1738, 0.1426, 0.0554, 0.1123, 0.1299, 0.1211, 0.1484, 0.1099, 0.1050,\n",
      "        0.1455, 0.0806, 0.1108, 0.0928, 0.1235, 0.1426, 0.1006, 0.0918, 0.0850,\n",
      "        0.1279, 0.1299, 0.1211, 0.1172, 0.1357, 0.0962, 0.1206, 0.1040, 0.1128,\n",
      "        0.1289, 0.1182, 0.1167, 0.0933, 0.1191, 0.1118, 0.1445, 0.0986, 0.1167,\n",
      "        0.1621, 0.1299, 0.1260, 0.1069, 0.1094, 0.1060, 0.1162, 0.1162, 0.1187,\n",
      "        0.1357, 0.1299, 0.1030, 0.1270, 0.1147, 0.0996, 0.1289, 0.1118, 0.1221,\n",
      "        0.1216, 0.1328, 0.1426, 0.0942, 0.1196, 0.1533, 0.1079, 0.1396, 0.0986,\n",
      "        0.1191, 0.1079, 0.1133, 0.0962, 0.1094, 0.1357, 0.0417, 0.1064, 0.1436,\n",
      "        0.1221, 0.1177, 0.1138, 0.1064, 0.1226, 0.1133, 0.1226, 0.1221, 0.1279,\n",
      "        0.0928, 0.1011, 0.1328, 0.1147, 0.1235, 0.1123, 0.1177, 0.1191, 0.1006,\n",
      "        0.0967, 0.1069, 0.0986, 0.1245, 0.1177, 0.1040, 0.1006, 0.1191, 0.1221,\n",
      "        0.0796, 0.1260, 0.1108, 0.1162, 0.1650, 0.0923, 0.1011, 0.1094, 0.0771,\n",
      "        0.1108, 0.1064, 0.1279, 0.1167, 0.1299, 0.1235, 0.1475, 0.1069, 0.1172,\n",
      "        0.1221, 0.0967, 0.0942, 0.0811, 0.1182, 0.1191, 0.0977, 0.0981, 0.0933,\n",
      "        0.1240, 0.0723, 0.1025, 0.0972, 0.1396, 0.1030, 0.1484, 0.1118, 0.1309,\n",
      "        0.1177, 0.1357, 0.1157, 0.1128, 0.0986, 0.0898, 0.1318, 0.1348, 0.1118,\n",
      "        0.1689, 0.1045, 0.1260, 0.1196, 0.1377, 0.1299, 0.1216, 0.1226, 0.1260,\n",
      "        0.1250, 0.0374, 0.1191, 0.1216, 0.1045, 0.1504, 0.1069, 0.1416, 0.1318,\n",
      "        0.1040, 0.1162, 0.1230, 0.1226, 0.1133, 0.1113, 0.0928, 0.1118, 0.1162,\n",
      "        0.1079, 0.0952, 0.1426, 0.1128, 0.1162, 0.1157, 0.1416, 0.1406, 0.1167,\n",
      "        0.1045, 0.1074, 0.1245, 0.1133, 0.1406, 0.1357, 0.0884, 0.1172, 0.1230,\n",
      "        0.1226, 0.1162, 0.1060, 0.1064, 0.1162, 0.1064, 0.0894, 0.1025, 0.0962,\n",
      "        0.0903, 0.1270, 0.1226, 0.1514, 0.1055, 0.1206, 0.0957, 0.1260, 0.1167,\n",
      "        0.1216, 0.1387, 0.1157, 0.1328, 0.1084, 0.1006, 0.1250, 0.1270, 0.1187,\n",
      "        0.0918, 0.1426, 0.1006, 0.1206, 0.1416, 0.0972, 0.1152, 0.0996, 0.1113,\n",
      "        0.1406, 0.1196, 0.1328, 0.1338, 0.1260, 0.1201, 0.1299, 0.1260, 0.1279,\n",
      "        0.1191, 0.1211, 0.1260, 0.1318, 0.1084, 0.0918, 0.1152, 0.0972, 0.1230,\n",
      "        0.1074, 0.1167, 0.1074, 0.0996, 0.1157, 0.1187, 0.1230, 0.1191, 0.1270,\n",
      "        0.0996, 0.1011, 0.1270, 0.1494, 0.1338, 0.1187, 0.1270, 0.0942, 0.0952,\n",
      "        0.1123, 0.1162, 0.1128, 0.1064, 0.1387, 0.1030, 0.0957, 0.1299, 0.1138,\n",
      "        0.1416, 0.1147, 0.1147, 0.0938, 0.1221, 0.1270, 0.1143, 0.1396, 0.1074,\n",
      "        0.1514, 0.0977, 0.1201, 0.0884, 0.1157, 0.1011, 0.1187, 0.0781, 0.1055,\n",
      "        0.0879, 0.1230, 0.0962, 0.1338, 0.1357, 0.1309, 0.1445, 0.1021, 0.1484,\n",
      "        0.1196, 0.1016, 0.0786, 0.1035, 0.1074, 0.1133, 0.1035, 0.1279, 0.1216,\n",
      "        0.1445, 0.1270, 0.1338, 0.1069, 0.1079, 0.1641, 0.1113, 0.1196, 0.1396,\n",
      "        0.1128, 0.1060, 0.1396, 0.1147, 0.1060, 0.1201, 0.1211, 0.1260, 0.1270,\n",
      "        0.0586, 0.1201, 0.1079, 0.1235, 0.1387, 0.0923, 0.1162, 0.1216, 0.1167,\n",
      "        0.1187, 0.1104, 0.1270, 0.1108, 0.1187, 0.1055, 0.1113, 0.1025, 0.1055,\n",
      "        0.1113, 0.1021, 0.1128, 0.1240, 0.1108, 0.1201, 0.1152, 0.1196, 0.1060,\n",
      "        0.1104, 0.1079, 0.1367, 0.0884, 0.0864, 0.1157, 0.1079, 0.0349, 0.1279,\n",
      "        0.1396, 0.1416, 0.1123, 0.1494, 0.1328, 0.1045, 0.1367, 0.1191, 0.1089,\n",
      "        0.1025, 0.1138, 0.1396, 0.1270, 0.1328, 0.1079, 0.1055, 0.1465, 0.1631,\n",
      "        0.1318, 0.1138, 0.1030, 0.1357, 0.1030, 0.1099, 0.1289, 0.1475, 0.1089,\n",
      "        0.1216, 0.1182, 0.1162, 0.1211, 0.1074, 0.1011, 0.1309, 0.1167, 0.1060,\n",
      "        0.1260, 0.1318, 0.1318, 0.1133, 0.1201, 0.1216, 0.1094, 0.0525, 0.1240,\n",
      "        0.0403, 0.1211, 0.1123, 0.1216, 0.0957, 0.1475, 0.1074, 0.1328, 0.1157,\n",
      "        0.0938, 0.0879, 0.1021, 0.1167, 0.1069, 0.1084, 0.1118, 0.1167, 0.1572,\n",
      "        0.1084, 0.1177, 0.1157, 0.1001, 0.1250, 0.1279, 0.1187, 0.0957, 0.1235,\n",
      "        0.0889, 0.1206, 0.1328, 0.1211, 0.1221, 0.0791, 0.1797, 0.1104, 0.1099,\n",
      "        0.0913, 0.1118, 0.1074, 0.1123, 0.1504, 0.0967, 0.1104, 0.1226, 0.0938,\n",
      "        0.1011, 0.1367, 0.1030, 0.1001, 0.0825, 0.0854, 0.1328, 0.1060, 0.0879,\n",
      "        0.1221, 0.1475, 0.1030, 0.1177, 0.1079, 0.1104, 0.1240, 0.1406, 0.1128,\n",
      "        0.1152, 0.0933, 0.1211, 0.1138, 0.1123, 0.1270, 0.1377, 0.1016, 0.1357,\n",
      "        0.1357, 0.1069, 0.1348, 0.1260, 0.1436, 0.0996, 0.1196, 0.1172, 0.1455,\n",
      "        0.0991, 0.1108, 0.1514, 0.1133, 0.1279, 0.1455, 0.1279, 0.1152, 0.1338,\n",
      "        0.1177, 0.1055, 0.1328, 0.1367, 0.0986, 0.1099, 0.1177, 0.1348],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5664, -0.7891, -0.2275,  ...,  0.0500, -0.2471, -0.1543],\n",
      "        [ 0.0405, -0.1455,  0.6836,  ..., -0.0280, -0.2676,  0.3164],\n",
      "        [ 0.0383,  0.0119,  0.1084,  ..., -0.1211,  0.0747, -0.1187],\n",
      "        ...,\n",
      "        [-0.5508, -0.7930, -0.4512,  ..., -0.2227, -0.4082,  1.8750],\n",
      "        [-0.4355,  0.0086,  0.2832,  ..., -0.0283,  1.1172, -0.5078],\n",
      "        [ 0.2949, -0.2080, -0.4492,  ..., -0.3828, -0.7305, -0.2451]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6211, -0.2539, -0.8438,  ...,  1.1406,  0.1973, -0.1069],\n",
      "        [ 0.2324, -0.7539, -0.1299,  ...,  0.2246, -0.0177,  0.5039],\n",
      "        [ 0.1650, -0.7852,  0.4141,  ...,  0.2256, -0.2539,  0.1826],\n",
      "        ...,\n",
      "        [ 0.6406,  0.1465, -0.3086,  ..., -0.9180,  0.9453,  2.4219],\n",
      "        [ 0.6133,  0.7227, -0.6172,  ...,  0.1138, -0.1162,  0.3242],\n",
      "        [-0.1768, -1.5312,  0.7539,  ...,  0.3164, -0.2275,  0.2490]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.9336e-02,  9.4727e-02,  4.1602e-01,  ..., -2.5586e-01,\n",
      "         -7.8906e-01,  4.8633e-01],\n",
      "        [ 2.6758e-01,  4.8438e-01, -5.0781e-02,  ...,  1.4062e-01,\n",
      "          5.5859e-01,  2.4609e-01],\n",
      "        [-7.1094e-01,  2.5391e-02,  2.5635e-02,  ...,  4.9744e-03,\n",
      "          3.0078e-01,  1.9336e-01],\n",
      "        ...,\n",
      "        [ 2.3730e-01, -3.5156e-01, -6.7383e-02,  ...,  4.5898e-01,\n",
      "          7.0312e-01,  4.6875e-01],\n",
      "        [ 4.0234e-01, -1.0547e-01, -6.0547e-02,  ..., -3.8086e-02,\n",
      "         -3.0469e-01, -2.3730e-01],\n",
      "        [ 3.8281e-01, -3.4961e-01,  9.3937e-05,  ..., -2.6367e-01,\n",
      "         -7.9297e-01,  3.0273e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1279, 0.1270, 0.0811, 0.1221, 0.1094, 0.0947, 0.1357, 0.1045, 0.0767,\n",
      "        0.1279, 0.1045, 0.0376, 0.0933, 0.0933, 0.0840, 0.1196, 0.0894, 0.0820,\n",
      "        0.1006, 0.0884, 0.0879, 0.0684, 0.1030, 0.1060, 0.0840, 0.0713, 0.0679,\n",
      "        0.1011, 0.1030, 0.0894, 0.1035, 0.1162, 0.1992, 0.0947, 0.0884, 0.0977,\n",
      "        0.1094, 0.0869, 0.0967, 0.0737, 0.0996, 0.1016, 0.1123, 0.0723, 0.0933,\n",
      "        0.1318, 0.1035, 0.1211, 0.0874, 0.0986, 0.0835, 0.1113, 0.0962, 0.1084,\n",
      "        0.1250, 0.1094, 0.0840, 0.1187, 0.0981, 0.0879, 0.1133, 0.0859, 0.0981,\n",
      "        0.1050, 0.1387, 0.1162, 0.0708, 0.1040, 0.1108, 0.0859, 0.1211, 0.0825,\n",
      "        0.0894, 0.0923, 0.1050, 0.0908, 0.0967, 0.1167, 0.0388, 0.0869, 0.1187,\n",
      "        0.0933, 0.0947, 0.0923, 0.0981, 0.1035, 0.1006, 0.1094, 0.1079, 0.1348,\n",
      "        0.0928, 0.0796, 0.0884, 0.0977, 0.1147, 0.0913, 0.1055, 0.1167, 0.0845,\n",
      "        0.0913, 0.0815, 0.0952, 0.0952, 0.0977, 0.0898, 0.0859, 0.0972, 0.1011,\n",
      "        0.0762, 0.0996, 0.0898, 0.0913, 0.1377, 0.0786, 0.0898, 0.0908, 0.0615,\n",
      "        0.0884, 0.0840, 0.1094, 0.1133, 0.0996, 0.1074, 0.1123, 0.0942, 0.1011,\n",
      "        0.0962, 0.0996, 0.0786, 0.0776, 0.1074, 0.1001, 0.0952, 0.0757, 0.0864,\n",
      "        0.1011, 0.6797, 0.0874, 0.0957, 0.1064, 0.0864, 0.1357, 0.0864, 0.1099,\n",
      "        0.0928, 0.1406, 0.0933, 0.0996, 0.0933, 0.0972, 0.1138, 0.1079, 0.0972,\n",
      "        0.1216, 0.0845, 0.0923, 0.1055, 0.1191, 0.0996, 0.0996, 0.0981, 0.1221,\n",
      "        0.1147, 0.0308, 0.0991, 0.0986, 0.0884, 0.1191, 0.0947, 0.1089, 0.1172,\n",
      "        0.0942, 0.1016, 0.0986, 0.1123, 0.0952, 0.0962, 0.0713, 0.0981, 0.0947,\n",
      "        0.0884, 0.0771, 0.1191, 0.0767, 0.0933, 0.1045, 0.1279, 0.1089, 0.1025,\n",
      "        0.0859, 0.0825, 0.1348, 0.0840, 0.1079, 0.1162, 0.0747, 0.0952, 0.1040,\n",
      "        0.0972, 0.1040, 0.0840, 0.0889, 0.0918, 0.0957, 0.0981, 0.0962, 0.0811,\n",
      "        0.0771, 0.1074, 0.1108, 0.1191, 0.0947, 0.1084, 0.0859, 0.1016, 0.1050,\n",
      "        0.1025, 0.1074, 0.0957, 0.1348, 0.0820, 0.0830, 0.0913, 0.0962, 0.0894,\n",
      "        0.0781, 0.1035, 0.0903, 0.0864, 0.1279, 0.0771, 0.0986, 0.0830, 0.0981,\n",
      "        0.1201, 0.1001, 0.1025, 0.1299, 0.1089, 0.0996, 0.1016, 0.0996, 0.0991,\n",
      "        0.0850, 0.0903, 0.1045, 0.1030, 0.0854, 0.0762, 0.1060, 0.0752, 0.1040,\n",
      "        0.1006, 0.0962, 0.0879, 0.0864, 0.0957, 0.0957, 0.0991, 0.1167, 0.1040,\n",
      "        0.0752, 0.0806, 0.1113, 0.1128, 0.1152, 0.0889, 0.1230, 0.0786, 0.0840,\n",
      "        0.1064, 0.1001, 0.0952, 0.0879, 0.1113, 0.0942, 0.0913, 0.1104, 0.0854,\n",
      "        0.1064, 0.0942, 0.0908, 0.0874, 0.1177, 0.1021, 0.0811, 0.1050, 0.0879,\n",
      "        0.1216, 0.1084, 0.0957, 0.0815, 0.1113, 0.0889, 0.0981, 0.0625, 0.0923,\n",
      "        0.0674, 0.0957, 0.0952, 0.1299, 0.1113, 0.0918, 0.1299, 0.1118, 0.1162,\n",
      "        0.1167, 0.0835, 0.0693, 0.0972, 0.0952, 0.1025, 0.0835, 0.1006, 0.1050,\n",
      "        0.1289, 0.1138, 0.1128, 0.1064, 0.0918, 0.1226, 0.0806, 0.0962, 0.1162,\n",
      "        0.1021, 0.0830, 0.1187, 0.1001, 0.0947, 0.1045, 0.0977, 0.0947, 0.1055,\n",
      "        0.0515, 0.1108, 0.0879, 0.0898, 0.1025, 0.0830, 0.1147, 0.0962, 0.1060,\n",
      "        0.0967, 0.0918, 0.0933, 0.0981, 0.1138, 0.1001, 0.0991, 0.0972, 0.0938,\n",
      "        0.0967, 0.1050, 0.0952, 0.1055, 0.0825, 0.1123, 0.0991, 0.0996, 0.0786,\n",
      "        0.0947, 0.0991, 0.1025, 0.0859, 0.0698, 0.1021, 0.0825, 0.0334, 0.1079,\n",
      "        0.1035, 0.1206, 0.0918, 0.1089, 0.1177, 0.0845, 0.1138, 0.1011, 0.1079,\n",
      "        0.0894, 0.1060, 0.1177, 0.1021, 0.0903, 0.0898, 0.0869, 0.1138, 0.1484,\n",
      "        0.1084, 0.0928, 0.0884, 0.1108, 0.0884, 0.0952, 0.1006, 0.1318, 0.0962,\n",
      "        0.1040, 0.1006, 0.1074, 0.1079, 0.0898, 0.0835, 0.1279, 0.0991, 0.0957,\n",
      "        0.1001, 0.1055, 0.0967, 0.0981, 0.1040, 0.0962, 0.0938, 0.0430, 0.0962,\n",
      "        0.0277, 0.1045, 0.0986, 0.0933, 0.0840, 0.1128, 0.1025, 0.1250, 0.1089,\n",
      "        0.0737, 0.0708, 0.0977, 0.1025, 0.0913, 0.0840, 0.0835, 0.0923, 0.1191,\n",
      "        0.0962, 0.1099, 0.1138, 0.0825, 0.0933, 0.0957, 0.0972, 0.0796, 0.0903,\n",
      "        0.0884, 0.0815, 0.1152, 0.1006, 0.1118, 0.0859, 0.0840, 0.0894, 0.0874,\n",
      "        0.0762, 0.1016, 0.0908, 0.0820, 0.1162, 0.0933, 0.0889, 0.1050, 0.0708,\n",
      "        0.0957, 0.1045, 0.0957, 0.0806, 0.0708, 0.0786, 0.1123, 0.0869, 0.0654,\n",
      "        0.0986, 0.1260, 0.0898, 0.1074, 0.0757, 0.0923, 0.1006, 0.1226, 0.0938,\n",
      "        0.0928, 0.0693, 0.1084, 0.1060, 0.1001, 0.1250, 0.1040, 0.0913, 0.1108,\n",
      "        0.1279, 0.1108, 0.1172, 0.1064, 0.1201, 0.0835, 0.1011, 0.0962, 0.1099,\n",
      "        0.0918, 0.1021, 0.1196, 0.1099, 0.1099, 0.1152, 0.1230, 0.0991, 0.1138,\n",
      "        0.1021, 0.0957, 0.1099, 0.1025, 0.0723, 0.1064, 0.1001, 0.2266],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0236, -0.0043,  0.0645,  ...,  0.0693, -0.0420, -0.0019],\n",
      "        [ 0.0208,  0.1289,  0.0193,  ..., -0.1299, -0.1592,  0.0508],\n",
      "        [-0.0308,  0.0101,  0.0300,  ..., -0.0270, -0.0801,  0.0525],\n",
      "        ...,\n",
      "        [-0.0544,  0.0237, -0.0109,  ..., -0.0688,  0.0366,  0.0189],\n",
      "        [ 0.0884,  0.0168,  0.0520,  ...,  0.0598, -0.0165,  0.0767],\n",
      "        [-0.0757,  0.0025, -0.0070,  ..., -0.0732,  0.0081, -0.0234]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0713, -0.6641, -0.2285,  ..., -0.0703,  0.3984, -0.4258],\n",
      "        [ 0.2773, -0.2363, -0.2031,  ...,  0.4375,  0.2383, -0.3730],\n",
      "        [-0.2236,  0.4863, -0.1523,  ..., -0.4297, -0.3926,  0.5977],\n",
      "        ...,\n",
      "        [-0.0684,  0.5430,  0.2578,  ..., -0.1592, -0.1846, -0.0728],\n",
      "        [ 0.1826, -0.2080,  0.1543,  ...,  0.1807, -0.1128,  0.2070],\n",
      "        [-0.2695,  0.5742, -0.3457,  ..., -0.3184, -0.1069, -0.1011]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1602, -0.0178,  0.2256,  ..., -0.7227,  0.2656,  0.7617],\n",
      "        [ 0.3320, -0.4746, -0.3496,  ...,  0.0038,  0.3047, -0.0544],\n",
      "        [-0.2754, -0.4277, -0.0221,  ...,  0.7148,  0.0693,  0.0139],\n",
      "        ...,\n",
      "        [-0.7188, -0.1611,  0.6992,  ...,  0.0234, -0.4688, -0.2109],\n",
      "        [ 0.9922, -0.1016,  1.3125,  ..., -1.0781,  1.0000,  0.5469],\n",
      "        [ 0.1152, -0.1572,  0.3965,  ...,  0.7500,  0.4316,  0.1709]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6289, -0.3613, -0.1699,  ..., -1.0469, -0.2051, -0.5508],\n",
      "        [-0.1416,  0.1885,  0.0762,  ..., -0.9023, -0.6094,  0.5234],\n",
      "        [ 0.4648, -0.2520, -0.8867,  ...,  0.4160,  1.3516,  0.2256],\n",
      "        ...,\n",
      "        [-0.7227,  0.3418,  0.3750,  ...,  0.6797, -1.1016,  0.6719],\n",
      "        [ 0.8516,  0.5117,  0.0332,  ..., -0.2617, -0.0077,  0.0164],\n",
      "        [-0.2852,  0.2812,  0.2197,  ..., -0.0596,  0.8203, -0.1826]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1836, 0.1641, 0.1250, 0.1533, 0.1543, 0.1504, 0.0530, 0.1680, 0.1328,\n",
      "        0.2109, 0.1699, 0.0752, 0.1504, 0.1396, 0.1514, 0.2090, 0.1445, 0.1377,\n",
      "        0.1836, 0.1201, 0.1367, 0.1108, 0.1406, 0.1787, 0.1533, 0.1191, 0.1191,\n",
      "        0.1699, 0.1484, 0.1562, 0.1357, 0.1621, 0.1768, 0.1621, 0.1348, 0.1475,\n",
      "        0.1641, 0.1572, 0.1602, 0.1235, 0.1602, 0.1396, 0.1729, 0.1348, 0.1377,\n",
      "        0.1777, 0.1582, 0.1758, 0.1436, 0.1387, 0.1289, 0.1572, 0.1455, 0.1709,\n",
      "        0.1533, 0.1650, 0.1406, 0.1631, 0.1543, 0.1396, 0.1455, 0.1484, 0.1582,\n",
      "        0.1357, 0.1582, 0.1689, 0.1348, 0.1631, 0.1807, 0.1406, 0.1836, 0.1230,\n",
      "        0.1494, 0.1406, 0.1299, 0.1484, 0.1562, 0.1816, 0.0508, 0.1523, 0.1611,\n",
      "        0.1416, 0.1484, 0.1367, 0.1621, 0.1475, 0.1523, 0.1670, 0.1602, 0.1475,\n",
      "        0.1279, 0.1270, 0.1338, 0.1572, 0.1699, 0.1514, 0.1465, 0.1650, 0.1299,\n",
      "        0.1367, 0.1328, 0.1396, 0.1465, 0.1514, 0.1396, 0.1475, 0.1641, 0.1455,\n",
      "        0.1162, 0.1445, 0.1406, 0.1377, 0.2051, 0.1270, 0.1504, 0.1562, 0.1133,\n",
      "        0.1279, 0.1328, 0.1689, 0.1504, 0.1572, 0.1543, 0.1631, 0.1279, 0.1514,\n",
      "        0.1484, 0.1504, 0.1235, 0.1328, 0.1592, 0.1504, 0.1377, 0.1147, 0.1260,\n",
      "        0.1533, 0.0527, 0.1514, 0.1465, 0.1484, 0.1426, 0.1689, 0.1494, 0.1621,\n",
      "        0.1553, 0.1611, 0.1533, 0.1533, 0.1416, 0.1348, 0.1660, 0.1660, 0.1514,\n",
      "        0.1553, 0.1426, 0.1611, 0.1484, 0.1475, 0.1592, 0.1611, 0.1582, 0.1895,\n",
      "        0.1699, 0.0635, 0.1377, 0.1455, 0.1436, 0.1826, 0.1328, 0.1650, 0.1553,\n",
      "        0.1338, 0.1416, 0.1357, 0.1602, 0.1436, 0.1377, 0.1289, 0.1484, 0.1611,\n",
      "        0.1504, 0.1182, 0.2021, 0.1309, 0.1494, 0.1494, 0.1670, 0.1748, 0.1494,\n",
      "        0.1357, 0.1387, 0.1562, 0.1533, 0.1562, 0.1533, 0.1367, 0.1641, 0.1699,\n",
      "        0.1396, 0.1523, 0.1445, 0.1338, 0.1416, 0.1416, 0.1318, 0.1245, 0.1309,\n",
      "        0.1436, 0.1611, 0.1533, 0.1787, 0.1602, 0.1680, 0.1445, 0.1553, 0.1436,\n",
      "        0.1436, 0.1953, 0.1533, 0.1689, 0.1406, 0.1445, 0.1582, 0.1523, 0.1709,\n",
      "        0.1230, 0.1455, 0.1299, 0.1475, 0.1504, 0.1377, 0.1416, 0.1348, 0.1504,\n",
      "        0.1924, 0.1543, 0.1631, 0.1670, 0.1777, 0.1641, 0.1475, 0.1426, 0.1533,\n",
      "        0.1396, 0.1348, 0.1553, 0.1602, 0.1680, 0.1206, 0.1494, 0.1138, 0.1475,\n",
      "        0.1553, 0.1484, 0.1309, 0.1328, 0.1572, 0.1533, 0.1631, 0.1592, 0.1631,\n",
      "        0.1406, 0.1562, 0.1699, 0.1875, 0.1455, 0.1719, 0.1738, 0.1553, 0.1240,\n",
      "        0.1533, 0.1572, 0.1406, 0.1260, 0.1855, 0.1191, 0.1338, 0.1562, 0.1338,\n",
      "        0.1738, 0.1338, 0.1670, 0.1357, 0.1592, 0.1562, 0.1221, 0.1729, 0.1445,\n",
      "        0.1719, 0.1504, 0.1426, 0.1260, 0.1650, 0.1455, 0.1514, 0.1152, 0.1348,\n",
      "        0.1138, 0.1572, 0.1328, 0.1592, 0.1455, 0.1543, 0.1719, 0.1475, 0.1748,\n",
      "        0.1475, 0.1226, 0.1172, 0.1387, 0.1406, 0.1689, 0.1338, 0.1475, 0.1484,\n",
      "        0.1562, 0.1455, 0.1758, 0.1592, 0.1436, 0.1943, 0.1367, 0.1396, 0.1807,\n",
      "        0.1475, 0.1494, 0.1582, 0.1572, 0.1387, 0.1621, 0.1465, 0.1436, 0.1514,\n",
      "        0.0898, 0.1494, 0.1416, 0.1572, 0.1611, 0.1406, 0.1719, 0.1494, 0.1543,\n",
      "        0.1514, 0.1318, 0.1533, 0.1436, 0.1768, 0.1475, 0.1484, 0.1250, 0.1445,\n",
      "        0.1650, 0.1387, 0.1562, 0.1650, 0.1504, 0.1592, 0.1465, 0.1426, 0.1201,\n",
      "        0.1533, 0.1611, 0.1582, 0.1367, 0.1250, 0.1396, 0.1387, 0.0381, 0.1641,\n",
      "        0.1729, 0.1611, 0.1562, 0.1709, 0.1602, 0.1289, 0.1738, 0.1426, 0.1670,\n",
      "        0.1348, 0.1641, 0.1660, 0.1504, 0.1523, 0.1367, 0.1328, 0.1660, 0.2070,\n",
      "        0.1562, 0.1582, 0.1270, 0.1582, 0.1494, 0.1572, 0.1729, 0.1855, 0.1436,\n",
      "        0.1631, 0.1641, 0.1562, 0.1436, 0.1465, 0.1230, 0.1621, 0.1504, 0.1309,\n",
      "        0.1553, 0.1494, 0.1572, 0.1484, 0.1494, 0.1533, 0.1523, 0.1016, 0.1650,\n",
      "        0.0449, 0.1367, 0.1475, 0.1602, 0.1582, 0.1631, 0.1621, 0.1748, 0.1406,\n",
      "        0.1328, 0.1436, 0.1396, 0.1523, 0.1357, 0.1250, 0.1494, 0.1416, 0.1768,\n",
      "        0.1338, 0.1494, 0.1523, 0.1240, 0.1426, 0.1465, 0.1572, 0.1309, 0.1494,\n",
      "        0.1289, 0.1504, 0.1768, 0.1377, 0.1533, 0.1226, 0.1797, 0.1367, 0.1426,\n",
      "        0.1289, 0.1553, 0.1523, 0.1436, 0.1680, 0.1348, 0.1504, 0.1641, 0.1172,\n",
      "        0.1396, 0.1631, 0.1475, 0.1270, 0.1182, 0.1147, 0.1738, 0.1279, 0.1279,\n",
      "        0.1650, 0.1670, 0.1357, 0.1553, 0.1387, 0.1543, 0.1768, 0.1973, 0.1533,\n",
      "        0.1445, 0.1206, 0.1582, 0.1553, 0.1572, 0.1777, 0.1758, 0.1299, 0.1816,\n",
      "        0.1855, 0.1465, 0.1582, 0.1699, 0.1709, 0.1338, 0.1533, 0.1299, 0.1631,\n",
      "        0.1387, 0.1455, 0.2100, 0.1436, 0.1670, 0.1670, 0.1904, 0.1572, 0.1553,\n",
      "        0.1621, 0.1357, 0.1562, 0.1797, 0.1167, 0.1514, 0.1533, 0.1533],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4980, -0.2676, -0.2969,  ...,  0.0752,  0.0703, -0.1494],\n",
      "        [-0.1680, -0.2969,  0.3672,  ..., -0.4668,  0.3164,  0.0732],\n",
      "        [ 0.0042,  0.2490,  0.4219,  ...,  0.2041, -0.0291,  0.2754],\n",
      "        ...,\n",
      "        [ 0.3281,  0.3555,  0.4609,  ...,  0.5234, -0.3770, -0.0349],\n",
      "        [-0.3086, -0.4414,  0.4473,  ...,  0.4922, -0.0347, -0.3926],\n",
      "        [-0.7891, -0.2637, -0.3242,  ...,  0.5859,  0.4395, -0.3574]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0703, -1.1406,  0.6523,  ...,  0.0840,  0.9648, -0.1406],\n",
      "        [ 0.1040, -0.3359,  0.6250,  ..., -0.2832,  1.3594,  0.4512],\n",
      "        [ 0.1562, -0.5117,  0.7617,  ...,  0.4199, -0.1348, -1.1719],\n",
      "        ...,\n",
      "        [-0.5820,  0.9922,  0.5000,  ...,  0.0574,  0.4258,  0.5703],\n",
      "        [-2.7500,  2.0625,  2.1250,  ...,  0.8320,  1.2031,  0.9961],\n",
      "        [ 0.2871, -1.2500,  0.1123,  ...,  0.4629,  0.7656,  0.2617]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8750, -0.0952,  0.1016,  ...,  1.1094, -0.3984, -0.0442],\n",
      "        [ 0.1318,  0.0327,  0.6719,  ..., -0.7500,  0.0806, -0.3203],\n",
      "        [ 0.1758,  0.3984, -0.3438,  ...,  1.1250,  1.1719,  0.2383],\n",
      "        ...,\n",
      "        [-0.3633,  0.5469, -0.7148,  ...,  0.3457, -0.0801,  0.3086],\n",
      "        [-0.1592,  1.0078, -0.2949,  ...,  0.4590, -0.6016, -0.0693],\n",
      "        [ 0.0410, -0.0123, -0.1992,  ..., -0.4629, -0.3438,  0.4688]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1250, 0.1250, 0.0869, 0.1182, 0.1094, 0.1035, 0.0483, 0.1089, 0.0815,\n",
      "        0.1270, 0.1152, 0.0417, 0.1025, 0.0908, 0.0923, 0.1299, 0.0938, 0.0864,\n",
      "        0.1064, 0.0918, 0.0918, 0.0742, 0.1016, 0.0972, 0.0942, 0.0767, 0.0713,\n",
      "        0.1050, 0.1045, 0.0869, 0.1006, 0.1172, 0.1377, 0.1025, 0.0942, 0.1045,\n",
      "        0.1089, 0.0913, 0.0967, 0.0786, 0.1069, 0.0986, 0.1104, 0.0845, 0.1021,\n",
      "        0.1123, 0.0981, 0.1206, 0.0903, 0.1035, 0.0850, 0.1182, 0.1021, 0.1118,\n",
      "        0.1172, 0.1104, 0.0908, 0.1177, 0.1001, 0.0850, 0.1104, 0.0923, 0.1050,\n",
      "        0.1108, 0.1318, 0.1143, 0.0762, 0.1060, 0.1235, 0.0908, 0.1230, 0.0835,\n",
      "        0.0894, 0.0918, 0.0957, 0.0996, 0.0942, 0.1196, 0.0452, 0.0986, 0.1113,\n",
      "        0.0972, 0.0942, 0.0977, 0.1060, 0.1113, 0.1074, 0.1001, 0.1089, 0.1094,\n",
      "        0.0889, 0.0850, 0.0942, 0.1045, 0.1240, 0.1011, 0.1108, 0.1216, 0.0889,\n",
      "        0.0825, 0.0869, 0.0918, 0.1055, 0.1108, 0.0938, 0.0913, 0.1094, 0.0947,\n",
      "        0.0801, 0.1016, 0.0869, 0.0962, 0.1338, 0.0791, 0.0923, 0.1011, 0.0718,\n",
      "        0.0947, 0.0913, 0.1182, 0.1001, 0.1011, 0.0962, 0.1104, 0.0938, 0.1099,\n",
      "        0.0972, 0.1001, 0.0864, 0.0835, 0.1113, 0.0952, 0.0933, 0.0791, 0.0850,\n",
      "        0.0981, 0.2256, 0.0952, 0.0962, 0.1069, 0.0884, 0.1226, 0.0918, 0.1152,\n",
      "        0.1069, 0.1270, 0.0918, 0.0967, 0.0923, 0.0903, 0.1060, 0.1138, 0.0898,\n",
      "        0.1211, 0.0801, 0.0972, 0.0977, 0.1079, 0.1025, 0.1021, 0.1011, 0.1270,\n",
      "        0.1104, 0.0302, 0.1089, 0.0972, 0.0928, 0.1167, 0.0996, 0.1035, 0.1177,\n",
      "        0.0947, 0.1123, 0.0923, 0.1216, 0.0972, 0.0977, 0.0781, 0.1133, 0.1074,\n",
      "        0.0835, 0.0815, 0.1113, 0.0850, 0.0967, 0.1089, 0.1128, 0.1128, 0.1128,\n",
      "        0.0918, 0.0879, 0.1084, 0.0991, 0.1055, 0.1016, 0.0840, 0.0947, 0.1040,\n",
      "        0.1035, 0.1035, 0.0942, 0.0918, 0.0942, 0.0942, 0.0972, 0.0918, 0.0879,\n",
      "        0.0879, 0.1035, 0.1128, 0.1147, 0.0928, 0.1035, 0.0879, 0.1177, 0.1084,\n",
      "        0.1021, 0.1060, 0.0947, 0.1211, 0.0762, 0.0874, 0.0938, 0.1030, 0.0854,\n",
      "        0.0840, 0.1021, 0.0898, 0.0938, 0.1226, 0.0879, 0.0986, 0.0889, 0.1025,\n",
      "        0.1201, 0.1021, 0.1055, 0.1245, 0.1113, 0.1021, 0.1084, 0.1001, 0.0942,\n",
      "        0.0933, 0.0972, 0.1143, 0.0991, 0.0962, 0.0825, 0.1011, 0.0815, 0.1079,\n",
      "        0.1025, 0.1021, 0.0854, 0.0908, 0.0957, 0.1006, 0.1035, 0.1206, 0.1040,\n",
      "        0.0835, 0.0864, 0.1035, 0.1235, 0.1147, 0.0996, 0.1167, 0.0864, 0.0874,\n",
      "        0.1045, 0.0996, 0.0967, 0.0923, 0.1167, 0.0918, 0.0938, 0.1133, 0.0889,\n",
      "        0.1050, 0.0957, 0.0938, 0.0898, 0.1196, 0.1118, 0.0850, 0.1089, 0.0942,\n",
      "        0.1167, 0.1001, 0.1021, 0.0918, 0.1079, 0.0913, 0.1040, 0.0708, 0.0977,\n",
      "        0.0757, 0.1035, 0.0957, 0.1309, 0.1118, 0.0991, 0.1172, 0.1001, 0.1064,\n",
      "        0.1123, 0.0845, 0.0845, 0.0991, 0.1006, 0.0981, 0.0879, 0.1035, 0.1040,\n",
      "        0.1162, 0.1143, 0.1162, 0.1001, 0.0898, 0.1216, 0.0938, 0.0938, 0.1074,\n",
      "        0.1011, 0.0811, 0.1162, 0.0933, 0.1030, 0.1133, 0.0977, 0.0977, 0.1060,\n",
      "        0.0610, 0.1201, 0.0869, 0.0889, 0.1118, 0.0854, 0.1157, 0.1016, 0.1099,\n",
      "        0.1030, 0.0977, 0.0991, 0.0967, 0.1060, 0.0938, 0.1001, 0.0991, 0.0938,\n",
      "        0.1089, 0.0952, 0.0952, 0.0986, 0.0928, 0.1069, 0.0908, 0.1045, 0.0889,\n",
      "        0.0967, 0.0977, 0.0986, 0.0850, 0.0757, 0.1064, 0.0879, 0.0342, 0.1099,\n",
      "        0.1118, 0.1318, 0.0977, 0.1177, 0.1162, 0.0815, 0.1045, 0.0972, 0.0952,\n",
      "        0.1011, 0.1035, 0.1064, 0.1025, 0.0952, 0.1001, 0.0918, 0.1201, 0.1289,\n",
      "        0.1099, 0.0947, 0.0977, 0.1113, 0.0889, 0.0967, 0.1016, 0.1216, 0.1016,\n",
      "        0.1016, 0.1064, 0.1055, 0.1016, 0.0938, 0.0845, 0.1084, 0.1001, 0.0967,\n",
      "        0.1040, 0.1021, 0.1055, 0.1050, 0.0952, 0.0977, 0.1030, 0.0425, 0.0996,\n",
      "        0.0344, 0.0991, 0.1089, 0.0913, 0.0933, 0.1113, 0.1025, 0.1245, 0.0947,\n",
      "        0.0830, 0.0771, 0.0981, 0.1040, 0.0962, 0.0942, 0.0898, 0.1011, 0.1270,\n",
      "        0.0918, 0.1143, 0.1025, 0.0801, 0.0977, 0.1025, 0.0957, 0.0776, 0.0923,\n",
      "        0.0933, 0.0811, 0.1167, 0.1055, 0.1094, 0.0815, 0.1113, 0.0864, 0.0942,\n",
      "        0.0806, 0.1079, 0.1001, 0.0864, 0.1084, 0.0874, 0.0977, 0.1108, 0.0796,\n",
      "        0.0996, 0.1118, 0.0996, 0.0854, 0.0752, 0.0859, 0.1055, 0.0938, 0.0806,\n",
      "        0.1045, 0.1162, 0.0908, 0.1006, 0.0786, 0.0986, 0.1045, 0.1147, 0.0967,\n",
      "        0.0952, 0.0762, 0.1079, 0.1074, 0.0972, 0.1250, 0.1084, 0.0972, 0.1196,\n",
      "        0.1211, 0.1060, 0.1143, 0.1064, 0.1201, 0.0864, 0.0903, 0.1025, 0.1206,\n",
      "        0.0967, 0.1030, 0.1226, 0.1055, 0.1108, 0.1138, 0.1172, 0.0923, 0.1128,\n",
      "        0.1089, 0.0977, 0.1016, 0.1118, 0.0781, 0.1045, 0.1025, 0.1523],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0479, -0.0228, -0.0104,  ...,  0.0225,  0.0070, -0.0081],\n",
      "        [-0.0386, -0.0124,  0.0125,  ...,  0.0110,  0.0142, -0.0247],\n",
      "        [ 0.0170,  0.0732,  0.0513,  ...,  0.0854,  0.0018, -0.0063],\n",
      "        ...,\n",
      "        [-0.0119,  0.0048,  0.0208,  ...,  0.0008,  0.0199, -0.0879],\n",
      "        [ 0.0250, -0.0182,  0.0033,  ...,  0.0957,  0.0139,  0.0266],\n",
      "        [-0.0189, -0.0097, -0.0557,  ...,  0.0108,  0.0432, -0.0801]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.7734, -0.8984,  0.2988,  ...,  0.6680, -0.1523, -0.2988],\n",
      "        [-0.4590, -0.0388, -0.0021,  ...,  0.5898, -0.3887, -0.7617],\n",
      "        [ 0.5820,  0.1128,  0.4258,  ..., -0.0356,  0.1660,  0.1050],\n",
      "        ...,\n",
      "        [-0.0708, -0.4336,  0.1367,  ..., -0.2412,  0.1367, -0.3926],\n",
      "        [ 0.2061,  0.3477,  0.2217,  ...,  1.0078, -0.5547, -0.0471],\n",
      "        [-0.1040, -0.0090,  0.3008,  ..., -0.3145,  0.4199, -0.3652]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3262,  1.2266, -0.2295,  ..., -0.1504,  0.3398, -0.5352],\n",
      "        [ 0.0986, -0.3496, -0.3359,  ..., -0.6211, -0.0022, -0.5039],\n",
      "        [ 0.6602,  1.4062,  0.6367,  ..., -0.0369,  0.7578,  0.3125],\n",
      "        ...,\n",
      "        [ 0.3125,  0.1504,  0.1182,  ..., -0.5625,  0.9258,  0.4043],\n",
      "        [ 1.4766,  0.7734, -0.1001,  ..., -0.8945,  0.2031, -0.1050],\n",
      "        [ 0.8438, -0.4355,  0.7383,  ...,  0.7188,  0.4961, -0.0227]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2852,  0.3477, -0.2891,  ...,  0.3945,  0.0388, -0.9766],\n",
      "        [ 0.3086,  0.6328,  0.2197,  ..., -0.2119, -0.3047,  0.3223],\n",
      "        [ 0.4922,  0.3555,  0.5234,  ...,  0.0996, -0.7773,  0.0442],\n",
      "        ...,\n",
      "        [-0.1758, -0.0289,  0.3691,  ..., -1.0078,  0.0505, -1.2344],\n",
      "        [ 0.3652,  0.1128, -0.2119,  ..., -0.3691, -0.2773, -0.4473],\n",
      "        [-0.2246,  0.2949, -0.0066,  ...,  0.4727, -0.1768,  0.4082]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1953, 0.1865, 0.1367, 0.2061, 0.1758, 0.1582, 0.0425, 0.1807, 0.1387,\n",
      "        0.2334, 0.2207, 0.0703, 0.1572, 0.1484, 0.1592, 0.2002, 0.1572, 0.1494,\n",
      "        0.1387, 0.1396, 0.1426, 0.1201, 0.1709, 0.1973, 0.1328, 0.1328, 0.1157,\n",
      "        0.1807, 0.1660, 0.1543, 0.1689, 0.1943, 0.1523, 0.1484, 0.1650, 0.1709,\n",
      "        0.1748, 0.1572, 0.1689, 0.1338, 0.1504, 0.1504, 0.1855, 0.1367, 0.1572,\n",
      "        0.1738, 0.1660, 0.2148, 0.1514, 0.1826, 0.1357, 0.2012, 0.1572, 0.2051,\n",
      "        0.1807, 0.2100, 0.1621, 0.1855, 0.1611, 0.1357, 0.1943, 0.1826, 0.1680,\n",
      "        0.1738, 0.1963, 0.2109, 0.1270, 0.1865, 0.1631, 0.1396, 0.1807, 0.1348,\n",
      "        0.1660, 0.1514, 0.1348, 0.1543, 0.1582, 0.2217, 0.0635, 0.1602, 0.1729,\n",
      "        0.1494, 0.1650, 0.1670, 0.1758, 0.1807, 0.1611, 0.1611, 0.1650, 0.1807,\n",
      "        0.1436, 0.1514, 0.1562, 0.1592, 0.2061, 0.1680, 0.1650, 0.1748, 0.1299,\n",
      "        0.1455, 0.1426, 0.1699, 0.1621, 0.1572, 0.1436, 0.1416, 0.1885, 0.1699,\n",
      "        0.1250, 0.1680, 0.1348, 0.1768, 0.2031, 0.1348, 0.1504, 0.1572, 0.1084,\n",
      "        0.1562, 0.1572, 0.1865, 0.1592, 0.1572, 0.1562, 0.1953, 0.1475, 0.1758,\n",
      "        0.1621, 0.1543, 0.1475, 0.1318, 0.1465, 0.1553, 0.1553, 0.1211, 0.1377,\n",
      "        0.1670, 0.0486, 0.1621, 0.1582, 0.1641, 0.1514, 0.1973, 0.1699, 0.1934,\n",
      "        0.1777, 0.2217, 0.1709, 0.1689, 0.1475, 0.1592, 0.1797, 0.1914, 0.1611,\n",
      "        0.2090, 0.1533, 0.1699, 0.1709, 0.1699, 0.1650, 0.1562, 0.1660, 0.1895,\n",
      "        0.1660, 0.0464, 0.1650, 0.1494, 0.1504, 0.2139, 0.1680, 0.1934, 0.1934,\n",
      "        0.1338, 0.1816, 0.1660, 0.1748, 0.1514, 0.1670, 0.1504, 0.1680, 0.1631,\n",
      "        0.1367, 0.1338, 0.1582, 0.1572, 0.1514, 0.1650, 0.1699, 0.1680, 0.1826,\n",
      "        0.1406, 0.1621, 0.1504, 0.1631, 0.1875, 0.1836, 0.1299, 0.1475, 0.2061,\n",
      "        0.1572, 0.1436, 0.1465, 0.1621, 0.1611, 0.1455, 0.1387, 0.1416, 0.1367,\n",
      "        0.1338, 0.1719, 0.1484, 0.1914, 0.1562, 0.1768, 0.1455, 0.1943, 0.1660,\n",
      "        0.1670, 0.1709, 0.1484, 0.1953, 0.1318, 0.1426, 0.1748, 0.1631, 0.1660,\n",
      "        0.1299, 0.1758, 0.1396, 0.1504, 0.1816, 0.1436, 0.1729, 0.1455, 0.1465,\n",
      "        0.2002, 0.1602, 0.1895, 0.1670, 0.1748, 0.1738, 0.1641, 0.1650, 0.1611,\n",
      "        0.1504, 0.1494, 0.1758, 0.1592, 0.1367, 0.1216, 0.1689, 0.1211, 0.1562,\n",
      "        0.1553, 0.1670, 0.1514, 0.1592, 0.1543, 0.1699, 0.1797, 0.1963, 0.1621,\n",
      "        0.1396, 0.1465, 0.1680, 0.2002, 0.1699, 0.1816, 0.1924, 0.1348, 0.1436,\n",
      "        0.1533, 0.1719, 0.1670, 0.1318, 0.1689, 0.1602, 0.1514, 0.1816, 0.1445,\n",
      "        0.1670, 0.1484, 0.1523, 0.1494, 0.1621, 0.1768, 0.1426, 0.1650, 0.1533,\n",
      "        0.2002, 0.1592, 0.1943, 0.1611, 0.1719, 0.1465, 0.1582, 0.1030, 0.1592,\n",
      "        0.1211, 0.1611, 0.1592, 0.1807, 0.1758, 0.1504, 0.1934, 0.1533, 0.1660,\n",
      "        0.1777, 0.1348, 0.1279, 0.1689, 0.1631, 0.1611, 0.1553, 0.1680, 0.1885,\n",
      "        0.1709, 0.1836, 0.1895, 0.1553, 0.1553, 0.2100, 0.1602, 0.1592, 0.1846,\n",
      "        0.1689, 0.1436, 0.1943, 0.1582, 0.1699, 0.1855, 0.1611, 0.1484, 0.1729,\n",
      "        0.0884, 0.1787, 0.1416, 0.1523, 0.1934, 0.1162, 0.1748, 0.1670, 0.1836,\n",
      "        0.1738, 0.1514, 0.1807, 0.1533, 0.1797, 0.1533, 0.1709, 0.1445, 0.1572,\n",
      "        0.1709, 0.1436, 0.1641, 0.1758, 0.1660, 0.1738, 0.1660, 0.1680, 0.1260,\n",
      "        0.1650, 0.1719, 0.1680, 0.1396, 0.1260, 0.1729, 0.1592, 0.0403, 0.1787,\n",
      "        0.2100, 0.1787, 0.1660, 0.1924, 0.1885, 0.1318, 0.1807, 0.1484, 0.1553,\n",
      "        0.1572, 0.1797, 0.1758, 0.1807, 0.1699, 0.1660, 0.1455, 0.1953, 0.2061,\n",
      "        0.1758, 0.1768, 0.1523, 0.1719, 0.1465, 0.1523, 0.1660, 0.1992, 0.1572,\n",
      "        0.1738, 0.1738, 0.1602, 0.1777, 0.1416, 0.1504, 0.1826, 0.1680, 0.1494,\n",
      "        0.1709, 0.1602, 0.1641, 0.1445, 0.1699, 0.1709, 0.1787, 0.0762, 0.1699,\n",
      "        0.0544, 0.1680, 0.1758, 0.1582, 0.1338, 0.1914, 0.1631, 0.1855, 0.1562,\n",
      "        0.1396, 0.1309, 0.1660, 0.1641, 0.1494, 0.1367, 0.1299, 0.1650, 0.2334,\n",
      "        0.1436, 0.1826, 0.1641, 0.1270, 0.1602, 0.1719, 0.1494, 0.1216, 0.1631,\n",
      "        0.1279, 0.1543, 0.1758, 0.1699, 0.1709, 0.1367, 0.2168, 0.1514, 0.1660,\n",
      "        0.1377, 0.1660, 0.1650, 0.1562, 0.1807, 0.1445, 0.1738, 0.1826, 0.1328,\n",
      "        0.1533, 0.1729, 0.1553, 0.1484, 0.1201, 0.1235, 0.1836, 0.1484, 0.1245,\n",
      "        0.1631, 0.2061, 0.1553, 0.1748, 0.1235, 0.1602, 0.1924, 0.1846, 0.1611,\n",
      "        0.1641, 0.1279, 0.1865, 0.1748, 0.1602, 0.1855, 0.1689, 0.1504, 0.1992,\n",
      "        0.2021, 0.1738, 0.1807, 0.1875, 0.2002, 0.1484, 0.1650, 0.1504, 0.1787,\n",
      "        0.1768, 0.1611, 0.2080, 0.1680, 0.1748, 0.1719, 0.1895, 0.1621, 0.1787,\n",
      "        0.1904, 0.1436, 0.1699, 0.1865, 0.1416, 0.1553, 0.1309, 0.1533],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0167, -0.1118,  0.2656,  ...,  0.5000, -0.2070,  0.3047],\n",
      "        [ 0.4961, -0.0195, -0.0786,  ..., -0.0623, -0.4453,  0.3711],\n",
      "        [ 0.2178,  0.4688,  0.0698,  ..., -0.1079, -0.0075, -0.4473],\n",
      "        ...,\n",
      "        [ 0.4766, -0.6328, -0.5039,  ...,  0.0151, -0.2490, -0.1245],\n",
      "        [-0.1650, -0.2207, -0.6758,  ...,  0.3242,  0.6914, -0.3984],\n",
      "        [-0.0762,  0.5078,  0.4863,  ..., -0.2012,  0.0854,  0.7617]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2832,  1.3047, -0.3496,  ...,  0.5781, -1.6250,  0.2021],\n",
      "        [-1.9453, -0.2383,  1.7109,  ...,  0.7461, -2.3906,  0.1147],\n",
      "        [ 0.9492,  0.2910,  0.7109,  ...,  1.8203,  1.7500,  1.1641],\n",
      "        ...,\n",
      "        [-2.0312,  1.8047, -0.5742,  ...,  0.9258,  0.0801,  0.5156],\n",
      "        [-0.7891,  0.7930, -0.2363,  ...,  0.3594, -0.0874,  0.6133],\n",
      "        [ 0.8711,  0.2520, -0.3965,  ..., -0.3379,  0.2578,  0.7422]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.7148, -0.0488, -0.6680,  ..., -1.0156, -0.1079,  0.7266],\n",
      "        [ 0.4707, -0.0845, -0.4395,  ...,  0.5586, -0.1279,  0.2500],\n",
      "        [ 0.0776, -0.0347,  0.1992,  ..., -0.3711,  0.4238,  0.2754],\n",
      "        ...,\n",
      "        [ 0.4316, -0.6016,  0.0618,  ...,  0.5859, -0.2246,  0.6211],\n",
      "        [ 0.3164,  0.4160, -0.6875,  ...,  0.3379, -0.9375,  0.6289],\n",
      "        [-0.6016, -0.1094,  0.1055,  ..., -0.0190, -0.2412, -0.3789]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1279, 0.1226, 0.0903, 0.1289, 0.1147, 0.1074, 0.0481, 0.1079, 0.0889,\n",
      "        0.1260, 0.1162, 0.0491, 0.1074, 0.0864, 0.0991, 0.1367, 0.1050, 0.0908,\n",
      "        0.1025, 0.1025, 0.0928, 0.0811, 0.1030, 0.0986, 0.0923, 0.0825, 0.0879,\n",
      "        0.1030, 0.1108, 0.0913, 0.1025, 0.1030, 0.1377, 0.1001, 0.1045, 0.1118,\n",
      "        0.1055, 0.0996, 0.1133, 0.0918, 0.0991, 0.1104, 0.1172, 0.0850, 0.1064,\n",
      "        0.1025, 0.1021, 0.1270, 0.0967, 0.1167, 0.0957, 0.1118, 0.1035, 0.1260,\n",
      "        0.1152, 0.1187, 0.0933, 0.1050, 0.1035, 0.0938, 0.1157, 0.0972, 0.1108,\n",
      "        0.1060, 0.1348, 0.1123, 0.0811, 0.1084, 0.1206, 0.0918, 0.1328, 0.0913,\n",
      "        0.1050, 0.1050, 0.0894, 0.0991, 0.1001, 0.1196, 0.0449, 0.1069, 0.1118,\n",
      "        0.1040, 0.0952, 0.0996, 0.1162, 0.1143, 0.1050, 0.1064, 0.1221, 0.1147,\n",
      "        0.0977, 0.0918, 0.1030, 0.1050, 0.1226, 0.1113, 0.1079, 0.1157, 0.0938,\n",
      "        0.0928, 0.0928, 0.1074, 0.1089, 0.1099, 0.0957, 0.0913, 0.1152, 0.0947,\n",
      "        0.0879, 0.1147, 0.0972, 0.0981, 0.1245, 0.0879, 0.1006, 0.1045, 0.0767,\n",
      "        0.0977, 0.0962, 0.1133, 0.0991, 0.1040, 0.0986, 0.1138, 0.0942, 0.1079,\n",
      "        0.1016, 0.1050, 0.1030, 0.0933, 0.1030, 0.0996, 0.1040, 0.0908, 0.0898,\n",
      "        0.0981, 0.2490, 0.0967, 0.1025, 0.1123, 0.0972, 0.1157, 0.0908, 0.1182,\n",
      "        0.1079, 0.1357, 0.1001, 0.1001, 0.1021, 0.0996, 0.1030, 0.1147, 0.1006,\n",
      "        0.1152, 0.0952, 0.1025, 0.1084, 0.1055, 0.1040, 0.1040, 0.1021, 0.1201,\n",
      "        0.1030, 0.0342, 0.1079, 0.0986, 0.0977, 0.1123, 0.0962, 0.1060, 0.1162,\n",
      "        0.1001, 0.1089, 0.0967, 0.1167, 0.1006, 0.1030, 0.0835, 0.1050, 0.1113,\n",
      "        0.0850, 0.0889, 0.1064, 0.0869, 0.1069, 0.1118, 0.1104, 0.1167, 0.1089,\n",
      "        0.0894, 0.0933, 0.0967, 0.1050, 0.1035, 0.1079, 0.0928, 0.1089, 0.1104,\n",
      "        0.1011, 0.1079, 0.0967, 0.0942, 0.1025, 0.0942, 0.0972, 0.0957, 0.0913,\n",
      "        0.0933, 0.1157, 0.1167, 0.1182, 0.1006, 0.0996, 0.0928, 0.1133, 0.1040,\n",
      "        0.1084, 0.1113, 0.1045, 0.1138, 0.0835, 0.0977, 0.1011, 0.0981, 0.0898,\n",
      "        0.0903, 0.1030, 0.0928, 0.1001, 0.1123, 0.0928, 0.1030, 0.0972, 0.1030,\n",
      "        0.1113, 0.1016, 0.1157, 0.1099, 0.1235, 0.1030, 0.1187, 0.1040, 0.0972,\n",
      "        0.0938, 0.0977, 0.1201, 0.1011, 0.0918, 0.0894, 0.1035, 0.0913, 0.1099,\n",
      "        0.1060, 0.1060, 0.0859, 0.0991, 0.0947, 0.1021, 0.1040, 0.1279, 0.0957,\n",
      "        0.0903, 0.0933, 0.1094, 0.1147, 0.1191, 0.1055, 0.1167, 0.1011, 0.0942,\n",
      "        0.1084, 0.1030, 0.1040, 0.0933, 0.1118, 0.1064, 0.0952, 0.1196, 0.0933,\n",
      "        0.1108, 0.0933, 0.1089, 0.0962, 0.1118, 0.1133, 0.0923, 0.1108, 0.1045,\n",
      "        0.1055, 0.1050, 0.1128, 0.0928, 0.1133, 0.0981, 0.1069, 0.0835, 0.1069,\n",
      "        0.0869, 0.1064, 0.0996, 0.1230, 0.1118, 0.1030, 0.1094, 0.0986, 0.1152,\n",
      "        0.1113, 0.0879, 0.0864, 0.1035, 0.1045, 0.1089, 0.0913, 0.1035, 0.1118,\n",
      "        0.1196, 0.1104, 0.1182, 0.1079, 0.0981, 0.1108, 0.0962, 0.0962, 0.1128,\n",
      "        0.1064, 0.0908, 0.1250, 0.0991, 0.1089, 0.1147, 0.0991, 0.0938, 0.1128,\n",
      "        0.0654, 0.1123, 0.0947, 0.1011, 0.1211, 0.0874, 0.1201, 0.1084, 0.1108,\n",
      "        0.1099, 0.0947, 0.1104, 0.1045, 0.1064, 0.1025, 0.1064, 0.0962, 0.1040,\n",
      "        0.1084, 0.0986, 0.1025, 0.1079, 0.0957, 0.1084, 0.1001, 0.1040, 0.0972,\n",
      "        0.1006, 0.1035, 0.0947, 0.0806, 0.0840, 0.1123, 0.0962, 0.0339, 0.1045,\n",
      "        0.1094, 0.1309, 0.1035, 0.1138, 0.1221, 0.0874, 0.1079, 0.0938, 0.1011,\n",
      "        0.1011, 0.1021, 0.1099, 0.1079, 0.0972, 0.1035, 0.0947, 0.1235, 0.1289,\n",
      "        0.1133, 0.1001, 0.0967, 0.1123, 0.0942, 0.0972, 0.1006, 0.1260, 0.0967,\n",
      "        0.1118, 0.1074, 0.1064, 0.1079, 0.0977, 0.0913, 0.1113, 0.1064, 0.0967,\n",
      "        0.1016, 0.1021, 0.1069, 0.1060, 0.1030, 0.1079, 0.1113, 0.0474, 0.1104,\n",
      "        0.0381, 0.1060, 0.1021, 0.1094, 0.0962, 0.1099, 0.1035, 0.1211, 0.0967,\n",
      "        0.0908, 0.0938, 0.1069, 0.1060, 0.0928, 0.0942, 0.0981, 0.1099, 0.1250,\n",
      "        0.0962, 0.1094, 0.1113, 0.0859, 0.1021, 0.1079, 0.0928, 0.0845, 0.0938,\n",
      "        0.0977, 0.0903, 0.1162, 0.1025, 0.1128, 0.0923, 0.1187, 0.0894, 0.1011,\n",
      "        0.0845, 0.1133, 0.1006, 0.0977, 0.1108, 0.0947, 0.1074, 0.1084, 0.0884,\n",
      "        0.1045, 0.1040, 0.1021, 0.0938, 0.0825, 0.0918, 0.1104, 0.0908, 0.0864,\n",
      "        0.1050, 0.1182, 0.0962, 0.1094, 0.0889, 0.1021, 0.1094, 0.1152, 0.0977,\n",
      "        0.1045, 0.0938, 0.1157, 0.1094, 0.1030, 0.1133, 0.1118, 0.1001, 0.1147,\n",
      "        0.1172, 0.1118, 0.1138, 0.1079, 0.1230, 0.0972, 0.0996, 0.1055, 0.1113,\n",
      "        0.1147, 0.1050, 0.1143, 0.1035, 0.1216, 0.1123, 0.1211, 0.1016, 0.1055,\n",
      "        0.1104, 0.1001, 0.1055, 0.1187, 0.0898, 0.0967, 0.1079, 0.1709],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0376, -0.0183,  0.0043,  ..., -0.0159, -0.0378, -0.1001],\n",
      "        [ 0.0183,  0.0513, -0.0547,  ..., -0.0525, -0.0010,  0.0510],\n",
      "        [ 0.0283,  0.0269,  0.0383,  ..., -0.0249,  0.0062,  0.0179],\n",
      "        ...,\n",
      "        [ 0.0164, -0.0016, -0.0771,  ..., -0.0043, -0.0034, -0.0031],\n",
      "        [-0.0137,  0.0164, -0.0544,  ...,  0.0732, -0.0747, -0.0015],\n",
      "        [ 0.0210,  0.0032,  0.0135,  ...,  0.0058,  0.0908, -0.0179]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2012, -0.3184, -0.2021,  ...,  0.1836, -0.1348, -0.8125],\n",
      "        [-0.2217, -0.0835, -0.4062,  ..., -0.1514,  0.0491, -0.2598],\n",
      "        [ 0.0854,  0.1309, -0.1387,  ...,  0.4062,  0.3008,  0.4160],\n",
      "        ...,\n",
      "        [ 0.4043, -0.2256, -0.3262,  ..., -0.5234, -0.2158, -0.0090],\n",
      "        [-0.4121,  0.0620, -0.0981,  ...,  0.3555, -0.5781, -0.1172],\n",
      "        [ 0.0576, -0.0408, -0.2793,  ...,  0.1592,  0.4062, -0.7070]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3047, -0.2676, -0.7344,  ..., -0.2275,  1.2188,  0.3887],\n",
      "        [-0.5391,  0.1455,  0.3418,  ..., -0.1094,  1.1641,  0.4512],\n",
      "        [ 0.4863, -0.0510,  0.5352,  ...,  0.3867,  0.9336,  0.1045],\n",
      "        ...,\n",
      "        [ 1.0781,  0.0854, -0.8008,  ...,  0.7266,  0.4688, -0.0732],\n",
      "        [-0.2578,  0.3047,  0.7461,  ..., -0.6992, -0.0859, -0.7656],\n",
      "        [ 0.1406, -0.0242,  0.0457,  ...,  0.2109,  0.5547,  0.6562]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1001,  1.3984,  0.8242,  ...,  0.7188,  1.7891, -0.0884],\n",
      "        [ 0.3652, -0.2988,  1.1016,  ...,  0.3105,  0.5000,  1.4844],\n",
      "        [-0.9805, -0.1836, -0.0569,  ...,  0.9648, -0.6250, -0.4551],\n",
      "        ...,\n",
      "        [-0.8203, -0.2520, -0.4004,  ...,  0.5469,  0.8242, -0.0236],\n",
      "        [ 0.9414, -0.3984,  0.7773,  ..., -0.3535,  0.5156,  0.6328],\n",
      "        [-0.5156, -0.6289,  0.1455,  ...,  0.3008,  0.5078, -0.1240]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1914, 0.1787, 0.1670, 0.1875, 0.1689, 0.1914, 0.0405, 0.1797, 0.1641,\n",
      "        0.2090, 0.2070, 0.0742, 0.1621, 0.1445, 0.1631, 0.2246, 0.1660, 0.1533,\n",
      "        0.1855, 0.1475, 0.1504, 0.1240, 0.1670, 0.2031, 0.1504, 0.1494, 0.1367,\n",
      "        0.1699, 0.1709, 0.1582, 0.1680, 0.1631, 0.1973, 0.1504, 0.1719, 0.1807,\n",
      "        0.1670, 0.1807, 0.1797, 0.1465, 0.1680, 0.1631, 0.1826, 0.1436, 0.1533,\n",
      "        0.1826, 0.1572, 0.2031, 0.1494, 0.1865, 0.1689, 0.1729, 0.1602, 0.2012,\n",
      "        0.1582, 0.1943, 0.1621, 0.1689, 0.1572, 0.1416, 0.1689, 0.1709, 0.1934,\n",
      "        0.1738, 0.1680, 0.1904, 0.1416, 0.1797, 0.1846, 0.1533, 0.1895, 0.1562,\n",
      "        0.1680, 0.1562, 0.1367, 0.1680, 0.1543, 0.1836, 0.0659, 0.1729, 0.1914,\n",
      "        0.1484, 0.1807, 0.1699, 0.1855, 0.1963, 0.1533, 0.1602, 0.1826, 0.1709,\n",
      "        0.1523, 0.1406, 0.1562, 0.1660, 0.1914, 0.1934, 0.1543, 0.1797, 0.1582,\n",
      "        0.1465, 0.1514, 0.1631, 0.1709, 0.1699, 0.1562, 0.1660, 0.2031, 0.1689,\n",
      "        0.1357, 0.1689, 0.1494, 0.1484, 0.1729, 0.1465, 0.1582, 0.1758, 0.1426,\n",
      "        0.1533, 0.1709, 0.1836, 0.1729, 0.1768, 0.1592, 0.1680, 0.1543, 0.1611,\n",
      "        0.1729, 0.1738, 0.1641, 0.1377, 0.1475, 0.1523, 0.1592, 0.1416, 0.1514,\n",
      "        0.1650, 0.0292, 0.1826, 0.1621, 0.1562, 0.1631, 0.2002, 0.1680, 0.1758,\n",
      "        0.1729, 0.2070, 0.1494, 0.1641, 0.1572, 0.1758, 0.1758, 0.1885, 0.1475,\n",
      "        0.1660, 0.1641, 0.1611, 0.1797, 0.1631, 0.1631, 0.1611, 0.1875, 0.1670,\n",
      "        0.1426, 0.0435, 0.1670, 0.1514, 0.1602, 0.2090, 0.1465, 0.1836, 0.2148,\n",
      "        0.1455, 0.1631, 0.1592, 0.1855, 0.1670, 0.1680, 0.1445, 0.1543, 0.1895,\n",
      "        0.1318, 0.1455, 0.1768, 0.1787, 0.1602, 0.1758, 0.1631, 0.1816, 0.1816,\n",
      "        0.1377, 0.1680, 0.1348, 0.1602, 0.1777, 0.1660, 0.1523, 0.1572, 0.1914,\n",
      "        0.1631, 0.1592, 0.1514, 0.1592, 0.1748, 0.1543, 0.1504, 0.1582, 0.1504,\n",
      "        0.1592, 0.1748, 0.1582, 0.1953, 0.1729, 0.1670, 0.1543, 0.1846, 0.1602,\n",
      "        0.1631, 0.1855, 0.1689, 0.1855, 0.1494, 0.1504, 0.1719, 0.1689, 0.1836,\n",
      "        0.1572, 0.1797, 0.1484, 0.1533, 0.1621, 0.1484, 0.1738, 0.1445, 0.1465,\n",
      "        0.1895, 0.1436, 0.1855, 0.1777, 0.1787, 0.1768, 0.1709, 0.1680, 0.1689,\n",
      "        0.1445, 0.1670, 0.1729, 0.1895, 0.1592, 0.1475, 0.1797, 0.1387, 0.1660,\n",
      "        0.1543, 0.1738, 0.1387, 0.1650, 0.1689, 0.1670, 0.1738, 0.2012, 0.1689,\n",
      "        0.1553, 0.1406, 0.1807, 0.1963, 0.1895, 0.1904, 0.1826, 0.1572, 0.1543,\n",
      "        0.1621, 0.1768, 0.1631, 0.1475, 0.1729, 0.1611, 0.1592, 0.1787, 0.1533,\n",
      "        0.1768, 0.1533, 0.1621, 0.1543, 0.1543, 0.1670, 0.1641, 0.1631, 0.1670,\n",
      "        0.2031, 0.1494, 0.1963, 0.1494, 0.1807, 0.1621, 0.1611, 0.1338, 0.1787,\n",
      "        0.1484, 0.1719, 0.1680, 0.1797, 0.1709, 0.1777, 0.1836, 0.1641, 0.1777,\n",
      "        0.1670, 0.1387, 0.1475, 0.1650, 0.1553, 0.1777, 0.1484, 0.1689, 0.1963,\n",
      "        0.1768, 0.1807, 0.1777, 0.1768, 0.1631, 0.1816, 0.1543, 0.1514, 0.2148,\n",
      "        0.1787, 0.1465, 0.1885, 0.1611, 0.1729, 0.1680, 0.1426, 0.1465, 0.1855,\n",
      "        0.1001, 0.1709, 0.1650, 0.1768, 0.1934, 0.1367, 0.1982, 0.1650, 0.1660,\n",
      "        0.1631, 0.1475, 0.1875, 0.1436, 0.1777, 0.1572, 0.1680, 0.1436, 0.1533,\n",
      "        0.1758, 0.1455, 0.1572, 0.1729, 0.1689, 0.1641, 0.1543, 0.1670, 0.1514,\n",
      "        0.1660, 0.1621, 0.1592, 0.1572, 0.1416, 0.1826, 0.1504, 0.0444, 0.1758,\n",
      "        0.1982, 0.1680, 0.1699, 0.1895, 0.1855, 0.1553, 0.1680, 0.1553, 0.1572,\n",
      "        0.1729, 0.1777, 0.2021, 0.1738, 0.1494, 0.1748, 0.1406, 0.1689, 0.1885,\n",
      "        0.1729, 0.1738, 0.1611, 0.1875, 0.1621, 0.1641, 0.1680, 0.1943, 0.1533,\n",
      "        0.1641, 0.1582, 0.1670, 0.1758, 0.1416, 0.1553, 0.1885, 0.1826, 0.1621,\n",
      "        0.1641, 0.1650, 0.1650, 0.1592, 0.1641, 0.1631, 0.1670, 0.0903, 0.1875,\n",
      "        0.0596, 0.1543, 0.1758, 0.1758, 0.1533, 0.1650, 0.1660, 0.1768, 0.1523,\n",
      "        0.1445, 0.1455, 0.1533, 0.1650, 0.1504, 0.1455, 0.1719, 0.1719, 0.1953,\n",
      "        0.1494, 0.1631, 0.1631, 0.1436, 0.1582, 0.1719, 0.1582, 0.1289, 0.1523,\n",
      "        0.1533, 0.1621, 0.1826, 0.1689, 0.1768, 0.1592, 0.2207, 0.1533, 0.1719,\n",
      "        0.1523, 0.1650, 0.1650, 0.1719, 0.1748, 0.1562, 0.1680, 0.1787, 0.1602,\n",
      "        0.1465, 0.1699, 0.1611, 0.1582, 0.1533, 0.1299, 0.1777, 0.1650, 0.1484,\n",
      "        0.1709, 0.1826, 0.1641, 0.1689, 0.1504, 0.1484, 0.1777, 0.1953, 0.1514,\n",
      "        0.1689, 0.1660, 0.1748, 0.1699, 0.1729, 0.1611, 0.1777, 0.1523, 0.1670,\n",
      "        0.1758, 0.1709, 0.1836, 0.1816, 0.1816, 0.1582, 0.1650, 0.1641, 0.1699,\n",
      "        0.1777, 0.1602, 0.1865, 0.1680, 0.1777, 0.1816, 0.1797, 0.1768, 0.1787,\n",
      "        0.1631, 0.1553, 0.1689, 0.1738, 0.1523, 0.1514, 0.1543, 0.1206],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1074,  0.2217, -0.5117,  ..., -0.1719, -0.4492, -0.3633],\n",
      "        [ 0.0874, -0.2314,  0.0081,  ...,  0.4492, -0.2373, -0.1069],\n",
      "        [-0.2969, -0.4258, -0.1079,  ...,  0.0884,  0.2490, -0.2969],\n",
      "        ...,\n",
      "        [-0.2178, -0.2949, -0.0036,  ..., -0.4668, -0.4746,  0.1050],\n",
      "        [-0.2559,  0.1001, -0.6016,  ..., -0.1768,  0.0019, -0.4863],\n",
      "        [-0.4238, -0.1816,  0.1260,  ...,  0.1104,  0.3965, -0.7227]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3203,  0.9219, -0.9766,  ...,  1.0625, -1.1953, -1.3125],\n",
      "        [ 0.3711, -0.0967, -0.1826,  ..., -0.6562,  0.5625,  0.7227],\n",
      "        [ 0.6680, -0.0369, -0.0070,  ..., -0.7266, -0.6289,  0.5117],\n",
      "        ...,\n",
      "        [ 1.6016, -1.3281, -2.0312,  ..., -2.1875,  1.7656,  0.7930],\n",
      "        [ 0.2949,  0.8359,  0.2656,  ..., -0.4590,  0.2910, -1.5469],\n",
      "        [ 0.1719,  0.6758, -0.1797,  ..., -1.3125,  0.2471, -0.5156]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6367,  0.1953,  0.7266,  ...,  0.6094, -0.3105,  0.3242],\n",
      "        [-0.3594,  0.1865, -0.0439,  ..., -0.4590, -0.5703, -0.0640],\n",
      "        [ 0.7852,  0.1504, -0.1885,  ...,  0.0708,  0.9219,  0.4844],\n",
      "        ...,\n",
      "        [ 0.0396,  1.0234, -1.3906,  ..., -0.0034,  0.0786,  0.4414],\n",
      "        [ 0.1621, -0.1177, -0.9062,  ...,  0.0189, -0.3457, -0.7656],\n",
      "        [-0.1377,  0.0903,  0.1436,  ...,  0.2539, -0.3613, -0.1270]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1230, 0.1289, 0.1001, 0.1250, 0.1089, 0.1172, 0.0530, 0.1079, 0.1006,\n",
      "        0.1260, 0.1230, 0.0645, 0.1152, 0.0923, 0.1064, 0.1387, 0.1089, 0.0991,\n",
      "        0.1055, 0.1074, 0.0962, 0.0864, 0.1099, 0.1035, 0.0962, 0.1011, 0.0947,\n",
      "        0.1094, 0.1177, 0.0986, 0.1074, 0.1143, 0.1709, 0.1045, 0.1147, 0.1182,\n",
      "        0.1006, 0.1118, 0.1206, 0.0923, 0.1055, 0.1147, 0.1191, 0.0991, 0.1113,\n",
      "        0.1118, 0.1064, 0.1260, 0.1074, 0.1250, 0.0986, 0.1211, 0.1064, 0.1318,\n",
      "        0.1133, 0.1147, 0.0977, 0.1167, 0.1089, 0.0981, 0.1250, 0.1040, 0.1108,\n",
      "        0.1104, 0.1357, 0.1074, 0.0898, 0.1157, 0.1123, 0.0972, 0.1289, 0.1035,\n",
      "        0.1089, 0.1021, 0.1035, 0.1099, 0.1030, 0.1182, 0.0527, 0.1064, 0.1157,\n",
      "        0.1138, 0.1035, 0.1089, 0.1221, 0.1177, 0.1118, 0.1143, 0.1216, 0.1167,\n",
      "        0.0996, 0.1016, 0.1089, 0.1099, 0.1299, 0.1128, 0.1182, 0.1147, 0.1011,\n",
      "        0.0981, 0.0972, 0.1021, 0.1177, 0.1069, 0.1118, 0.1016, 0.1201, 0.1006,\n",
      "        0.0947, 0.1089, 0.0938, 0.1030, 0.1152, 0.1011, 0.1016, 0.1216, 0.0898,\n",
      "        0.1133, 0.1006, 0.1206, 0.1055, 0.1069, 0.1074, 0.1128, 0.1021, 0.1089,\n",
      "        0.1050, 0.1152, 0.1001, 0.1021, 0.1128, 0.1143, 0.1074, 0.0991, 0.0933,\n",
      "        0.1035, 0.2334, 0.1138, 0.0991, 0.1172, 0.1069, 0.1099, 0.1011, 0.1172,\n",
      "        0.1079, 0.1406, 0.1089, 0.1079, 0.1035, 0.1035, 0.1055, 0.1128, 0.1021,\n",
      "        0.1216, 0.1025, 0.1006, 0.1147, 0.1099, 0.1099, 0.1118, 0.1147, 0.1167,\n",
      "        0.1064, 0.0391, 0.1104, 0.1001, 0.1045, 0.1133, 0.0977, 0.1074, 0.1138,\n",
      "        0.1045, 0.1108, 0.1021, 0.1191, 0.1099, 0.1104, 0.1011, 0.1152, 0.1167,\n",
      "        0.0947, 0.0952, 0.1079, 0.1050, 0.1108, 0.1187, 0.1030, 0.1143, 0.1084,\n",
      "        0.0947, 0.1055, 0.1040, 0.0996, 0.1055, 0.1050, 0.1006, 0.1138, 0.1226,\n",
      "        0.1079, 0.1025, 0.1030, 0.1060, 0.1055, 0.1040, 0.1011, 0.1025, 0.0981,\n",
      "        0.1055, 0.1187, 0.1191, 0.1260, 0.1016, 0.1089, 0.0933, 0.1270, 0.1221,\n",
      "        0.1128, 0.1113, 0.1152, 0.1094, 0.0913, 0.0972, 0.1011, 0.1074, 0.1050,\n",
      "        0.0977, 0.1035, 0.1016, 0.1089, 0.1187, 0.0933, 0.1016, 0.0933, 0.1069,\n",
      "        0.1167, 0.1006, 0.1123, 0.1172, 0.1118, 0.1040, 0.1157, 0.1089, 0.0952,\n",
      "        0.1011, 0.1016, 0.1211, 0.1025, 0.1006, 0.0957, 0.1104, 0.1040, 0.1084,\n",
      "        0.1074, 0.1016, 0.0957, 0.1030, 0.1108, 0.1064, 0.1094, 0.1152, 0.1050,\n",
      "        0.0972, 0.0991, 0.1133, 0.1128, 0.1187, 0.1157, 0.1177, 0.1099, 0.0952,\n",
      "        0.1113, 0.1162, 0.0986, 0.1035, 0.1006, 0.1128, 0.1001, 0.1201, 0.0986,\n",
      "        0.1157, 0.0957, 0.1040, 0.0986, 0.1089, 0.1162, 0.0986, 0.1167, 0.1118,\n",
      "        0.1055, 0.1138, 0.1162, 0.1016, 0.1167, 0.1030, 0.1118, 0.0835, 0.1167,\n",
      "        0.0918, 0.1123, 0.1060, 0.1152, 0.1128, 0.1089, 0.1187, 0.1133, 0.1118,\n",
      "        0.1006, 0.1030, 0.0918, 0.1050, 0.1064, 0.1128, 0.0879, 0.1030, 0.1196,\n",
      "        0.1143, 0.1064, 0.1387, 0.1177, 0.1118, 0.1128, 0.1006, 0.1006, 0.1172,\n",
      "        0.1108, 0.1011, 0.1167, 0.1040, 0.0986, 0.1138, 0.1099, 0.0947, 0.1123,\n",
      "        0.0806, 0.1187, 0.0986, 0.1069, 0.1206, 0.0981, 0.1328, 0.1079, 0.1113,\n",
      "        0.1143, 0.1021, 0.1187, 0.1040, 0.1099, 0.1040, 0.1074, 0.1016, 0.1128,\n",
      "        0.1016, 0.1079, 0.1006, 0.1138, 0.1094, 0.1099, 0.0977, 0.1069, 0.1138,\n",
      "        0.1104, 0.1104, 0.1089, 0.0986, 0.0986, 0.1064, 0.0981, 0.0454, 0.1123,\n",
      "        0.1104, 0.1426, 0.1147, 0.1226, 0.1182, 0.0918, 0.1099, 0.0894, 0.0977,\n",
      "        0.1021, 0.1104, 0.1206, 0.1060, 0.1006, 0.1123, 0.0928, 0.1211, 0.1240,\n",
      "        0.1260, 0.1133, 0.1001, 0.1201, 0.0947, 0.1123, 0.1094, 0.1216, 0.1040,\n",
      "        0.1094, 0.1084, 0.0986, 0.1099, 0.0938, 0.0991, 0.1182, 0.1030, 0.1040,\n",
      "        0.1089, 0.1089, 0.1089, 0.1079, 0.1055, 0.1157, 0.1133, 0.0554, 0.1074,\n",
      "        0.0466, 0.1021, 0.1074, 0.1128, 0.1040, 0.1133, 0.1060, 0.1196, 0.1021,\n",
      "        0.0962, 0.1006, 0.1030, 0.1099, 0.1084, 0.1021, 0.0981, 0.1118, 0.1182,\n",
      "        0.1011, 0.1128, 0.1118, 0.0962, 0.1113, 0.1128, 0.1040, 0.0889, 0.0977,\n",
      "        0.1030, 0.1006, 0.1279, 0.1162, 0.1089, 0.1011, 0.1099, 0.0981, 0.1108,\n",
      "        0.0933, 0.1084, 0.1108, 0.1084, 0.1123, 0.1011, 0.1089, 0.1099, 0.1021,\n",
      "        0.1021, 0.1084, 0.1123, 0.0933, 0.0957, 0.0938, 0.1152, 0.1006, 0.0859,\n",
      "        0.1128, 0.1201, 0.1011, 0.1069, 0.0957, 0.1016, 0.1016, 0.1230, 0.1074,\n",
      "        0.1108, 0.0972, 0.1094, 0.1099, 0.1050, 0.1221, 0.1226, 0.1011, 0.1177,\n",
      "        0.1113, 0.1143, 0.1079, 0.1147, 0.1250, 0.1030, 0.1055, 0.1025, 0.1162,\n",
      "        0.1216, 0.1084, 0.1113, 0.1084, 0.1128, 0.1089, 0.1318, 0.1040, 0.1138,\n",
      "        0.1133, 0.1006, 0.1060, 0.1216, 0.0981, 0.1001, 0.1016, 0.1973],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0096,  0.0092,  0.0007,  ..., -0.0275, -0.0410,  0.0791],\n",
      "        [ 0.0403,  0.0400, -0.0430,  ...,  0.0093,  0.0273, -0.0854],\n",
      "        [ 0.0197, -0.0044, -0.0223,  ..., -0.0061,  0.0530,  0.0055],\n",
      "        ...,\n",
      "        [ 0.0645, -0.0145, -0.0140,  ..., -0.0410,  0.0425, -0.0454],\n",
      "        [ 0.0525,  0.0250, -0.0166,  ...,  0.0100,  0.0302, -0.0281],\n",
      "        [-0.0006, -0.0165, -0.0150,  ..., -0.0342,  0.0130,  0.0574]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3652,  0.1016, -0.1318,  ..., -0.2617,  0.1245,  0.1348],\n",
      "        [-0.2471,  0.1709, -0.0398,  ..., -0.1895, -0.1475, -0.4238],\n",
      "        [ 0.0060, -0.0806,  0.4395,  ...,  0.3848, -0.0337, -0.0815],\n",
      "        ...,\n",
      "        [-0.2500, -0.0225,  0.2236,  ..., -0.2754,  0.0178, -0.3398],\n",
      "        [-0.2793, -0.5234, -0.3672,  ...,  0.1138, -0.1670, -0.3125],\n",
      "        [ 0.0942, -0.2988,  0.1309,  ...,  0.7188,  0.9844, -0.3945]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0000,  0.3125,  1.0234,  ...,  0.4941, -0.1924, -0.9102],\n",
      "        [ 0.1455,  0.3340,  0.4414,  ..., -0.2080, -1.2266, -0.6406],\n",
      "        [ 0.5938,  0.0310,  0.4434,  ..., -0.3730,  1.1953, -0.2559],\n",
      "        ...,\n",
      "        [ 0.5508,  0.3848, -0.2285,  ...,  0.2432,  0.3496,  0.7539],\n",
      "        [-0.4453,  0.6602,  0.8867,  ...,  0.4043,  0.2910, -0.1709],\n",
      "        [ 0.5898, -1.1094, -0.0415,  ...,  0.9609, -0.3359, -0.3242]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8633, -0.7773, -1.1016,  ..., -0.9844, -0.1738,  0.1309],\n",
      "        [-1.0000,  0.0121,  0.3340,  ...,  0.0796, -1.0234,  0.6680],\n",
      "        [ 0.4023, -0.6406, -0.4141,  ..., -0.8984, -0.3652, -0.1367],\n",
      "        ...,\n",
      "        [-0.3691,  0.7812, -0.6367,  ...,  0.1309, -0.3262,  0.2021],\n",
      "        [ 0.6953,  0.4219,  0.5195,  ..., -0.6016,  0.0254,  0.1738],\n",
      "        [ 1.1250,  0.1914, -0.2676,  ..., -0.5742,  0.2734, -0.2158]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1865, 0.1729, 0.1611, 0.1680, 0.1621, 0.1670, 0.0398, 0.1729, 0.1475,\n",
      "        0.1758, 0.2070, 0.0850, 0.1670, 0.1406, 0.1631, 0.2158, 0.1699, 0.1582,\n",
      "        0.1719, 0.1611, 0.1445, 0.1318, 0.1699, 0.1953, 0.1768, 0.1592, 0.1533,\n",
      "        0.1738, 0.1621, 0.1553, 0.1738, 0.1406, 0.2109, 0.1621, 0.1670, 0.1875,\n",
      "        0.1738, 0.1729, 0.1816, 0.1406, 0.1650, 0.1475, 0.1729, 0.1514, 0.1729,\n",
      "        0.1729, 0.1660, 0.1973, 0.1631, 0.1836, 0.1680, 0.1709, 0.1553, 0.1924,\n",
      "        0.1660, 0.1836, 0.1523, 0.1670, 0.1582, 0.1416, 0.1494, 0.1670, 0.1689,\n",
      "        0.1641, 0.1904, 0.1885, 0.1533, 0.1592, 0.1748, 0.1514, 0.1660, 0.1592,\n",
      "        0.1895, 0.1631, 0.1309, 0.1631, 0.1738, 0.1709, 0.0635, 0.1768, 0.1699,\n",
      "        0.1602, 0.1885, 0.2002, 0.1768, 0.1855, 0.1484, 0.1699, 0.1758, 0.1543,\n",
      "        0.1426, 0.1621, 0.1758, 0.1748, 0.1963, 0.1807, 0.1641, 0.1592, 0.1582,\n",
      "        0.1543, 0.1680, 0.1660, 0.1699, 0.1689, 0.1934, 0.1455, 0.1816, 0.1650,\n",
      "        0.1416, 0.1572, 0.1523, 0.1523, 0.1611, 0.1631, 0.1533, 0.1709, 0.1523,\n",
      "        0.1592, 0.1855, 0.1953, 0.1592, 0.1738, 0.1484, 0.1729, 0.1611, 0.1494,\n",
      "        0.1611, 0.1885, 0.1689, 0.1611, 0.1426, 0.1543, 0.1553, 0.1436, 0.1553,\n",
      "        0.1680, 0.0115, 0.1846, 0.1650, 0.1846, 0.1631, 0.1729, 0.1582, 0.1738,\n",
      "        0.1807, 0.2021, 0.1484, 0.1494, 0.1572, 0.1670, 0.1650, 0.1885, 0.1582,\n",
      "        0.1299, 0.1445, 0.1602, 0.1494, 0.1641, 0.1689, 0.1611, 0.1729, 0.1533,\n",
      "        0.1553, 0.0378, 0.1592, 0.1758, 0.1748, 0.1797, 0.1465, 0.1729, 0.1768,\n",
      "        0.1533, 0.1631, 0.1689, 0.1621, 0.1670, 0.1738, 0.1494, 0.1641, 0.1660,\n",
      "        0.1299, 0.1455, 0.1445, 0.1758, 0.1660, 0.1631, 0.1514, 0.1787, 0.1885,\n",
      "        0.1514, 0.1787, 0.0894, 0.1494, 0.1895, 0.1484, 0.1533, 0.1826, 0.1904,\n",
      "        0.1709, 0.1396, 0.1562, 0.1494, 0.1621, 0.1572, 0.1807, 0.1602, 0.1660,\n",
      "        0.1494, 0.1709, 0.1494, 0.1729, 0.1611, 0.1602, 0.1611, 0.1729, 0.1660,\n",
      "        0.1797, 0.1836, 0.1650, 0.1719, 0.1514, 0.1562, 0.1514, 0.1543, 0.1836,\n",
      "        0.1562, 0.1904, 0.1719, 0.1504, 0.1631, 0.1445, 0.1582, 0.1504, 0.1553,\n",
      "        0.1582, 0.1436, 0.1943, 0.1611, 0.1904, 0.1582, 0.1758, 0.1689, 0.1592,\n",
      "        0.1582, 0.1416, 0.1855, 0.1602, 0.1582, 0.1572, 0.1650, 0.1758, 0.1572,\n",
      "        0.1572, 0.1641, 0.1504, 0.1670, 0.1660, 0.1592, 0.1855, 0.1846, 0.1748,\n",
      "        0.1602, 0.1582, 0.1670, 0.1797, 0.1885, 0.1836, 0.1846, 0.1689, 0.1504,\n",
      "        0.1641, 0.1611, 0.1719, 0.1475, 0.1699, 0.1602, 0.1660, 0.1846, 0.1338,\n",
      "        0.1699, 0.1504, 0.1797, 0.1543, 0.1572, 0.1611, 0.1807, 0.1709, 0.1709,\n",
      "        0.1816, 0.1387, 0.1924, 0.1631, 0.1670, 0.1680, 0.1533, 0.1787, 0.1719,\n",
      "        0.1553, 0.1611, 0.1582, 0.1699, 0.1592, 0.1641, 0.1855, 0.1494, 0.1699,\n",
      "        0.1650, 0.1465, 0.1670, 0.1533, 0.1631, 0.1826, 0.1523, 0.1445, 0.1807,\n",
      "        0.1836, 0.1719, 0.1885, 0.1572, 0.1719, 0.1670, 0.1670, 0.1680, 0.1816,\n",
      "        0.1602, 0.1729, 0.1836, 0.1641, 0.1572, 0.1592, 0.1328, 0.1621, 0.1855,\n",
      "        0.1064, 0.1543, 0.1758, 0.1758, 0.1885, 0.1338, 0.1885, 0.1533, 0.1826,\n",
      "        0.1680, 0.1396, 0.1992, 0.1504, 0.1758, 0.1787, 0.1602, 0.1494, 0.1504,\n",
      "        0.1660, 0.1504, 0.1611, 0.1641, 0.1768, 0.1641, 0.1455, 0.1758, 0.1592,\n",
      "        0.1641, 0.1807, 0.1553, 0.1611, 0.1543, 0.1680, 0.1436, 0.0471, 0.1963,\n",
      "        0.2090, 0.0942, 0.1865, 0.1758, 0.1602, 0.1367, 0.1631, 0.1484, 0.1396,\n",
      "        0.1562, 0.1592, 0.1836, 0.1641, 0.1699, 0.1729, 0.1465, 0.1768, 0.1836,\n",
      "        0.1602, 0.1729, 0.1650, 0.1865, 0.1416, 0.1846, 0.1543, 0.1855, 0.1465,\n",
      "        0.1680, 0.1680, 0.1504, 0.1680, 0.1592, 0.1641, 0.1748, 0.1670, 0.1680,\n",
      "        0.1797, 0.1553, 0.1748, 0.1523, 0.1650, 0.1631, 0.1660, 0.0737, 0.1748,\n",
      "        0.0718, 0.1514, 0.1572, 0.1758, 0.1592, 0.1631, 0.1787, 0.1562, 0.1719,\n",
      "        0.1621, 0.1709, 0.1572, 0.1719, 0.1855, 0.1602, 0.1582, 0.1650, 0.1768,\n",
      "        0.1445, 0.1670, 0.1328, 0.1572, 0.1455, 0.1855, 0.1670, 0.1475, 0.1768,\n",
      "        0.1572, 0.1680, 0.1533, 0.1650, 0.1650, 0.1367, 0.2158, 0.1455, 0.1797,\n",
      "        0.1572, 0.1611, 0.1650, 0.1641, 0.1621, 0.1465, 0.1670, 0.1699, 0.1953,\n",
      "        0.1416, 0.1602, 0.1729, 0.1436, 0.1602, 0.1406, 0.1807, 0.1689, 0.1436,\n",
      "        0.1729, 0.1797, 0.1621, 0.1650, 0.1562, 0.1533, 0.1807, 0.1699, 0.1572,\n",
      "        0.1699, 0.1689, 0.1729, 0.1572, 0.1641, 0.1426, 0.1709, 0.1572, 0.1738,\n",
      "        0.1553, 0.1670, 0.1641, 0.1982, 0.1650, 0.1553, 0.1611, 0.1865, 0.1729,\n",
      "        0.1768, 0.1670, 0.2188, 0.1699, 0.1729, 0.1631, 0.1875, 0.1738, 0.1758,\n",
      "        0.1836, 0.1504, 0.1562, 0.1475, 0.1621, 0.1475, 0.1445, 0.1357],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1592, -0.2275,  0.0111,  ...,  0.2871, -0.2090,  0.0493],\n",
      "        [ 0.0742, -0.0063,  0.2617,  ...,  0.2070,  0.0503,  0.0383],\n",
      "        [-0.4941, -0.1055,  0.0413,  ..., -0.4297, -0.2812, -0.1543],\n",
      "        ...,\n",
      "        [ 0.0481, -0.2246,  0.2617,  ...,  0.0136,  0.1465, -0.2227],\n",
      "        [-0.3184, -0.3984, -0.4512,  ...,  0.0244, -0.0835, -0.2432],\n",
      "        [-0.3340, -0.0410,  0.0830,  ..., -0.2871, -0.3945, -0.1748]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1709,  0.8750,  2.0625,  ..., -0.2178, -0.7578, -0.8828],\n",
      "        [-0.7852, -0.4219, -0.9531,  ..., -0.2100,  0.4316,  0.2520],\n",
      "        [-0.2324,  0.8359,  1.1406,  ...,  0.0791,  1.6016, -0.7617],\n",
      "        ...,\n",
      "        [-2.0156,  0.6250, -0.4688,  ..., -0.3184, -0.3398, -0.5195],\n",
      "        [-1.6562,  0.4414,  0.1641,  ..., -0.3770,  0.6094, -1.8594],\n",
      "        [-0.6289,  0.1060,  0.5898,  ..., -0.8555, -2.4531,  0.5547]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.6836, -0.3867,  0.3125,  ..., -0.5195,  0.5273, -0.6797],\n",
      "        [-0.3027,  0.6758,  0.0437,  ..., -0.4414, -0.9375, -0.5977],\n",
      "        [-0.2100,  0.2031, -0.2930,  ..., -1.4062, -0.1494,  0.1211],\n",
      "        ...,\n",
      "        [-0.9688,  0.7969,  0.4785,  ...,  0.0679, -0.1416, -0.4707],\n",
      "        [ 0.1426, -0.1797,  0.2539,  ..., -1.3984,  0.6758, -0.0864],\n",
      "        [ 0.3594, -0.5781,  0.2695,  ...,  0.9297,  0.9023,  0.0618]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1250, 0.1216, 0.1133, 0.1201, 0.1196, 0.1079, 0.0669, 0.1143, 0.1143,\n",
      "        0.1201, 0.1279, 0.0688, 0.1152, 0.0972, 0.1235, 0.1377, 0.1064, 0.1089,\n",
      "        0.1167, 0.1094, 0.1040, 0.0928, 0.1094, 0.1079, 0.1177, 0.1138, 0.1099,\n",
      "        0.1123, 0.1162, 0.1079, 0.1226, 0.1177, 0.2285, 0.1138, 0.1118, 0.1152,\n",
      "        0.0981, 0.1167, 0.1299, 0.1021, 0.1147, 0.1138, 0.1152, 0.1079, 0.1108,\n",
      "        0.1206, 0.1113, 0.1187, 0.1143, 0.1177, 0.1099, 0.1279, 0.1128, 0.1216,\n",
      "        0.1104, 0.1152, 0.1128, 0.1089, 0.1152, 0.1045, 0.1167, 0.1123, 0.1157,\n",
      "        0.1089, 0.1309, 0.1040, 0.1060, 0.1113, 0.1118, 0.1084, 0.1318, 0.1025,\n",
      "        0.1069, 0.1069, 0.1108, 0.1113, 0.1050, 0.1177, 0.0693, 0.1201, 0.1172,\n",
      "        0.1172, 0.1182, 0.1196, 0.1279, 0.1133, 0.1216, 0.1216, 0.1426, 0.1318,\n",
      "        0.1157, 0.1040, 0.1191, 0.1182, 0.1206, 0.1216, 0.1226, 0.1191, 0.1094,\n",
      "        0.1060, 0.1045, 0.1162, 0.1245, 0.1128, 0.1216, 0.1108, 0.1216, 0.0991,\n",
      "        0.1113, 0.1118, 0.1079, 0.1157, 0.1147, 0.1089, 0.1162, 0.1187, 0.1060,\n",
      "        0.1162, 0.1069, 0.1104, 0.1035, 0.1133, 0.1167, 0.1104, 0.1045, 0.1084,\n",
      "        0.1099, 0.1279, 0.1123, 0.1235, 0.1260, 0.1079, 0.1089, 0.1050, 0.1143,\n",
      "        0.1089, 0.6484, 0.1221, 0.1138, 0.1128, 0.1123, 0.1133, 0.1113, 0.1079,\n",
      "        0.1118, 0.1357, 0.1177, 0.1113, 0.1060, 0.1201, 0.1050, 0.1162, 0.1143,\n",
      "        0.1250, 0.1094, 0.1074, 0.1143, 0.1055, 0.1157, 0.1147, 0.1289, 0.1250,\n",
      "        0.1074, 0.0317, 0.1108, 0.1162, 0.1118, 0.1260, 0.1099, 0.1162, 0.1289,\n",
      "        0.0991, 0.1289, 0.1030, 0.1167, 0.1069, 0.1138, 0.1055, 0.1157, 0.1157,\n",
      "        0.1045, 0.1157, 0.1191, 0.1113, 0.1172, 0.1143, 0.0972, 0.1226, 0.1035,\n",
      "        0.1050, 0.1187, 0.0796, 0.0977, 0.1206, 0.1157, 0.1099, 0.1230, 0.1113,\n",
      "        0.1230, 0.1123, 0.1040, 0.1147, 0.1113, 0.1152, 0.1235, 0.1182, 0.1064,\n",
      "        0.1128, 0.1157, 0.1104, 0.1177, 0.1030, 0.1064, 0.1011, 0.1318, 0.1182,\n",
      "        0.1270, 0.1328, 0.1279, 0.1133, 0.0977, 0.1099, 0.1079, 0.1143, 0.1118,\n",
      "        0.1094, 0.1099, 0.1084, 0.1123, 0.1147, 0.1050, 0.1069, 0.1040, 0.1030,\n",
      "        0.1138, 0.1016, 0.1250, 0.1187, 0.1147, 0.0986, 0.1201, 0.1104, 0.1069,\n",
      "        0.1006, 0.1074, 0.1279, 0.1021, 0.1133, 0.1250, 0.1191, 0.1309, 0.1079,\n",
      "        0.1128, 0.1113, 0.1001, 0.1104, 0.1167, 0.1099, 0.1152, 0.1167, 0.1108,\n",
      "        0.1045, 0.1079, 0.1104, 0.1152, 0.1201, 0.1211, 0.1143, 0.1226, 0.1108,\n",
      "        0.1128, 0.1118, 0.1118, 0.1118, 0.1055, 0.1118, 0.1074, 0.1157, 0.0967,\n",
      "        0.1240, 0.0996, 0.1143, 0.1128, 0.1182, 0.1089, 0.1035, 0.1104, 0.1060,\n",
      "        0.1123, 0.1138, 0.1177, 0.1157, 0.1108, 0.1118, 0.1216, 0.1064, 0.1230,\n",
      "        0.1138, 0.1182, 0.1064, 0.1187, 0.1133, 0.1177, 0.1245, 0.1025, 0.1270,\n",
      "        0.1133, 0.1060, 0.1079, 0.1084, 0.1074, 0.1206, 0.1006, 0.1089, 0.1206,\n",
      "        0.1162, 0.1011, 0.1348, 0.1240, 0.1191, 0.1050, 0.1128, 0.1064, 0.1196,\n",
      "        0.1270, 0.1162, 0.1182, 0.1108, 0.1196, 0.1118, 0.1079, 0.1118, 0.1138,\n",
      "        0.0947, 0.1069, 0.1050, 0.1201, 0.1260, 0.1021, 0.1553, 0.1157, 0.1230,\n",
      "        0.1172, 0.1074, 0.1187, 0.1079, 0.1138, 0.1187, 0.1221, 0.1152, 0.1099,\n",
      "        0.1128, 0.1226, 0.1118, 0.1187, 0.1147, 0.1152, 0.1084, 0.1118, 0.1299,\n",
      "        0.1172, 0.1182, 0.1050, 0.1079, 0.1108, 0.1143, 0.1143, 0.0396, 0.1162,\n",
      "        0.1445, 0.1631, 0.1128, 0.1138, 0.1104, 0.1064, 0.1152, 0.1011, 0.1045,\n",
      "        0.1079, 0.1089, 0.1108, 0.1069, 0.1074, 0.1167, 0.1123, 0.1230, 0.1157,\n",
      "        0.1279, 0.1152, 0.1108, 0.1260, 0.1118, 0.1040, 0.1128, 0.1221, 0.1040,\n",
      "        0.1167, 0.1084, 0.1006, 0.1113, 0.1050, 0.1133, 0.1147, 0.1167, 0.1196,\n",
      "        0.1157, 0.1074, 0.1118, 0.1104, 0.1182, 0.1240, 0.1147, 0.0645, 0.1104,\n",
      "        0.0532, 0.1147, 0.1123, 0.1240, 0.1157, 0.1147, 0.1167, 0.1172, 0.1079,\n",
      "        0.1016, 0.1143, 0.1064, 0.1157, 0.1162, 0.1152, 0.1123, 0.1196, 0.1152,\n",
      "        0.1069, 0.1104, 0.1147, 0.1011, 0.1182, 0.1104, 0.1133, 0.1030, 0.1074,\n",
      "        0.1108, 0.1099, 0.1074, 0.1143, 0.1113, 0.1064, 0.1357, 0.1133, 0.1250,\n",
      "        0.1025, 0.1006, 0.1128, 0.1172, 0.1113, 0.1001, 0.1216, 0.1108, 0.1191,\n",
      "        0.1157, 0.1089, 0.1133, 0.1016, 0.1084, 0.0972, 0.1235, 0.1050, 0.1167,\n",
      "        0.1167, 0.1289, 0.1055, 0.1118, 0.1182, 0.1113, 0.1123, 0.1167, 0.1152,\n",
      "        0.1201, 0.1167, 0.1050, 0.1118, 0.1157, 0.1074, 0.1201, 0.1089, 0.1143,\n",
      "        0.1187, 0.1079, 0.1089, 0.1094, 0.1211, 0.1157, 0.1133, 0.1250, 0.1118,\n",
      "        0.1289, 0.1147, 0.1201, 0.1187, 0.1089, 0.1133, 0.1348, 0.1069, 0.1177,\n",
      "        0.1279, 0.1157, 0.1069, 0.1318, 0.1025, 0.1006, 0.1079, 0.4043],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0053,  0.0153, -0.0056,  ...,  0.0104, -0.0272,  0.0447],\n",
      "        [ 0.0161,  0.0422, -0.0012,  ..., -0.0067,  0.0091, -0.0175],\n",
      "        [ 0.0349,  0.0114,  0.0002,  ..., -0.0121, -0.0469, -0.0222],\n",
      "        ...,\n",
      "        [-0.0132, -0.0010,  0.0106,  ..., -0.0144,  0.0219,  0.0265],\n",
      "        [ 0.0100,  0.0143,  0.0115,  ...,  0.0077,  0.0327,  0.0142],\n",
      "        [ 0.0177,  0.0271, -0.0070,  ...,  0.0161, -0.0354,  0.0354]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0166,  0.1216,  0.2617,  ..., -0.1113,  0.2441,  0.0938],\n",
      "        [ 0.0344, -0.0332, -0.1484,  ...,  0.0708,  0.0889, -0.1660],\n",
      "        [ 0.2393,  0.2949, -0.0645,  ..., -0.0747,  0.1030,  0.4512],\n",
      "        ...,\n",
      "        [ 0.1973,  0.0221, -0.0640,  ...,  0.0928,  0.0090,  0.0089],\n",
      "        [-0.2334,  0.0498, -0.2754,  ..., -0.1494,  0.0481,  0.0588],\n",
      "        [ 0.1494, -0.0640, -0.1338,  ...,  0.0383, -0.2598,  0.2930]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.8711, -0.0344,  1.0312,  ...,  0.8633, -1.0938,  0.7227],\n",
      "        [-0.3633, -0.5000, -1.0312,  ..., -0.8320, -0.6562, -1.1328],\n",
      "        [ 1.1953, -0.6016,  0.9023,  ..., -0.4375, -0.0957,  1.1719],\n",
      "        ...,\n",
      "        [ 0.0063, -0.1230, -1.0078,  ...,  0.5352,  0.0254,  1.4688],\n",
      "        [ 1.2422, -0.4277, -1.9297,  ...,  0.2637, -0.9062,  0.7227],\n",
      "        [ 0.8633,  0.8242,  0.1416,  ...,  0.3477,  1.3750,  0.4512]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5078,  0.4531, -0.9180,  ...,  0.1328, -0.4668,  0.3633],\n",
      "        [ 0.5859, -0.5625,  0.3809,  ..., -1.7500, -1.7891, -2.3281],\n",
      "        [ 0.7070,  1.1094, -0.3926,  ...,  1.2031, -0.3320,  0.8164],\n",
      "        ...,\n",
      "        [ 1.0703, -0.9336,  0.0869,  ..., -0.5508, -0.0601, -0.1973],\n",
      "        [ 0.2090,  0.4688, -0.7227,  ..., -0.1504, -0.3438,  1.5859],\n",
      "        [ 0.4551, -0.2578,  0.1406,  ...,  0.3359, -0.9023, -0.2295]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2002, 0.1807, 0.1836, 0.1719, 0.1836, 0.1807, 0.0251, 0.1982, 0.1924,\n",
      "        0.1943, 0.2236, 0.0933, 0.2070, 0.1699, 0.1787, 0.2158, 0.1895, 0.1895,\n",
      "        0.1748, 0.1934, 0.1660, 0.1504, 0.1934, 0.1953, 0.1924, 0.1797, 0.1758,\n",
      "        0.1885, 0.1904, 0.1943, 0.1758, 0.1533, 0.1797, 0.1787, 0.1826, 0.1963,\n",
      "        0.1846, 0.1924, 0.1973, 0.1738, 0.2031, 0.1768, 0.1963, 0.1709, 0.1748,\n",
      "        0.1836, 0.1865, 0.2100, 0.1914, 0.1924, 0.1953, 0.1768, 0.1914, 0.2236,\n",
      "        0.1738, 0.2041, 0.1777, 0.1826, 0.1904, 0.1650, 0.1846, 0.2031, 0.2100,\n",
      "        0.1797, 0.1670, 0.2109, 0.2061, 0.1670, 0.1699, 0.1875, 0.1611, 0.1670,\n",
      "        0.1924, 0.1846, 0.1436, 0.1885, 0.1924, 0.1836, 0.0253, 0.2051, 0.2002,\n",
      "        0.1875, 0.2002, 0.2246, 0.2031, 0.1943, 0.1738, 0.2051, 0.2217, 0.1523,\n",
      "        0.1641, 0.1758, 0.1943, 0.1836, 0.1934, 0.2266, 0.1768, 0.1982, 0.1650,\n",
      "        0.2041, 0.1826, 0.1885, 0.1914, 0.1846, 0.2129, 0.1787, 0.2031, 0.1865,\n",
      "        0.1816, 0.1641, 0.1777, 0.1787, 0.1553, 0.1787, 0.1787, 0.1924, 0.1699,\n",
      "        0.1816, 0.1953, 0.2002, 0.1807, 0.2246, 0.1680, 0.1807, 0.1719, 0.1660,\n",
      "        0.1855, 0.2285, 0.1846, 0.1904, 0.1738, 0.1602, 0.1748, 0.1797, 0.1934,\n",
      "        0.1855, 0.0400, 0.2148, 0.2061, 0.1963, 0.1738, 0.2090, 0.1934, 0.1777,\n",
      "        0.2061, 0.2207, 0.1816, 0.1699, 0.1934, 0.1807, 0.1748, 0.2246, 0.2012,\n",
      "        0.1553, 0.1865, 0.1807, 0.1572, 0.1875, 0.1846, 0.1670, 0.1963, 0.1523,\n",
      "        0.1777, 0.0635, 0.1768, 0.1738, 0.2002, 0.2334, 0.1523, 0.1914, 0.2139,\n",
      "        0.1748, 0.1895, 0.1934, 0.1797, 0.1953, 0.1895, 0.1895, 0.1846, 0.1816,\n",
      "        0.1582, 0.1846, 0.1758, 0.2139, 0.1953, 0.1953, 0.1182, 0.2168, 0.2031,\n",
      "        0.1719, 0.1973, 0.0801, 0.1826, 0.2041, 0.1758, 0.1895, 0.1895, 0.1934,\n",
      "        0.1904, 0.1729, 0.1777, 0.1865, 0.2021, 0.1816, 0.1953, 0.1836, 0.1895,\n",
      "        0.1816, 0.1836, 0.1533, 0.1963, 0.1826, 0.1885, 0.1943, 0.2129, 0.1514,\n",
      "        0.2012, 0.1895, 0.2041, 0.1963, 0.1748, 0.1689, 0.1777, 0.1777, 0.2334,\n",
      "        0.1904, 0.2012, 0.1895, 0.1924, 0.1875, 0.1914, 0.1875, 0.1680, 0.1572,\n",
      "        0.1660, 0.1768, 0.2041, 0.1797, 0.1709, 0.1807, 0.2061, 0.1934, 0.1992,\n",
      "        0.1748, 0.1602, 0.1768, 0.2021, 0.1855, 0.1807, 0.1865, 0.1963, 0.1777,\n",
      "        0.1826, 0.2021, 0.1768, 0.2070, 0.1758, 0.1875, 0.1924, 0.2129, 0.2090,\n",
      "        0.1855, 0.1758, 0.1777, 0.2041, 0.2061, 0.2070, 0.1885, 0.2021, 0.1777,\n",
      "        0.1719, 0.1963, 0.1963, 0.1709, 0.1865, 0.1807, 0.1904, 0.1768, 0.1611,\n",
      "        0.1982, 0.1758, 0.2051, 0.1748, 0.1582, 0.1777, 0.2021, 0.1748, 0.1807,\n",
      "        0.1982, 0.1611, 0.1846, 0.1953, 0.1729, 0.1973, 0.1826, 0.2051, 0.2070,\n",
      "        0.1816, 0.1934, 0.1768, 0.1924, 0.1895, 0.1895, 0.1914, 0.1650, 0.1816,\n",
      "        0.1797, 0.1729, 0.2031, 0.1641, 0.1855, 0.2002, 0.1729, 0.1816, 0.1973,\n",
      "        0.2119, 0.1807, 0.2100, 0.1689, 0.2051, 0.1797, 0.1836, 0.1777, 0.1982,\n",
      "        0.2178, 0.2100, 0.2061, 0.2090, 0.1680, 0.1855, 0.1641, 0.1934, 0.1846,\n",
      "        0.1289, 0.1709, 0.2178, 0.2275, 0.2207, 0.1680, 0.2080, 0.1797, 0.2070,\n",
      "        0.2109, 0.1680, 0.2061, 0.1807, 0.2158, 0.2031, 0.1943, 0.1748, 0.1934,\n",
      "        0.2070, 0.1895, 0.1807, 0.1943, 0.1982, 0.1787, 0.1846, 0.1816, 0.1807,\n",
      "        0.1953, 0.2158, 0.1777, 0.1660, 0.1865, 0.1943, 0.1768, 0.0503, 0.2217,\n",
      "        0.2041, 0.0091, 0.2148, 0.1895, 0.1865, 0.1816, 0.1943, 0.1699, 0.1621,\n",
      "        0.1582, 0.1758, 0.1953, 0.1758, 0.1982, 0.1904, 0.1836, 0.1865, 0.2002,\n",
      "        0.1826, 0.2168, 0.1895, 0.2021, 0.1895, 0.2148, 0.1934, 0.1963, 0.1797,\n",
      "        0.1904, 0.1797, 0.1865, 0.1719, 0.1748, 0.1953, 0.1826, 0.1768, 0.1719,\n",
      "        0.2012, 0.1768, 0.1924, 0.1709, 0.1865, 0.1924, 0.1846, 0.0737, 0.1914,\n",
      "        0.0786, 0.1699, 0.1650, 0.2041, 0.1855, 0.1895, 0.1914, 0.1289, 0.1992,\n",
      "        0.1914, 0.2070, 0.1797, 0.1924, 0.1914, 0.1719, 0.1846, 0.1924, 0.1826,\n",
      "        0.1592, 0.1768, 0.1455, 0.1787, 0.1709, 0.1875, 0.1924, 0.1699, 0.1904,\n",
      "        0.1797, 0.1992, 0.1562, 0.1895, 0.1953, 0.1768, 0.2480, 0.1914, 0.2051,\n",
      "        0.1729, 0.1602, 0.2100, 0.1982, 0.1523, 0.1729, 0.2100, 0.2197, 0.2373,\n",
      "        0.1680, 0.1855, 0.1992, 0.1670, 0.1973, 0.1641, 0.1973, 0.1953, 0.1924,\n",
      "        0.2021, 0.2070, 0.1963, 0.1699, 0.1875, 0.1777, 0.2021, 0.2061, 0.1719,\n",
      "        0.1885, 0.2041, 0.1797, 0.1719, 0.1865, 0.1514, 0.1895, 0.1836, 0.1582,\n",
      "        0.1875, 0.2041, 0.1709, 0.1973, 0.1982, 0.1719, 0.1826, 0.2236, 0.1797,\n",
      "        0.1934, 0.1768, 0.2158, 0.1748, 0.2207, 0.1768, 0.2139, 0.2080, 0.1963,\n",
      "        0.1855, 0.1846, 0.1895, 0.1729, 0.1787, 0.1475, 0.1768, 0.0869],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2275, -0.0654,  0.0601,  ..., -0.0018,  0.0603,  0.1167],\n",
      "        [-0.1904, -0.1729,  0.4375,  ...,  0.3027,  0.1787,  0.3164],\n",
      "        [ 0.1064,  0.0408, -0.0918,  ...,  0.0311, -0.1602, -0.0894],\n",
      "        ...,\n",
      "        [-0.1270,  0.1055, -0.1572,  ...,  0.1118, -0.1387,  0.1533],\n",
      "        [ 0.0106, -0.2773, -0.3184,  ..., -0.1914,  0.2275,  0.1553],\n",
      "        [-0.0500, -0.0815,  0.7578,  ..., -0.2539,  0.6484,  0.1113]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1377,  0.6836,  1.3203,  ...,  1.0156, -0.7188, -0.6758],\n",
      "        [-0.9258,  0.1621, -1.2969,  ...,  1.8750,  0.1807, -0.7422],\n",
      "        [-0.1553, -0.0049, -0.6680,  ...,  0.9883,  2.3750,  0.5352],\n",
      "        ...,\n",
      "        [ 0.4688,  1.7031,  1.5391,  ..., -1.3828,  0.1533, -0.4180],\n",
      "        [-0.4199, -0.5859, -0.9414,  ...,  0.1904,  0.6016,  0.9375],\n",
      "        [ 0.6406,  0.2451, -0.4648,  ..., -2.2969,  2.3281, -0.4668]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-5.9375e-01, -3.0078e-01,  5.1172e-01,  ..., -4.1211e-01,\n",
      "          8.0859e-01,  7.6953e-01],\n",
      "        [-5.6641e-01, -2.3633e-01, -1.2344e+00,  ..., -6.8750e-01,\n",
      "          9.6680e-02, -2.0142e-02],\n",
      "        [-1.7090e-01,  2.1973e-01,  3.8672e-01,  ...,  1.0254e-01,\n",
      "         -2.2754e-01, -6.3672e-01],\n",
      "        ...,\n",
      "        [ 1.2578e+00,  3.4180e-01,  6.3281e-01,  ..., -4.2152e-04,\n",
      "         -7.0312e-01,  5.1172e-01],\n",
      "        [ 8.0566e-02, -2.1289e-01, -8.3594e-01,  ...,  4.3750e-01,\n",
      "          7.0312e-01, -3.4570e-01],\n",
      "        [-1.6797e-01,  4.8584e-02, -1.5234e-01,  ...,  3.4766e-01,\n",
      "         -5.5469e-01, -8.9355e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1084, 0.1050, 0.1035, 0.0928, 0.1094, 0.1025, 0.0708, 0.0986, 0.1084,\n",
      "        0.0991, 0.0928, 0.0708, 0.1074, 0.0967, 0.1094, 0.1357, 0.1055, 0.1055,\n",
      "        0.1206, 0.1079, 0.1060, 0.1123, 0.0991, 0.0903, 0.1357, 0.1299, 0.1191,\n",
      "        0.1167, 0.1001, 0.0986, 0.1060, 0.0952, 0.3613, 0.1143, 0.1021, 0.1147,\n",
      "        0.0981, 0.1196, 0.1094, 0.1001, 0.1094, 0.1050, 0.1118, 0.1050, 0.0962,\n",
      "        0.1104, 0.0996, 0.1064, 0.1055, 0.1006, 0.1143, 0.1050, 0.1040, 0.1011,\n",
      "        0.0933, 0.1001, 0.1016, 0.1001, 0.1074, 0.1079, 0.1021, 0.1035, 0.1084,\n",
      "        0.0903, 0.1030, 0.0957, 0.1191, 0.0977, 0.1045, 0.1104, 0.1147, 0.1050,\n",
      "        0.1094, 0.0933, 0.1055, 0.1055, 0.1060, 0.1123, 0.0840, 0.1123, 0.0947,\n",
      "        0.1177, 0.1016, 0.1099, 0.1152, 0.1006, 0.1064, 0.1133, 0.1318, 0.1016,\n",
      "        0.1006, 0.0938, 0.1123, 0.1050, 0.1050, 0.1230, 0.1064, 0.0947, 0.0996,\n",
      "        0.1089, 0.1055, 0.1099, 0.1079, 0.1011, 0.1245, 0.1025, 0.1162, 0.1011,\n",
      "        0.1064, 0.1079, 0.1074, 0.0986, 0.1079, 0.1016, 0.1001, 0.1069, 0.1074,\n",
      "        0.1021, 0.1021, 0.0981, 0.1045, 0.1240, 0.1074, 0.0977, 0.1035, 0.0981,\n",
      "        0.1035, 0.1182, 0.1084, 0.1216, 0.1089, 0.1006, 0.0952, 0.1108, 0.1074,\n",
      "        0.1025, 1.1406, 0.1118, 0.1074, 0.1006, 0.1094, 0.1113, 0.1001, 0.1006,\n",
      "        0.1099, 0.1182, 0.1064, 0.0977, 0.1006, 0.1104, 0.1084, 0.1069, 0.1011,\n",
      "        0.0938, 0.1050, 0.1011, 0.1045, 0.1021, 0.1084, 0.1001, 0.1196, 0.1030,\n",
      "        0.0933, 0.0535, 0.1074, 0.1021, 0.1016, 0.1030, 0.1011, 0.1006, 0.1035,\n",
      "        0.1035, 0.1182, 0.0986, 0.1143, 0.0957, 0.1152, 0.1021, 0.1060, 0.1011,\n",
      "        0.1055, 0.1260, 0.1104, 0.1064, 0.1162, 0.1196, 0.0737, 0.1128, 0.0991,\n",
      "        0.1040, 0.1152, 0.0625, 0.1016, 0.1069, 0.1006, 0.1069, 0.1118, 0.1025,\n",
      "        0.1250, 0.1006, 0.1074, 0.1084, 0.1113, 0.1099, 0.1216, 0.1069, 0.1094,\n",
      "        0.1079, 0.0991, 0.1025, 0.0986, 0.1079, 0.1079, 0.0991, 0.1167, 0.0991,\n",
      "        0.1367, 0.1270, 0.1157, 0.1030, 0.1060, 0.1055, 0.1040, 0.1016, 0.1191,\n",
      "        0.1055, 0.1030, 0.1060, 0.1084, 0.1025, 0.1030, 0.1074, 0.1006, 0.1016,\n",
      "        0.0947, 0.0942, 0.1099, 0.0977, 0.1035, 0.0952, 0.1270, 0.1045, 0.0972,\n",
      "        0.1089, 0.1030, 0.1108, 0.0938, 0.1030, 0.1226, 0.1006, 0.1338, 0.0986,\n",
      "        0.1069, 0.0933, 0.1055, 0.1079, 0.1099, 0.1069, 0.1055, 0.0977, 0.1079,\n",
      "        0.1064, 0.1128, 0.1025, 0.0947, 0.0996, 0.1055, 0.1006, 0.1250, 0.1006,\n",
      "        0.1016, 0.1108, 0.0977, 0.1060, 0.1001, 0.1133, 0.1030, 0.1079, 0.0928,\n",
      "        0.1113, 0.1021, 0.1104, 0.1060, 0.1006, 0.1011, 0.1055, 0.1055, 0.1040,\n",
      "        0.1104, 0.1030, 0.1016, 0.1021, 0.1050, 0.1025, 0.1060, 0.1426, 0.1143,\n",
      "        0.1079, 0.1079, 0.0903, 0.1074, 0.0898, 0.0972, 0.0977, 0.1094, 0.1104,\n",
      "        0.1016, 0.0933, 0.1289, 0.1001, 0.0972, 0.1060, 0.1084, 0.0923, 0.1084,\n",
      "        0.1001, 0.0913, 0.1226, 0.1143, 0.1108, 0.1001, 0.1035, 0.1016, 0.1021,\n",
      "        0.1035, 0.1069, 0.1045, 0.0981, 0.1079, 0.1069, 0.1074, 0.1064, 0.0981,\n",
      "        0.0952, 0.1050, 0.1104, 0.1172, 0.1147, 0.1025, 0.1572, 0.1108, 0.1025,\n",
      "        0.1089, 0.1050, 0.1045, 0.0938, 0.1123, 0.1157, 0.0952, 0.1157, 0.1045,\n",
      "        0.1035, 0.1289, 0.1113, 0.1094, 0.1099, 0.1060, 0.1060, 0.1016, 0.1484,\n",
      "        0.1104, 0.1123, 0.0923, 0.1069, 0.1104, 0.1055, 0.1104, 0.0505, 0.1133,\n",
      "        0.1235, 0.1953, 0.1094, 0.1089, 0.1001, 0.1006, 0.1084, 0.1030, 0.1084,\n",
      "        0.0967, 0.0986, 0.1030, 0.0918, 0.1064, 0.0972, 0.1084, 0.1152, 0.0986,\n",
      "        0.1060, 0.1074, 0.1064, 0.1187, 0.1035, 0.1084, 0.1108, 0.1064, 0.1074,\n",
      "        0.1040, 0.0928, 0.1079, 0.0996, 0.1250, 0.1143, 0.0991, 0.0967, 0.0972,\n",
      "        0.1025, 0.1089, 0.1089, 0.1074, 0.1055, 0.1021, 0.1040, 0.1123, 0.0991,\n",
      "        0.0566, 0.1025, 0.0972, 0.1040, 0.1055, 0.0957, 0.1089, 0.0898, 0.1138,\n",
      "        0.1167, 0.1172, 0.1069, 0.1113, 0.1069, 0.1196, 0.1099, 0.1079, 0.0962,\n",
      "        0.1011, 0.1006, 0.0967, 0.1133, 0.1001, 0.1118, 0.1094, 0.1045, 0.1055,\n",
      "        0.0977, 0.1089, 0.0786, 0.1069, 0.1006, 0.1084, 0.1245, 0.0967, 0.1162,\n",
      "        0.1001, 0.0991, 0.1191, 0.1074, 0.0981, 0.1025, 0.1055, 0.1104, 0.1455,\n",
      "        0.1099, 0.1094, 0.1196, 0.1001, 0.1157, 0.1104, 0.1064, 0.1064, 0.1152,\n",
      "        0.1069, 0.1030, 0.0996, 0.0972, 0.1157, 0.1079, 0.1050, 0.1074, 0.1196,\n",
      "        0.1074, 0.1187, 0.1021, 0.0947, 0.0981, 0.1006, 0.1270, 0.1035, 0.0938,\n",
      "        0.0981, 0.1050, 0.0903, 0.1079, 0.1045, 0.1138, 0.1069, 0.1289, 0.0942,\n",
      "        0.1035, 0.1011, 0.0933, 0.1079, 0.1133, 0.1099, 0.1289, 0.1152, 0.1045,\n",
      "        0.1055, 0.1108, 0.1064, 0.0854, 0.1182, 0.1084, 0.1040, 0.2441],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.5527e-01,  1.3672e-01,  1.8555e-01,  9.3262e-02,  1.6309e-01,\n",
      "         1.4648e-01, -9.2506e-05,  1.4941e-01,  1.9531e-01,  1.2598e-01,\n",
      "         1.0498e-01,  6.3965e-02,  1.6406e-01,  1.4844e-01,  1.8066e-01,\n",
      "         1.8555e-01,  1.5918e-01,  1.8262e-01,  1.1621e-01,  1.6797e-01,\n",
      "         1.5527e-01,  1.4551e-01,  1.5723e-01,  9.8145e-02,  1.9434e-01,\n",
      "         2.0703e-01,  1.8652e-01,  1.6699e-01,  1.3770e-01,  1.5918e-01,\n",
      "         1.6211e-01,  9.6191e-02,  2.5391e-01,  1.6602e-01,  1.6797e-01,\n",
      "         1.6602e-01,  1.4160e-01,  1.6211e-01,  1.6113e-01,  1.7188e-01,\n",
      "         1.7969e-01,  1.2793e-01,  1.7969e-01,  1.5918e-01,  1.4062e-01,\n",
      "         1.2305e-01,  1.4648e-01,  1.1865e-01,  1.7969e-01,  1.4844e-01,\n",
      "         1.8457e-01,  1.3672e-01,  1.6699e-01,  1.6895e-01,  1.5332e-01,\n",
      "         1.3770e-01,  1.5723e-01,  1.4160e-01,  1.4648e-01,  1.4648e-01,\n",
      "         1.3672e-01,  1.6602e-01,  1.5527e-01,  1.2695e-01,  1.0742e-01,\n",
      "         1.2793e-01,  1.9238e-01,  1.3379e-01,  1.2305e-01,  1.8457e-01,\n",
      "         1.1182e-01,  1.6797e-01,  1.6406e-01,  1.6895e-01,  1.4746e-01,\n",
      "         1.8066e-01,  1.8262e-01,  1.2891e-01,  2.1851e-02,  1.7871e-01,\n",
      "         1.3867e-01,  1.5527e-01,  1.6602e-01,  1.6309e-01,  1.5820e-01,\n",
      "         1.4160e-01,  1.6016e-01,  1.3672e-01,  1.9043e-01,  1.4062e-01,\n",
      "         1.5332e-01,  1.6016e-01,  1.8457e-01,  1.5527e-01,  1.5039e-01,\n",
      "         1.9336e-01,  1.5039e-01,  1.2988e-01,  1.5527e-01,  1.9238e-01,\n",
      "         1.6602e-01,  1.4844e-01,  1.5625e-01,  1.4844e-01,  1.7188e-01,\n",
      "         1.5430e-01,  1.4844e-01,  1.5723e-01,  1.8359e-01,  1.3672e-01,\n",
      "         1.7480e-01,  1.4941e-01,  9.8633e-02,  1.9141e-01,  1.6992e-01,\n",
      "         1.6699e-01,  1.9922e-01,  1.6504e-01,  1.6602e-01,  1.4258e-01,\n",
      "         1.4453e-01,  1.6797e-01,  1.5723e-01,  1.5039e-01,  1.6016e-01,\n",
      "         1.4941e-01,  1.5039e-01,  1.9922e-01,  1.9141e-01,  2.0410e-01,\n",
      "         1.2598e-01,  1.5137e-01,  1.4355e-01,  1.6699e-01,  1.8066e-01,\n",
      "         1.5039e-01,  2.5787e-03,  1.9043e-01,  1.8652e-01,  1.4648e-01,\n",
      "         1.7578e-01,  1.3281e-01,  1.5430e-01,  1.3477e-01,  1.4551e-01,\n",
      "         1.3086e-01,  1.6309e-01,  1.5625e-01,  1.6113e-01,  1.5820e-01,\n",
      "         1.2207e-01,  1.4453e-01,  1.7383e-01,  9.1309e-02,  1.6699e-01,\n",
      "         1.5332e-01,  1.5137e-01,  1.4355e-01,  1.5527e-01,  1.7578e-01,\n",
      "         1.9531e-01,  1.2891e-01,  1.4062e-01,  4.4250e-03,  1.5430e-01,\n",
      "         1.7578e-01,  1.7383e-01,  1.4355e-01,  1.4941e-01,  1.4551e-01,\n",
      "         1.4062e-01,  1.5918e-01,  1.2402e-01,  1.7773e-01,  1.4844e-01,\n",
      "         1.7480e-01,  1.6406e-01,  1.6797e-01,  1.6504e-01,  1.6504e-01,\n",
      "         1.7090e-01,  1.8652e-01,  1.5137e-01,  1.7871e-01,  1.7090e-01,\n",
      "         1.4551e-01,  8.3008e-02,  1.6797e-01,  1.5039e-01,  1.6406e-01,\n",
      "         1.7090e-01,  5.6885e-02,  1.5234e-01,  1.4551e-01,  1.3086e-01,\n",
      "         1.8262e-01,  1.5723e-01,  1.3965e-01,  1.4551e-01,  1.3770e-01,\n",
      "         1.7969e-01,  1.7969e-01,  1.7676e-01,  1.7480e-01,  1.8262e-01,\n",
      "         1.5137e-01,  1.7383e-01,  1.7285e-01,  1.3770e-01,  1.1377e-01,\n",
      "         1.2158e-01,  1.6211e-01,  1.4355e-01,  1.8066e-01,  1.4453e-01,\n",
      "         1.5820e-01,  1.6797e-01,  1.5137e-01,  1.9727e-01,  1.4844e-01,\n",
      "         1.8457e-01,  1.6504e-01,  1.3965e-01,  1.6113e-01,  2.0410e-01,\n",
      "         1.7871e-01,  1.4160e-01,  1.6797e-01,  1.7773e-01,  1.6211e-01,\n",
      "         1.7188e-01,  1.6504e-01,  1.6602e-01,  1.5234e-01,  1.0400e-01,\n",
      "         1.4746e-01,  1.6895e-01,  1.4746e-01,  1.3965e-01,  1.5332e-01,\n",
      "         1.8457e-01,  1.5137e-01,  1.6113e-01,  1.6113e-01,  1.5332e-01,\n",
      "         1.4746e-01,  1.5918e-01,  1.8848e-01,  1.5234e-01,  1.6406e-01,\n",
      "         1.1279e-01,  1.2793e-01,  1.5430e-01,  1.4746e-01,  1.6602e-01,\n",
      "         1.6895e-01,  1.8164e-01,  1.4941e-01,  1.4746e-01,  1.5527e-01,\n",
      "         1.4746e-01,  1.6504e-01,  1.8262e-01,  1.4453e-01,  1.5332e-01,\n",
      "         1.4648e-01,  1.3965e-01,  1.4648e-01,  2.2168e-01,  1.7090e-01,\n",
      "         1.4160e-01,  1.7285e-01,  1.5820e-01,  1.8164e-01,  1.2451e-01,\n",
      "         1.4746e-01,  1.7969e-01,  1.4551e-01,  1.4648e-01,  1.5625e-01,\n",
      "         1.6699e-01,  1.8652e-01,  1.7578e-01,  1.4941e-01,  1.3672e-01,\n",
      "         1.4648e-01,  1.3281e-01,  1.7188e-01,  1.5039e-01,  1.4941e-01,\n",
      "         1.4941e-01,  1.8457e-01,  1.3965e-01,  1.7480e-01,  1.5234e-01,\n",
      "         1.5918e-01,  1.3086e-01,  1.7578e-01,  1.4160e-01,  1.5820e-01,\n",
      "         1.1719e-01,  1.3086e-01,  1.5332e-01,  1.4648e-01,  1.5527e-01,\n",
      "         1.8359e-01,  1.5332e-01,  1.5625e-01,  1.9043e-01,  1.3867e-01,\n",
      "         1.6504e-01,  1.8457e-01,  1.6113e-01,  1.3477e-01,  1.6113e-01,\n",
      "         1.3672e-01,  1.2793e-01,  1.5918e-01,  1.6016e-01,  1.6699e-01,\n",
      "         1.2891e-01,  1.6016e-01,  1.5527e-01,  1.4258e-01,  1.4844e-01,\n",
      "         1.8555e-01,  1.4453e-01,  1.7383e-01,  1.6992e-01,  1.4648e-01,\n",
      "         1.5430e-01,  1.5039e-01,  1.4844e-01,  9.0332e-02,  1.4355e-01,\n",
      "         1.5039e-01,  1.5918e-01,  1.7773e-01,  1.3867e-01,  1.1914e-01,\n",
      "         1.4355e-01,  1.4941e-01,  1.5723e-01,  1.5918e-01,  1.4551e-01,\n",
      "         1.4648e-01,  1.6895e-01,  1.9727e-01,  1.4746e-01,  1.4258e-01,\n",
      "         1.5137e-01,  1.6895e-01,  1.6211e-01,  1.6211e-01,  1.7285e-01,\n",
      "         1.5137e-01,  1.4160e-01,  1.6211e-01,  1.4160e-01,  1.8359e-01,\n",
      "         1.6797e-01,  1.6797e-01,  1.3672e-01,  1.8555e-01,  2.2070e-01,\n",
      "         1.3477e-01,  1.8945e-01,  1.5198e-02,  1.6895e-01,  1.2793e-01,\n",
      "         4.0283e-03,  1.6699e-01,  1.4453e-01,  1.1670e-01,  1.7773e-01,\n",
      "         1.4648e-01,  1.3281e-01,  1.6406e-01,  1.2793e-01,  1.5430e-01,\n",
      "         1.6797e-01,  1.2207e-01,  1.5918e-01,  1.5820e-01,  1.8359e-01,\n",
      "         1.2109e-01,  1.0889e-01,  1.3672e-01,  1.7383e-01,  1.8164e-01,\n",
      "         1.6406e-01,  1.7676e-01,  1.7090e-01,  1.5625e-01,  1.6504e-01,\n",
      "         1.4355e-01,  1.5723e-01,  1.4062e-01,  1.5430e-01,  1.4160e-01,\n",
      "         1.7188e-01,  1.8457e-01,  1.4062e-01,  1.3477e-01,  1.4453e-01,\n",
      "         1.5332e-01,  1.4844e-01,  1.4941e-01,  1.4746e-01,  1.6406e-01,\n",
      "         1.6309e-01,  1.5918e-01,  4.7852e-02,  1.5234e-01,  5.5664e-02,\n",
      "         1.4453e-01,  1.4941e-01,  1.6309e-01,  1.8164e-01,  1.6016e-01,\n",
      "         1.6211e-01,  7.5195e-02,  1.6895e-01,  1.8262e-01,  2.0215e-01,\n",
      "         1.4648e-01,  1.6016e-01,  1.6895e-01,  1.8066e-01,  1.5234e-01,\n",
      "         1.4453e-01,  1.1768e-01,  1.4258e-01,  1.4746e-01,  1.0596e-01,\n",
      "         1.4258e-01,  1.4941e-01,  1.6699e-01,  1.6797e-01,  1.8555e-01,\n",
      "         1.6797e-01,  1.5137e-01,  1.8262e-01,  8.8379e-02,  1.6016e-01,\n",
      "         1.4746e-01,  1.9629e-01,  8.3984e-02,  1.6797e-01,  1.7188e-01,\n",
      "         1.8359e-01,  1.5039e-01,  1.6602e-01,  1.7090e-01,  1.4160e-01,\n",
      "         1.5430e-01,  1.4941e-01,  1.4355e-01,  2.0996e-01,  1.3867e-01,\n",
      "         1.4746e-01,  1.8066e-01,  1.6016e-01,  1.8066e-01,  1.5234e-01,\n",
      "         1.4160e-01,  1.6602e-01,  2.1387e-01,  1.5625e-01,  1.4746e-01,\n",
      "         1.5723e-01,  1.3574e-01,  1.9434e-01,  1.6992e-01,  1.6016e-01,\n",
      "         1.1768e-01,  1.5723e-01,  1.5137e-01,  2.2656e-01,  1.2695e-01,\n",
      "         1.2598e-01,  1.5234e-01,  9.6680e-02,  1.7188e-01,  1.7969e-01,\n",
      "         9.9609e-02,  1.2988e-01,  1.5625e-01,  1.3672e-01,  1.5234e-01,\n",
      "         1.2891e-01,  1.7383e-01,  1.4551e-01,  1.1084e-01,  1.4453e-01,\n",
      "         1.7285e-01,  1.4844e-01,  9.0820e-02,  1.5039e-01,  1.6602e-01,\n",
      "         1.4648e-01,  1.5527e-01,  1.7676e-01,  1.5039e-01,  1.5137e-01,\n",
      "         1.7480e-01,  1.4062e-01,  9.5703e-02,  1.8164e-01,  1.5039e-01,\n",
      "         1.6406e-01,  1.1816e-01], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k in model_en2de.parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model_v.parameters():\n",
    "    k.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for k in model_v.parameters():\n",
    "    print(k.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
