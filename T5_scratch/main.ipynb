{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer,T5Model,T5ForConditionalGeneration\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 508785, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=16,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=4,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_w_synthetic_num_points', type=int, default=4,      help='train_w_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=4,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=3,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=5,      help='gradient clipping')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=3e-2,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=3e-2,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-4,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.7,    help='momentum')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.5,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.5,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23 03:59:55 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 33.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23 03:59:55 PM |\t  Namespace(A_lr=0.0001, batch_size=16, decay=0.001, epochs=50, gpu=0, grad_clip=5, learning_rate_min=1e-08, momentum=0.7, pre_epochs=3, syndata_loss_ratio=0.5, train_A=0, train_A_num_points=4, train_num_points=8000, train_v_num_points=4, train_w_num_points=4, train_w_synthetic_num_points=4, traindata_loss_ratio=0.5, v_lr=0.003, valid_begin=1, valid_num_points=100, w_lr=0.003)\n",
      "03/23 03:59:55 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "03/23 03:59:55 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "config = AutoConfig.from_pretrained(\"t5-small\")\n",
    "model_scartch = T5ForConditionalGeneration(config=config)\n",
    "torch.save(model_scartch,'T5_scartch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23 04:00:29 PM |\t  train len: 8000\n",
      "03/23 04:00:29 PM |\t  train_w_num_points_len: 2000\n",
      "03/23 04:00:29 PM |\t  train_w_synthetic_num_points_len: 2000\n",
      "03/23 04:00:29 PM |\t  train_v_num_points_len: 2000\n",
      "03/23 04:00:29 PM |\t  train_A_num_points_len: 2000\n",
      "03/23 04:00:29 PM |\t  valid len: 100\n",
      "03/23 04:00:29 PM |\t  test len: 3003\n",
      "03/23 04:00:29 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#,ignore_index = tokenizer.pad_token_id)#\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][:args.valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = 'translate English to German: ' + t['en'] \n",
    "# preprocess(train)\n",
    "# preprocess(valid)\n",
    "# preprocess(test)\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_w_synthetic_num_points_len = num_batch * args.train_w_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_w_synthetic_num_points_len: %d\",train_w_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=2)#, sampler=RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(),args.w_lr)#,momentum=args.momentum,weight_decay=args.decay)\n",
    "# scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=30, gamma=0.5)\n",
    "scheduler_w  = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(),args.v_lr)#,momentum=args.momentum,weight_decay=args.decay)\n",
    "# scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=30, gamma=0.5)\n",
    "scheduler_v  = torch.optim.lr_scheduler.CosineAnnealingLR(v_optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x = ['im going to eat now ','it is my nameit is']\n",
    "# # for index,i in enumerate(x) :\n",
    "# #     x[index] = 'translate Enlgish to German:' + x[index]\n",
    "# # y= tokenize(x, tokenizer, max_length = max_length)\n",
    "# # input = y[0].cuda()\n",
    "# # output  = model_v.generate(input,max_length=max_length)\n",
    "# # tokenizer.batch_decode(output)\n",
    "\n",
    "\n",
    "# metric_bleu =  load_metric('sacrebleu')\n",
    "# predlist = ['Eine republikanische Strategie zur Bekämpfung der Wiederwahl Obamas','Die republikanischen Führer rechtfertigten ihre Politik mit der Notwendigkeit , Wahlbetrug zu bekämpfen .']\n",
    "# targetlist = ['Eine republikanische Strategie um der Wiederwahl von Obama entgegenzutreten','Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit , den Wahlbetrug zu bekämpfen']\n",
    "# predlist = ['Eine republikanische Strategie zur Bekämpfung der Wiederwahl Obamas', 'Die republikanischen Führer rechtfertigten ihre Politik mit der Notwendigkeit, Wahlbetrug zu bekämpfen.']\n",
    "# targetlist =['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
    "# \n",
    "# predlist = [x.lower().translate( str.maketrans('', '', string.punctuation))  for x in predlist]\n",
    "# targetlist = [[x.lower().translate( str.maketrans('', '', string.punctuation))] for x in targetlist]\n",
    "# print(predlist)\n",
    "# print(targetlist)\n",
    "# metric_bleu.add_batch(predictions=predlist, references=targetlist)\n",
    "\n",
    "# sacrebleu_score = metric_bleu.compute()\n",
    "# print(sacrebleu_score)\n",
    "# from nltk.translate import bleu\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# def bleu(reference_captions, predicted_caption):\n",
    "#     return 100 * sentence_bleu(reference_captions, predicted_caption,\n",
    "#                                weights=(0.25, 0.25, 0.25,0.25), smoothing_function=SmoothingFunction().method1)\n",
    "# x = bleu(targetlist[1],predlist[1])+bleu(targetlist[0],predlist[0])\n",
    "# print(x/2)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    metric_bleu =  load_metric('bleu')\n",
    "\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).cuda()\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).cuda()\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).cuda()\n",
    "        with torch.no_grad():\n",
    "            ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "            acc+= ls\n",
    "            counter+= 1\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            # print('pre',pre)\n",
    "            try:\n",
    "                x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "                pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "                label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "                \n",
    "                pred_str = [x.replace('.', '')  for x in pred_decoded]\n",
    "                label_str = [[x.replace('.', '')] for x in label_decoded]\n",
    "                pred_list = [x.replace('.', '').split()  for x in pred_decoded]\n",
    "                label_list = [[x.replace('.', '').split()] for x in label_decoded]\n",
    "                #pred_str = [x.translate( str.maketrans('', '', string.punctuation)) for x in pred_decoded] \n",
    "                # label_str = [[x.translate( str.maketrans('', '', string.punctuation))] for x in label_decoded]\n",
    "                # pred_list = [x.translate( str.maketrans('', '', string.punctuation)).split()  for x in pred_decoded]#TODO:improve\n",
    "                # label_list = [[x.translate( str.maketrans('', '', string.punctuation)).split()] for x in label_decoded]#TODO:improve\n",
    "                if  step%100==0:\n",
    "                    logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                    logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                    logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "                metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "                metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "                \n",
    "               \n",
    "            except Exception as ex:\n",
    "                print(tokenizer.batch_decode(pre),[[x] for x in tokenizer.batch_decode(test_dataloadery)])\n",
    "                raise Exception(ex)\n",
    "        # logging.info(f\"loss:{ls}\")\n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])\n",
    "    logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    writer.add_scalar(model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/sacreBLEU\",sacrebleu_score['score'], global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/BLEU\",bleu_score['bleu'], global_step=epoch)\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    \n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    counter = 0\n",
    "    wsize = args.train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = args.train_w_synthetic_num_points\n",
    "    vsize = args.train_v_num_points \n",
    "    Asize = args.train_A_num_points \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        counter+=1\n",
    "        batch_loss_w, batch_loss_v = 0, 0\n",
    "        \n",
    "        train_x = Variable(batch[0], requires_grad=False).cuda()\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        train_y = Variable(batch[2], requires_grad=False).cuda()\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).cuda() \n",
    "\n",
    "        input_w = train_x[:wsize]\n",
    "        \n",
    "        input_w_attn = train_x_attn[:wsize]\n",
    "        output_w = train_y[:wsize]\n",
    "        output_w_attn = train_y_attn[:wsize]\n",
    "        attn_idx = attn_idx_list[args.train_w_num_points*step:(args.train_w_num_points*step+args.train_w_num_points)]\n",
    "           \n",
    "        input_syn = train_x[wsize:wsize+synsize]\n",
    "        input_syn_attn = train_x_attn[wsize:wsize+synsize]\n",
    "\n",
    "        input_v = train_x[wsize+synsize:wsize+synsize+vsize]\n",
    "        input_v_attn = train_x_attn[wsize+synsize:wsize+synsize+vsize]\n",
    "        output_v = train_y[wsize+synsize:wsize+synsize+vsize]\n",
    "        output_v_attn = train_y_attn[wsize+synsize:wsize+synsize+vsize]\n",
    "\n",
    "        input_A_v      = train_x[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        input_A_v_attn = train_x_attn[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        output_A_v      = train_y[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        output_A_v_attn = train_y_attn[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "       \n",
    "\n",
    "        if (epoch <= args.epochs) and (args.train_A == 1) and epoch >= args.pre_epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "        \n",
    "        if  epoch <= args.epochs and epoch <= args.epochs:\n",
    "            \n",
    "            w_optimizer.zero_grad()\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            batch_loss_w += loss_w.item()\n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "            loss_w.backward()\n",
    "            # nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "            w_optimizer.step()\n",
    "        # if epoch >= args.pre_epochs and epoch <= args.epochs:\n",
    "        #     v_optimizer.zero_grad()\n",
    "        #     loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)#,input_v,input_v_attn,output_v,output_v_attn)\n",
    "        #     loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            \n",
    "        #     v_loss =  (args.syndata_loss_ratio*loss_aug+args.traindata_loss_ratio*loss)/num_batch\n",
    "            \n",
    "        #     batch_loss_v += v_loss.item()\n",
    "        #     v_trainloss_acc+=v_loss.item()\n",
    "        #     v_loss.backward()\n",
    "        #     # nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "        #     v_optimizer.step()     \n",
    "                \n",
    "            \n",
    "        if(step*args.batch_size%50==0):\n",
    "            logging.info(f\"{step*args.batch_size*100/(args.train_num_points)}%\")\n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    \n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23 04:13:07 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.0029940831253583177,\t\tlr_v:0.0029940831253583177----------------\n",
      "03/23 04:13:08 PM |\t  0.0%\n",
      "03/23 04:13:13 PM |\t  5.0%\n",
      "03/23 04:13:18 PM |\t  10.0%\n",
      "03/23 04:13:23 PM |\t  15.0%\n",
      "03/23 04:13:27 PM |\t  20.0%\n",
      "03/23 04:13:32 PM |\t  25.0%\n",
      "03/23 04:13:37 PM |\t  30.0%\n",
      "03/23 04:13:42 PM |\t  35.0%\n",
      "03/23 04:13:46 PM |\t  40.0%\n",
      "03/23 04:13:51 PM |\t  45.0%\n",
      "03/23 04:13:55 PM |\t  50.0%\n",
      "03/23 04:14:00 PM |\t  55.0%\n",
      "03/23 04:14:05 PM |\t  60.0%\n",
      "03/23 04:14:09 PM |\t  65.0%\n",
      "03/23 04:14:14 PM |\t  70.0%\n",
      "03/23 04:14:19 PM |\t  75.0%\n",
      "03/23 04:14:24 PM |\t  80.0%\n",
      "03/23 04:14:29 PM |\t  85.0%\n",
      "03/23 04:14:34 PM |\t  90.0%\n",
      "03/23 04:14:38 PM |\t  95.0%\n",
      "03/23 04:14:43 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:14:43 PM |\t  w_train_loss:1.1995284657459706,v_train_loss:0\n",
      "03/23 04:14:43 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.0029793303201378963,\t\tlr_v:0.0029793303201378963----------------\n",
      "03/23 04:14:45 PM |\t  0.0%\n",
      "03/23 04:14:50 PM |\t  5.0%\n",
      "03/23 04:14:54 PM |\t  10.0%\n",
      "03/23 04:14:59 PM |\t  15.0%\n",
      "03/23 04:15:04 PM |\t  20.0%\n",
      "03/23 04:15:09 PM |\t  25.0%\n",
      "03/23 04:15:13 PM |\t  30.0%\n",
      "03/23 04:15:18 PM |\t  35.0%\n",
      "03/23 04:15:22 PM |\t  40.0%\n",
      "03/23 04:15:27 PM |\t  45.0%\n",
      "03/23 04:15:32 PM |\t  50.0%\n",
      "03/23 04:15:36 PM |\t  55.0%\n",
      "03/23 04:15:41 PM |\t  60.0%\n",
      "03/23 04:15:46 PM |\t  65.0%\n",
      "03/23 04:15:50 PM |\t  70.0%\n",
      "03/23 04:15:55 PM |\t  75.0%\n",
      "03/23 04:15:59 PM |\t  80.0%\n",
      "03/23 04:16:04 PM |\t  85.0%\n",
      "03/23 04:16:09 PM |\t  90.0%\n",
      "03/23 04:16:13 PM |\t  95.0%\n",
      "03/23 04:16:18 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:16:18 PM |\t  w_train_loss:1.1893535072449595,v_train_loss:0\n",
      "03/23 04:16:18 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:0.0029587625584743277,\t\tlr_v:0.0029587625584743277----------------\n",
      "03/23 04:16:20 PM |\t  0.0%\n",
      "03/23 04:16:24 PM |\t  5.0%\n",
      "03/23 04:16:29 PM |\t  10.0%\n",
      "03/23 04:16:34 PM |\t  15.0%\n",
      "03/23 04:16:38 PM |\t  20.0%\n",
      "03/23 04:16:43 PM |\t  25.0%\n",
      "03/23 04:16:48 PM |\t  30.0%\n",
      "03/23 04:16:52 PM |\t  35.0%\n",
      "03/23 04:16:57 PM |\t  40.0%\n",
      "03/23 04:17:02 PM |\t  45.0%\n",
      "03/23 04:17:06 PM |\t  50.0%\n",
      "03/23 04:17:11 PM |\t  55.0%\n",
      "03/23 04:17:15 PM |\t  60.0%\n",
      "03/23 04:17:20 PM |\t  65.0%\n",
      "03/23 04:17:25 PM |\t  70.0%\n",
      "03/23 04:17:29 PM |\t  75.0%\n",
      "03/23 04:17:34 PM |\t  80.0%\n",
      "03/23 04:17:39 PM |\t  85.0%\n",
      "03/23 04:17:43 PM |\t  90.0%\n",
      "03/23 04:17:48 PM |\t  95.0%\n",
      "03/23 04:17:52 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:17:52 PM |\t  w_train_loss:1.1792595720035024,v_train_loss:0\n",
      "03/23 04:17:52 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:0.0029324609425582164,\t\tlr_v:0.0029324609425582164----------------\n",
      "03/23 04:17:54 PM |\t  0.0%\n",
      "03/23 04:17:59 PM |\t  5.0%\n",
      "03/23 04:18:03 PM |\t  10.0%\n",
      "03/23 04:18:08 PM |\t  15.0%\n",
      "03/23 04:18:12 PM |\t  20.0%\n",
      "03/23 04:18:17 PM |\t  25.0%\n",
      "03/23 04:18:22 PM |\t  30.0%\n",
      "03/23 04:18:26 PM |\t  35.0%\n",
      "03/23 04:18:31 PM |\t  40.0%\n",
      "03/23 04:18:35 PM |\t  45.0%\n",
      "03/23 04:18:40 PM |\t  50.0%\n",
      "03/23 04:18:44 PM |\t  55.0%\n",
      "03/23 04:18:49 PM |\t  60.0%\n",
      "03/23 04:18:53 PM |\t  65.0%\n",
      "03/23 04:18:58 PM |\t  70.0%\n",
      "03/23 04:19:02 PM |\t  75.0%\n",
      "03/23 04:19:07 PM |\t  80.0%\n",
      "03/23 04:19:11 PM |\t  85.0%\n",
      "03/23 04:19:16 PM |\t  90.0%\n",
      "03/23 04:19:21 PM |\t  95.0%\n",
      "03/23 04:19:25 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:19:25 PM |\t  w_train_loss:1.170518810977228,v_train_loss:0\n",
      "03/23 04:19:25 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:0.0029005292030015164,\t\tlr_v:0.0029005292030015164----------------\n",
      "03/23 04:19:27 PM |\t  0.0%\n",
      "03/23 04:19:31 PM |\t  5.0%\n",
      "03/23 04:19:36 PM |\t  10.0%\n",
      "03/23 04:19:40 PM |\t  15.0%\n",
      "03/23 04:19:45 PM |\t  20.0%\n",
      "03/23 04:19:49 PM |\t  25.0%\n",
      "03/23 04:19:54 PM |\t  30.0%\n",
      "03/23 04:19:59 PM |\t  35.0%\n",
      "03/23 04:20:03 PM |\t  40.0%\n",
      "03/23 04:20:08 PM |\t  45.0%\n",
      "03/23 04:20:12 PM |\t  50.0%\n",
      "03/23 04:20:17 PM |\t  55.0%\n",
      "03/23 04:20:21 PM |\t  60.0%\n",
      "03/23 04:20:26 PM |\t  65.0%\n",
      "03/23 04:20:30 PM |\t  70.0%\n",
      "03/23 04:20:35 PM |\t  75.0%\n",
      "03/23 04:20:39 PM |\t  80.0%\n",
      "03/23 04:20:44 PM |\t  85.0%\n",
      "03/23 04:20:49 PM |\t  90.0%\n",
      "03/23 04:20:53 PM |\t  95.0%\n",
      "03/23 04:20:58 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:20:58 PM |\t  w_train_loss:1.166769367933739,v_train_loss:0\n",
      "03/23 04:20:58 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:0.002863093289176917,\t\tlr_v:0.002863093289176917----------------\n",
      "03/23 04:20:59 PM |\t  0.0%\n",
      "03/23 04:21:04 PM |\t  5.0%\n",
      "03/23 04:21:08 PM |\t  10.0%\n",
      "03/23 04:21:13 PM |\t  15.0%\n",
      "03/23 04:21:17 PM |\t  20.0%\n",
      "03/23 04:21:22 PM |\t  25.0%\n",
      "03/23 04:21:26 PM |\t  30.0%\n",
      "03/23 04:21:31 PM |\t  35.0%\n",
      "03/23 04:21:35 PM |\t  40.0%\n",
      "03/23 04:21:40 PM |\t  45.0%\n",
      "03/23 04:21:44 PM |\t  50.0%\n",
      "03/23 04:21:49 PM |\t  55.0%\n",
      "03/23 04:21:53 PM |\t  60.0%\n",
      "03/23 04:21:58 PM |\t  65.0%\n",
      "03/23 04:22:02 PM |\t  70.0%\n",
      "03/23 04:22:07 PM |\t  75.0%\n",
      "03/23 04:22:11 PM |\t  80.0%\n",
      "03/23 04:22:16 PM |\t  85.0%\n",
      "03/23 04:22:21 PM |\t  90.0%\n",
      "03/23 04:22:25 PM |\t  95.0%\n",
      "03/23 04:22:30 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:22:30 PM |\t  w_train_loss:1.161571952921804,v_train_loss:0\n",
      "03/23 04:22:30 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:0.0028203008718591573,\t\tlr_v:0.0028203008718591573----------------\n",
      "03/23 04:22:31 PM |\t  0.0%\n",
      "03/23 04:22:36 PM |\t  5.0%\n",
      "03/23 04:22:40 PM |\t  10.0%\n",
      "03/23 04:22:45 PM |\t  15.0%\n",
      "03/23 04:22:49 PM |\t  20.0%\n",
      "03/23 04:22:54 PM |\t  25.0%\n",
      "03/23 04:22:58 PM |\t  30.0%\n",
      "03/23 04:23:03 PM |\t  35.0%\n",
      "03/23 04:23:08 PM |\t  40.0%\n",
      "03/23 04:23:12 PM |\t  45.0%\n",
      "03/23 04:23:17 PM |\t  50.0%\n",
      "03/23 04:23:21 PM |\t  55.0%\n",
      "03/23 04:23:26 PM |\t  60.0%\n",
      "03/23 04:23:30 PM |\t  65.0%\n",
      "03/23 04:23:35 PM |\t  70.0%\n",
      "03/23 04:23:39 PM |\t  75.0%\n",
      "03/23 04:23:44 PM |\t  80.0%\n",
      "03/23 04:23:49 PM |\t  85.0%\n",
      "03/23 04:23:53 PM |\t  90.0%\n",
      "03/23 04:23:58 PM |\t  95.0%\n",
      "03/23 04:24:02 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:24:02 PM |\t  w_train_loss:1.1543816052144393,v_train_loss:0\n",
      "03/23 04:24:02 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:0.0027723207601267406,\t\tlr_v:0.0027723207601267406----------------\n",
      "03/23 04:24:04 PM |\t  0.0%\n",
      "03/23 04:24:09 PM |\t  5.0%\n",
      "03/23 04:24:13 PM |\t  10.0%\n",
      "03/23 04:24:18 PM |\t  15.0%\n",
      "03/23 04:24:23 PM |\t  20.0%\n",
      "03/23 04:24:27 PM |\t  25.0%\n",
      "03/23 04:24:32 PM |\t  30.0%\n",
      "03/23 04:24:36 PM |\t  35.0%\n",
      "03/23 04:24:41 PM |\t  40.0%\n",
      "03/23 04:24:45 PM |\t  45.0%\n",
      "03/23 04:24:50 PM |\t  50.0%\n",
      "03/23 04:24:54 PM |\t  55.0%\n",
      "03/23 04:24:59 PM |\t  60.0%\n",
      "03/23 04:25:03 PM |\t  65.0%\n",
      "03/23 04:25:08 PM |\t  70.0%\n",
      "03/23 04:25:13 PM |\t  75.0%\n",
      "03/23 04:25:17 PM |\t  80.0%\n",
      "03/23 04:25:22 PM |\t  85.0%\n",
      "03/23 04:25:26 PM |\t  90.0%\n",
      "03/23 04:25:31 PM |\t  95.0%\n",
      "03/23 04:25:35 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:25:35 PM |\t  w_train_loss:1.1493955426849425,v_train_loss:0\n",
      "03/23 04:25:35 PM |\t  \n",
      "\n",
      "  ----------------epoch:8,\t\tlr_w:0.0027193422348205285,\t\tlr_v:0.0027193422348205285----------------\n",
      "03/23 04:25:37 PM |\t  0.0%\n",
      "03/23 04:25:42 PM |\t  5.0%\n",
      "03/23 04:25:46 PM |\t  10.0%\n",
      "03/23 04:25:51 PM |\t  15.0%\n",
      "03/23 04:25:56 PM |\t  20.0%\n",
      "03/23 04:26:00 PM |\t  25.0%\n",
      "03/23 04:26:05 PM |\t  30.0%\n",
      "03/23 04:26:09 PM |\t  35.0%\n",
      "03/23 04:26:14 PM |\t  40.0%\n",
      "03/23 04:26:19 PM |\t  45.0%\n",
      "03/23 04:26:23 PM |\t  50.0%\n",
      "03/23 04:26:28 PM |\t  55.0%\n",
      "03/23 04:26:33 PM |\t  60.0%\n",
      "03/23 04:26:37 PM |\t  65.0%\n",
      "03/23 04:26:42 PM |\t  70.0%\n",
      "03/23 04:26:47 PM |\t  75.0%\n",
      "03/23 04:26:52 PM |\t  80.0%\n",
      "03/23 04:26:56 PM |\t  85.0%\n",
      "03/23 04:27:01 PM |\t  90.0%\n",
      "03/23 04:27:06 PM |\t  95.0%\n",
      "03/23 04:27:10 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:27:10 PM |\t  w_train_loss:1.1574681430938654,v_train_loss:0\n",
      "03/23 04:27:10 PM |\t  \n",
      "\n",
      "  ----------------epoch:9,\t\tlr_w:0.0026615743011843876,\t\tlr_v:0.0026615743011843876----------------\n",
      "03/23 04:27:12 PM |\t  0.0%\n",
      "03/23 04:27:17 PM |\t  5.0%\n",
      "03/23 04:27:22 PM |\t  10.0%\n",
      "03/23 04:27:26 PM |\t  15.0%\n",
      "03/23 04:27:31 PM |\t  20.0%\n",
      "03/23 04:27:35 PM |\t  25.0%\n",
      "03/23 04:27:40 PM |\t  30.0%\n",
      "03/23 04:27:45 PM |\t  35.0%\n",
      "03/23 04:27:49 PM |\t  40.0%\n",
      "03/23 04:27:54 PM |\t  45.0%\n",
      "03/23 04:27:58 PM |\t  50.0%\n",
      "03/23 04:28:03 PM |\t  55.0%\n",
      "03/23 04:28:07 PM |\t  60.0%\n",
      "03/23 04:28:12 PM |\t  65.0%\n",
      "03/23 04:28:16 PM |\t  70.0%\n",
      "03/23 04:28:21 PM |\t  75.0%\n",
      "03/23 04:28:26 PM |\t  80.0%\n",
      "03/23 04:28:30 PM |\t  85.0%\n",
      "03/23 04:28:35 PM |\t  90.0%\n",
      "03/23 04:28:40 PM |\t  95.0%\n",
      "03/23 04:28:44 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:28:44 PM |\t  w_train_loss:1.1515186470933259,v_train_loss:0\n",
      "03/23 04:28:44 PM |\t  \n",
      "\n",
      "  ----------------epoch:10,\t\tlr_w:0.0025992448636313415,\t\tlr_v:0.0025992448636313415----------------\n",
      "03/23 04:28:46 PM |\t  0.0%\n",
      "03/23 04:28:51 PM |\t  5.0%\n",
      "03/23 04:28:55 PM |\t  10.0%\n",
      "03/23 04:29:00 PM |\t  15.0%\n",
      "03/23 04:29:05 PM |\t  20.0%\n",
      "03/23 04:29:09 PM |\t  25.0%\n",
      "03/23 04:29:14 PM |\t  30.0%\n",
      "03/23 04:29:19 PM |\t  35.0%\n",
      "03/23 04:29:23 PM |\t  40.0%\n",
      "03/23 04:29:28 PM |\t  45.0%\n",
      "03/23 04:29:33 PM |\t  50.0%\n",
      "03/23 04:29:37 PM |\t  55.0%\n",
      "03/23 04:29:42 PM |\t  60.0%\n",
      "03/23 04:29:47 PM |\t  65.0%\n",
      "03/23 04:29:51 PM |\t  70.0%\n",
      "03/23 04:29:56 PM |\t  75.0%\n",
      "03/23 04:30:01 PM |\t  80.0%\n",
      "03/23 04:30:05 PM |\t  85.0%\n",
      "03/23 04:30:10 PM |\t  90.0%\n",
      "03/23 04:30:14 PM |\t  95.0%\n",
      "03/23 04:30:19 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:30:19 PM |\t  w_train_loss:1.1506093042553402,v_train_loss:0\n",
      "03/23 04:30:23 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "03/23 04:30:23 PM |\t  pred_decoded[:2]:['Die.', 'Die.']\n",
      "03/23 04:30:23 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "03/23 04:30:31 PM |\t  model_w_in_main sacreBLEU : 0.000000\n",
      "03/23 04:30:31 PM |\t  model_w_in_main BLEU : 0.000000\n",
      "03/23 04:30:31 PM |\t  model_w_in_main test loss : 2.434062\n",
      "03/23 04:31:08 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "03/23 04:31:08 PM |\t  pred_decoded[:2]:['189189 beforehand beforehand whisper whisper beforehand noroc norocschiffschiff noroc whisper noroc beforehand189 whisper189 noroc Sergi Sergi beforehand Sergi norocilliardilliardschiff whisperschiffilliard noroc189 Sergiilliard Sergischiff Sergi189schiff politics politicsilliard politics noroc politicsschiff beforehand politics Sergi politics189 politics whisper Sergi reimburse reimburse noroc OF OFschiff OF Sergi OF noroc reimburse OFilliard OF reimburse Sergi whisper OF whisper politics beforehand OF189 OF politics OF beforehand reimburse189 reimburseschiff reimburse beforehandilliard beforehand researcher researcher OF researcher Sergi researcher politics researcher noroc researcherschiff researcher whisper researcherKnownKnown researcher beforehandKnown norocKnown whisperKnownilliardKnownschiffKnown politicsKnown OFKnown SergiKnown beforehandschiff terminology terminology OF terminology noroc terminologyschiffversammlungversammlungschiffcolocoloschiffcaractéristiquescaractéristiquesschiff Marco MarcoKnownversammlung OFcaractéristiques noroccaractéristiques OF Marco Sergicaractéristiques Marco noroc Marco OFversammlung Marco whisper Marco politics terminology Marco reimburseversammlungcaractéristiques Sergi Marcoschiff algebra algebra OF Jon JonKnown Jonschiff Jon OF algebra Joncaractéristiques Jon algebra Marco algebraversammlung Jon Sergi Joncolo Jon noroc Jon terminology Jonversammlung algebra Sergicolo Marco researcher Jon MarcocaractéristiquesKnown Marco beforehandcaractéristiquesilliard Jon researcher terminologycaractéristiques politics Marcocolocaractéristiques beforehand Marco terminology algebra norocversammlung noroccolo Sergi algebraschiffposiposi OFposi MarcoposicoloposiKnownposi researcherposi Jonposi norocposi reimburse terminology Sergiposi beforehand Jon politics Jonilliard algebra terminology researcher algebraKnown algebra politicscaractéristiques terminologyversammlungcolo algebracolo researchercoloKnown reimburse Marcoilliard researcher MarcoversammlungKnowncolo noroc Sec Sec OF Secversammlung Sec Jon Secschiff Sec Marco Sec noroc algebra Seccaractéristiques Secposi Sec whisper Sec politics Sec reimburse SecKnown Secilliard Sec algebracaractéristiques whisper Jon beforehand Seccoloilliardposi whisper reimburse researcher189 Jon189caractéristiquesposi politicscolo politics algebrailliard Marcoexistexist norocexistcaractéristiquesexist Marco Jonexist OFexistschiffexist Sergiexist algebra whispercaractéristiques reimburseexist Secexist whisperexist Jon reimburseKnownexist terminologyexistilliardexist189 Sec beforehandexistcolo Sec researcher Sec terminology politicsexist beforehand algebra beforehandposischiff189exist politicsversammlungposicaractéristiques algebraexist reimburse Jonwhetherwhetherschiffwhethercaractéristiqueswhetherposiwhether Jon Thur Thur noroc Thur Sec Thur Marco Thur politics Thur Sergiwhether SecwhetherKnown Thur Jon whisper Thur whispercolowhether norocwhether OF ThurKnownwhether algebra Thurcaractéristiques Thurilliard Thurexist Thur OFwhether Sergi Thur beforehand Thurschiff Thur reimbursecaractéristiques researchercaractéristiques189whether Marcowhethercolo OFcoloexistKnowncaractéristiques Black Black Jon Black Marco Blackschiff Blackcaractéristiquesversammlung BlackKnown Black noroc Blackexist Black OF Black beforehand Black algebra Black politics Black Sergi Black terminology Blackwhether Black189 Black whisper Black Thur Black Sec Blackversammlungwhetherilliardwhether terminology reimburse Blackposi Black researcher reimburseilliard whisperilliardcaractéristiques', 'wormworm 1945 1945 Michigan Michiganworm Michigan 1945secondsecond Michigan Consumer Consumer”,”,second Consumersecond”, Consumer Michigansecond Marco Marco”, Marco Consumer Marco 1945 Marco Michigan Marco trousers trousers”, trousers Marco transparent transparentsecond transparent Marcosecond trousers transparent”, transparent trousers 1945”, 1945 transparent Consumer trouserssecond 1945 Consumer transparentEZEZ MarcoEZ transparent 1945EZ ConsumerEZsecondEZ 1945 trousersEZ”,EZ trousers Consumer 1945wormsecondobscurobscursecondASPASPsecond 1600 1600second ingrediente ingredientesecond High Highsecond OF OFsecond Pyramid Pyramidsecondilliardilliard OF Pyramidobscur OF Marco ingredienteASP Marco OFilliard MarcoASP ingrediente”, ingrediente Marcoobscur”, Highobscurilliard High Marco Pyramid ingrediente trousers OFEZ ingrediente OF Highilliardobscur 1600obscur trousers Pyramid Marco HighEZ 1600 transparent 1600EZ Pyramid trousersobscur Pyramid OFobscur Marcoilliard Pyramid”, OF 1600 PyramidASP transparentASP”,obscur High”, Pyramid transparent ingrediente PyramidEZobscurASP Michigan ingrediente Michigan transparentobscurEZ Michigan OF ingredienteEZASP trousersASP Pyramid 1600 OF”, 1600illiard”,ASPobscur ingrediente 1600 ingrediente transparentilliardsecond Sec Sec ingrediente Sec OF Sec Pyramid Sec Marco Secobscur SecASP Secilliard Sec”, Sec transparent Sec High SecEZ Sec Michiganobscur transparent Michigan Sec Consumer 1600 MarcoAgriculturalAgriculturalsecondAgriculturalASPAgricultural ingredienteAgricultural PyramidAgricultural SecAgricultural OF trousersAgriculturalobscurAgricultural transparentAgricultural Marco 1600”,Agricultural”,illiard ingrediente Consumerilliard MichiganASP OF Michiganilliard Consumer Seccaractéristiquescaractéristiques transparentcaractéristiques ingredientecaractéristiques Secsecondcaractéristiques Marcocaractéristiquessecond angajat angajatsecondshelteredshelteredsecond repeated repeated Marco vacant vacantcaractéristiques vacantsecond vacant Marcosheltered ingrediente angajat Sec angajatcaractéristiques OFcaractéristiquesEZ vacantEZcaractéristiques angajat ingredientesheltered OFsheltered Marco repeatedcaractéristiques Pyramid vacant transparent vacant Pyramidsheltered Pyramid angajat OFASPsheltered Michigancaractéristiques trousers vacant Sec vacantAgricultural vacant High transparent angajat Marco angajat transparent Pyramidcaractéristiquesobscurcaractéristiques Consumer ingrediente vacant ingrediente repeated Secsheltered vacant repeated OF vacant trousers repeated vacantobscur vacantsheltered”,caractéristiquesilliard angajat Consumer vacant Consumerobscur angajat vacantilliardshelteredcaractéristiquessheltered trouserscaractéristiquesschiffschiff Marcoschiff OFschiff transparentschiff vacantschiff ingredienteschiffobscurschiffEZschiffcaractéristiquesAgriculturalschiff Highschiff Secschiff Michiganschiffshelteredschiffsecondschiff repeatedsecondwormschiff”,schiff Consumerschiffilliardschiff PyramidschiffAgricultural angajatschiff angajatobscur repeatedEZ High vacantASPschiff 1600caractéristiques repeatedschiff trousers ingrediente High Pyramid Michigansheltered repeatedilliardcaractéristiques”,shelteredAgricultural High ingredienteilliard transparentsheltered Sec repeated transparent repeated trouserssheltered Consumer OF repeatedsheltered angajat Pyramidilliard vacant MichiganEZ angajatAgriculturalcaractéristiques High Consumer High repeated angajat trousersschiff boundary boundary transparent boundary Marco boundarycaractéristiques boundaryEZ boundary ingrediente boundary vacant boundary Sec boundary OF boundaryobscur boundary Consumer boundaryschiffASP boundary trousers boundarysecond boundarysheltered boundary”,']\n",
      "03/23 04:31:08 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "03/23 04:34:16 PM |\t  model_v_in_main sacreBLEU : 0.002163\n",
      "03/23 04:34:16 PM |\t  model_v_in_main BLEU : 0.000000\n",
      "03/23 04:34:16 PM |\t  model_v_in_main test loss : 3.973979\n",
      "03/23 04:34:16 PM |\t  \n",
      "\n",
      "  ----------------epoch:11,\t\tlr_w:0.0025325998258850127,\t\tlr_v:0.0025325998258850127----------------\n",
      "03/23 04:34:18 PM |\t  0.0%\n",
      "03/23 04:34:22 PM |\t  5.0%\n",
      "03/23 04:34:27 PM |\t  10.0%\n",
      "03/23 04:34:31 PM |\t  15.0%\n",
      "03/23 04:34:36 PM |\t  20.0%\n",
      "03/23 04:34:40 PM |\t  25.0%\n",
      "03/23 04:34:45 PM |\t  30.0%\n",
      "03/23 04:34:49 PM |\t  35.0%\n",
      "03/23 04:34:54 PM |\t  40.0%\n",
      "03/23 04:34:58 PM |\t  45.0%\n",
      "03/23 04:35:03 PM |\t  50.0%\n",
      "03/23 04:35:07 PM |\t  55.0%\n",
      "03/23 04:35:12 PM |\t  60.0%\n",
      "03/23 04:35:16 PM |\t  65.0%\n",
      "03/23 04:35:21 PM |\t  70.0%\n",
      "03/23 04:35:25 PM |\t  75.0%\n",
      "03/23 04:35:30 PM |\t  80.0%\n",
      "03/23 04:35:34 PM |\t  85.0%\n",
      "03/23 04:35:39 PM |\t  90.0%\n",
      "03/23 04:35:43 PM |\t  95.0%\n",
      "03/23 04:35:48 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:35:48 PM |\t  w_train_loss:1.1574574637925252,v_train_loss:0\n",
      "03/23 04:35:48 PM |\t  \n",
      "\n",
      "  ----------------epoch:12,\t\tlr_w:0.0024619021200395215,\t\tlr_v:0.0024619021200395215----------------\n",
      "03/23 04:35:49 PM |\t  0.0%\n",
      "03/23 04:35:54 PM |\t  5.0%\n",
      "03/23 04:35:58 PM |\t  10.0%\n",
      "03/23 04:36:03 PM |\t  15.0%\n",
      "03/23 04:36:07 PM |\t  20.0%\n",
      "03/23 04:36:12 PM |\t  25.0%\n",
      "03/23 04:36:16 PM |\t  30.0%\n",
      "03/23 04:36:21 PM |\t  35.0%\n",
      "03/23 04:36:25 PM |\t  40.0%\n",
      "03/23 04:36:30 PM |\t  45.0%\n",
      "03/23 04:36:35 PM |\t  50.0%\n",
      "03/23 04:36:39 PM |\t  55.0%\n",
      "03/23 04:36:44 PM |\t  60.0%\n",
      "03/23 04:36:48 PM |\t  65.0%\n",
      "03/23 04:36:53 PM |\t  70.0%\n",
      "03/23 04:36:57 PM |\t  75.0%\n",
      "03/23 04:37:02 PM |\t  80.0%\n",
      "03/23 04:37:06 PM |\t  85.0%\n",
      "03/23 04:37:11 PM |\t  90.0%\n",
      "03/23 04:37:15 PM |\t  95.0%\n",
      "03/23 04:37:20 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:37:20 PM |\t  w_train_loss:1.1541170488926582,v_train_loss:0\n",
      "03/23 04:37:20 PM |\t  \n",
      "\n",
      "  ----------------epoch:13,\t\tlr_w:0.0023874306683599944,\t\tlr_v:0.0023874306683599944----------------\n",
      "03/23 04:37:21 PM |\t  0.0%\n",
      "03/23 04:37:26 PM |\t  5.0%\n",
      "03/23 04:37:31 PM |\t  10.0%\n",
      "03/23 04:37:35 PM |\t  15.0%\n",
      "03/23 04:37:40 PM |\t  20.0%\n",
      "03/23 04:37:44 PM |\t  25.0%\n",
      "03/23 04:37:49 PM |\t  30.0%\n",
      "03/23 04:37:53 PM |\t  35.0%\n",
      "03/23 04:37:58 PM |\t  40.0%\n",
      "03/23 04:38:03 PM |\t  45.0%\n",
      "03/23 04:38:07 PM |\t  50.0%\n",
      "03/23 04:38:12 PM |\t  55.0%\n",
      "03/23 04:38:16 PM |\t  60.0%\n",
      "03/23 04:38:21 PM |\t  65.0%\n",
      "03/23 04:38:25 PM |\t  70.0%\n",
      "03/23 04:38:30 PM |\t  75.0%\n",
      "03/23 04:38:35 PM |\t  80.0%\n",
      "03/23 04:38:39 PM |\t  85.0%\n",
      "03/23 04:38:44 PM |\t  90.0%\n",
      "03/23 04:38:48 PM |\t  95.0%\n",
      "03/23 04:38:53 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:38:53 PM |\t  w_train_loss:1.1466157769900747,v_train_loss:0\n",
      "03/23 04:38:53 PM |\t  \n",
      "\n",
      "  ----------------epoch:14,\t\tlr_w:0.002309479281909425,\t\tlr_v:0.002309479281909425----------------\n",
      "03/23 04:38:55 PM |\t  0.0%\n",
      "03/23 04:38:59 PM |\t  5.0%\n",
      "03/23 04:39:04 PM |\t  10.0%\n",
      "03/23 04:39:08 PM |\t  15.0%\n",
      "03/23 04:39:13 PM |\t  20.0%\n",
      "03/23 04:39:17 PM |\t  25.0%\n",
      "03/23 04:39:22 PM |\t  30.0%\n",
      "03/23 04:39:27 PM |\t  35.0%\n",
      "03/23 04:39:32 PM |\t  40.0%\n",
      "03/23 04:39:37 PM |\t  45.0%\n",
      "03/23 04:39:41 PM |\t  50.0%\n",
      "03/23 04:39:46 PM |\t  55.0%\n",
      "03/23 04:39:51 PM |\t  60.0%\n",
      "03/23 04:39:55 PM |\t  65.0%\n",
      "03/23 04:40:00 PM |\t  70.0%\n",
      "03/23 04:40:04 PM |\t  75.0%\n",
      "03/23 04:40:09 PM |\t  80.0%\n",
      "03/23 04:40:13 PM |\t  85.0%\n",
      "03/23 04:40:18 PM |\t  90.0%\n",
      "03/23 04:40:22 PM |\t  95.0%\n",
      "03/23 04:40:27 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:40:27 PM |\t  w_train_loss:1.142038333171513,v_train_loss:0\n",
      "03/23 04:40:27 PM |\t  \n",
      "\n",
      "  ----------------epoch:15,\t\tlr_w:0.0022283555003345325,\t\tlr_v:0.0022283555003345325----------------\n",
      "03/23 04:40:29 PM |\t  0.0%\n",
      "03/23 04:40:33 PM |\t  5.0%\n",
      "03/23 04:40:37 PM |\t  10.0%\n",
      "03/23 04:40:42 PM |\t  15.0%\n",
      "03/23 04:40:47 PM |\t  20.0%\n",
      "03/23 04:40:51 PM |\t  25.0%\n",
      "03/23 04:40:56 PM |\t  30.0%\n",
      "03/23 04:41:01 PM |\t  35.0%\n",
      "03/23 04:41:05 PM |\t  40.0%\n",
      "03/23 04:41:10 PM |\t  45.0%\n",
      "03/23 04:41:14 PM |\t  50.0%\n",
      "03/23 04:41:19 PM |\t  55.0%\n",
      "03/23 04:41:23 PM |\t  60.0%\n",
      "03/23 04:41:28 PM |\t  65.0%\n",
      "03/23 04:41:33 PM |\t  70.0%\n",
      "03/23 04:41:37 PM |\t  75.0%\n",
      "03/23 04:41:42 PM |\t  80.0%\n",
      "03/23 04:41:47 PM |\t  85.0%\n",
      "03/23 04:41:52 PM |\t  90.0%\n",
      "03/23 04:41:56 PM |\t  95.0%\n",
      "03/23 04:42:01 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:42:01 PM |\t  w_train_loss:1.1454790506977588,v_train_loss:0\n",
      "03/23 04:42:01 PM |\t  \n",
      "\n",
      "  ----------------epoch:16,\t\tlr_w:0.0021443793773725205,\t\tlr_v:0.0021443793773725205----------------\n",
      "03/23 04:42:03 PM |\t  0.0%\n",
      "03/23 04:42:07 PM |\t  5.0%\n",
      "03/23 04:42:12 PM |\t  10.0%\n",
      "03/23 04:42:17 PM |\t  15.0%\n",
      "03/23 04:42:21 PM |\t  20.0%\n",
      "03/23 04:42:26 PM |\t  25.0%\n",
      "03/23 04:42:30 PM |\t  30.0%\n",
      "03/23 04:42:35 PM |\t  35.0%\n",
      "03/23 04:42:40 PM |\t  40.0%\n",
      "03/23 04:42:44 PM |\t  45.0%\n",
      "03/23 04:42:49 PM |\t  50.0%\n",
      "03/23 04:42:53 PM |\t  55.0%\n",
      "03/23 04:42:58 PM |\t  60.0%\n",
      "03/23 04:43:03 PM |\t  65.0%\n",
      "03/23 04:43:07 PM |\t  70.0%\n",
      "03/23 04:43:12 PM |\t  75.0%\n",
      "03/23 04:43:16 PM |\t  80.0%\n",
      "03/23 04:43:21 PM |\t  85.0%\n",
      "03/23 04:43:26 PM |\t  90.0%\n",
      "03/23 04:43:30 PM |\t  95.0%\n",
      "03/23 04:43:35 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:43:35 PM |\t  w_train_loss:1.1457347201067023,v_train_loss:0\n",
      "03/23 04:43:35 PM |\t  \n",
      "\n",
      "  ----------------epoch:17,\t\tlr_w:0.002057882216850965,\t\tlr_v:0.002057882216850965----------------\n",
      "03/23 04:43:37 PM |\t  0.0%\n",
      "03/23 04:43:42 PM |\t  5.0%\n",
      "03/23 04:43:47 PM |\t  10.0%\n",
      "03/23 04:43:52 PM |\t  15.0%\n",
      "03/23 04:43:56 PM |\t  20.0%\n",
      "03/23 04:44:01 PM |\t  25.0%\n",
      "03/23 04:44:06 PM |\t  30.0%\n",
      "03/23 04:44:10 PM |\t  35.0%\n",
      "03/23 04:44:15 PM |\t  40.0%\n",
      "03/23 04:44:19 PM |\t  45.0%\n",
      "03/23 04:44:24 PM |\t  50.0%\n",
      "03/23 04:44:29 PM |\t  55.0%\n",
      "03/23 04:44:33 PM |\t  60.0%\n",
      "03/23 04:44:38 PM |\t  65.0%\n",
      "03/23 04:44:43 PM |\t  70.0%\n",
      "03/23 04:44:47 PM |\t  75.0%\n",
      "03/23 04:44:52 PM |\t  80.0%\n",
      "03/23 04:44:56 PM |\t  85.0%\n",
      "03/23 04:45:01 PM |\t  90.0%\n",
      "03/23 04:45:06 PM |\t  95.0%\n",
      "03/23 04:45:10 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/23 04:45:10 PM |\t  w_train_loss:1.1459420172614045,v_train_loss:0\n",
      "03/23 04:45:10 PM |\t  \n",
      "\n",
      "  ----------------epoch:18,\t\tlr_w:0.0019692052641436418,\t\tlr_v:0.0019692052641436418----------------\n",
      "03/23 04:45:12 PM |\t  0.0%\n",
      "03/23 04:45:17 PM |\t  5.0%\n",
      "03/23 04:45:22 PM |\t  10.0%\n",
      "03/23 04:45:26 PM |\t  15.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_80460/3139137489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_80460/2991365865.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mbatch_loss_w\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mw_trainloss_acc\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mloss_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;31m# nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mw_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin==1):\n",
    "#     my_test(valid_dataloader,model_w,-1) #before train\n",
    "#     my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "\n",
    "    if(epoch%10==0 and epoch!=0):\n",
    "        my_test(valid_dataloader,model_w,epoch) \n",
    "        my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "     \n",
    "   \n",
    "   \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
