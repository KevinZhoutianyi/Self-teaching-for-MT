{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer,AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length,target_language\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from os.path import exists\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 10, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 200, help='train data number')\n",
    "parser.add_argument('--test_num_points', type=int,              default = 50, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=16,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=4,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=0,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--num_workers', type=int,                  default=0,      help='num_workers')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='google/t5-small-lm-adapt',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='google/t5-small-lm-adapt',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='T5spec',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=50,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=80,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=500,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=10,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--beta1', type=float,                      default=0.9,    help='momentum')\n",
    "parser.add_argument('--beta2', type=float,                      default=0.98,    help='momentum')\n",
    "parser.add_argument('--warm', type=float,                       default=10,    help='warmup step')\n",
    "parser.add_argument('--num_step_lr', type=float,                default=1,    help='warmup step')\n",
    "parser.add_argument('--decay_lr', type=float,                   default=0.7,    help='warmup step')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.5,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.5,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=1 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "args.test_num = args.test_num//args.batch_size * args.batch_size\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5_scratch\\wandb\\run-20220617_165752-2qhyaovy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/2qhyaovy\" target=\"_blank\">T5spec</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/2qhyaovy?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1d7f327fdf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "wandb.init(project=\"Selftraining\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 04:57:57 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 32.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 04:57:57 PM |\t  Namespace(A_lr=100, batch_size=16, beta1=0.9, beta2=0.98, decay=0.001, decay_lr=0.7, epochs=500, exp_name='T5spec', gpu=0, grad_acc_count=-1, grad_clip=1, learning_rate_min=1e-08, model_name_student='google/t5-small-lm-adapt', model_name_teacher='google/t5-small-lm-adapt', num_step_lr=1, num_workers=0, pre_epochs=0, rep_num=48, syndata_loss_ratio=0.5, test_num=80, test_num_points=50, train_A=1, train_A_num_points=4, train_num_points=200, train_v_num_points=0, train_v_synthetic_num_points=8, train_w_num_points=4, traindata_loss_ratio=0.5, unrolled_v_lr=0.001, unrolled_w_lr=0.001, v_lr=0.001, valid_begin=1, valid_num_points=10, w_lr=0.001, warm=10)\n",
      "06/17 04:57:57 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "06/17 04:57:57 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 04:57:59 PM |\t  modelsize:76.961152MB\n",
      "06/17 04:58:00 PM |\t  modelsize:76.961152MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name_teacher\n",
    "pretrained  =  AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "\n",
    "if(exists(pathname+'.pt')==False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained,pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained  =  AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "if(exists(pathname+'.pt')==False):\n",
    "    logging.info(f'saving to {pathname}')\n",
    "    torch.save(pretrained,pathname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 04:58:02 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-fcff064badad2159.arrow\n",
      "06/17 04:58:02 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-ef861152e003e0c7.arrow\n",
      "06/17 04:58:02 PM |\t  Loading cached shuffled indices for dataset at C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2\\cache-848b25adc701ab2b.arrow\n",
      "06/17 04:58:02 PM |\t  train len: 192\n",
      "06/17 04:58:02 PM |\t  train_w_num_points_len: 48\n",
      "06/17 04:58:02 PM |\t  train_v_synthetic_num_points_len: 96\n",
      "06/17 04:58:02 PM |\t  train_v_num_points_len: 0\n",
      "06/17 04:58:02 PM |\t  train_A_num_points_len: 48\n",
      "06/17 04:58:02 PM |\t  valid len: 10\n",
      "06/17 04:58:02 PM |\t  test len: 50\n",
      "06/17 04:58:02 PM |\t  {'de': 'Dank unseres Personals und Geräteparks sind wir täglich rund um die Uhr in der Lage, uns allen erdenklichen Herausforderungen zu stellen.', 'en': 'translate English to German: Our staff and equipment stands ready to answer any challenges 24/7.'}\n",
      "06/17 04:58:02 PM |\t  {'de': 'Diese Entscheidung rief in der Öffentlichkeit eine lebhafte Diskussion hervor.', 'en': 'translate English to German: The resolution caused lively public debate.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess the data, make a dataloader\n",
    "import random\n",
    "modelname = args.model_name_teacher\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#teacher shouldn't have label smoothing, especially when student got same size.\n",
    "criterion_v = torch.nn.CrossEntropyLoss( reduction='none')#,label_smoothing=args.smoothing) #without LS, V may be too confident to that syn data, and LS do well for real data also.\n",
    "\n",
    "\n",
    "\n",
    "train = dataset['train'].shuffle(seed=seed_).select(range(args.train_num_points))\n",
    "valid = dataset['validation'].shuffle(seed=seed_).select(range(args.valid_num_points))\n",
    "test = dataset['test'].shuffle(seed=seed_).select(range(args.test_num_points))#[L_t+L_v:L_t+L_v+L_test]\n",
    "train = train['translation']\n",
    "valid = valid['translation']\n",
    "test = test['translation']\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en']  #needed for T5\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "#TODO: Syn_input should be monolingual data, should try en-fo's en. cuz wmt may align\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "\n",
    "'''\n",
    "each mini batch consist of : \n",
    "1. data to train W\n",
    "2. monolingual data to generate parallel data\n",
    "3. data to train V\n",
    "4. data to train A\n",
    "'''\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "# logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "06/17 04:58:02 PM |\t  train data get\n",
      "06/17 04:58:02 PM |\t  train data loader get\n",
      "06/17 04:58:02 PM |\t  valid data loader get\n",
      "06/17 04:58:02 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=args.batch_size, pin_memory=args.num_workers>0, num_workers=args.num_workers)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(),  lr= args.w_lr ,  betas=(args.beta1, args.beta2) ,eps=1e-9 )\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_w  =   StepLR(w_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_w  = Scheduler(w_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.w_lr)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion_v, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(),  lr= args.v_lr ,  betas=(args.beta1,args.beta2) ,eps=1e-9  )\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_v  =   StepLR(v_optimizer, step_size=args.num_step_lr, gamma=args.decay_lr)\n",
    "# scheduler_v  = Scheduler(v_optimizer,dim_embed=512, warmup_steps=args.warm, initlr = args.v_lr)\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    # metric_bleu =  load_metric('bleu')\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        \n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)[:args.train_w_num_points]\n",
    "        ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        pre = model.generate(test_dataloaderx)\n",
    "        x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "        pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "        label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "        \n",
    "        pred_str = [x  for x in pred_decoded]\n",
    "        label_str = [[x] for x in label_decoded]\n",
    "        # pred_list = [x.split()  for x in pred_decoded]\n",
    "        # label_list = [[x.split()] for x in label_decoded]\n",
    "        metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "        # metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "        if  step==0:\n",
    "            logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "            logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "            logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            \n",
    "            \n",
    "    logging.info('computing score...') \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    # bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    # logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    # del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter):\n",
    "\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.train_w_num_points\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points\n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size = [wsize, synsize, vsize, Asize]\n",
    "    bs = args.batch_size\n",
    "    w_model.eval()\n",
    "    v_model.eval()\n",
    "\n",
    "    logging.info(f\"split size:{split_size}\")\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        tot_iter[0] += bs\n",
    "        \n",
    "\n",
    "        # logging.info(f\"GPU mem :{getGPUMem(device)}%\")\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        (input_w, input_syn, input_v, input_A_v) = torch.split(train_x, split_size)\n",
    "        (input_w_attn, input_syn_attn, input_v_attn,\n",
    "         input_A_v_attn) = torch.split(train_x_attn, split_size)\n",
    "        (output_w, _, output_v, output_A_v) = torch.split(train_y, split_size)\n",
    "        (output_w_attn, _, output_v_attn, output_A_v_attn) = torch.split(\n",
    "            train_y_attn, split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "        if(True):# let v train on syn data and w data\n",
    "            input_v = input_w\n",
    "            input_v_attn = input_w_attn\n",
    "            output_v = output_w\n",
    "            output_v_attn = output_w_attn\n",
    "            vsize = wsize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if (args.train_A == 1):\n",
    "            epsilon_w = args.unrolled_w_lr\n",
    "            epsilon_v  = args.unrolled_v_lr\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, output_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, output_v_attn, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, output_A_v_attn, v_optimizer,\n",
    "                                             attn_idx, epsilon_w, epsilon_v)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                          output_w_attn, attn_idx, A, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "\n",
    "\n",
    "        v_optimizer.zero_grad()\n",
    "        loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "\n",
    "        loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                        output_v_attn, v_model)\n",
    "        v_loss = (args.traindata_loss_ratio*loss +\n",
    "                  loss_aug*args.syndata_loss_ratio)\n",
    "        v_trainloss_acc += v_loss.item()\n",
    "        v_loss.backward()\n",
    "        objs_v_syn.update(loss_aug.item(), synsize)\n",
    "        objs_v_train.update(loss.item(), vsize)\n",
    "        v_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valloss = my_loss2(input_A_v, input_A_v_attn,  output_A_v, output_A_v_attn,v_model)\n",
    "            objs_v_val.update(valloss.item(), Asize)\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            my_test(validdataloader, model_w, epoch)\n",
    "            my_test(validdataloader, model_v, epoch)\n",
    "            logging.info(str((\"Attention Weights A : \", A.ReLU(A.alpha))))\n",
    "            torch.save(model_w,'./model/'+'model_w.pt')#+now+\n",
    "            torch.save(model_v,'./model/'+'model_v.pt')\n",
    "            torch.save(A,'./model/'+'A.pt')\n",
    "            torch.save(model_w,os.path.join(wandb.run.dir, \"model_w.pt\"))\n",
    "            torch.save(model_v,os.path.join(wandb.run.dir, \"model_v.pt\"))\n",
    "            torch.save(A,os.path.join(wandb.run.dir, \"A.pt\"))\n",
    "            wandb.save(\"./files/*.pt\", base_path=\"./files\", policy=\"live\")\n",
    "\n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info(f\"{progress:5.3}%:\\t  W_train_loss:{objs_w.avg:^.7f}\\tV_train_syn_loss:{objs_v_syn.avg:^.7f}\\tV_train_loss:{objs_v_train.avg:^.7f}\\t  V_star_val_loss:{objs_v_star_val.avg:^.7f}\\t  V_val_loss:{objs_v_val.avg:^.7f}\")\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_val.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "\n",
    "    return w_trainloss_acc, v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/17 04:59:58 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.00049,\t\tlr_v:0.00049,\t\tlr_A:49.0----------------\n",
      "06/17 04:59:58 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:00:16 PM |\t   18.2%:\t  W_train_loss:2.1449904\tV_train_syn_loss:0.8056690\tV_train_loss:2.7722190\t  V_star_val_loss:4.7613775\t  V_val_loss:4.7406042\n",
      "06/17 05:00:31 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:00:31 PM |\t  pred_decoded[:2]:['Die Diem der Psychiatrist ist die Psychiatrist die Psychiatrist die Psychiatrist die Psychiatrist die Psychiatrist die Psychiatrist die Psychiatrist die Psych', 'Die city der Stadt ist in ancient Greek and ancient Egyptian mythologies.']\n",
      "06/17 05:00:31 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:00:31 PM |\t  computing score...\n",
      "06/17 05:00:31 PM |\t  model_w_in_main sacreBLEU : 0.858502\n",
      "06/17 05:00:31 PM |\t  model_w_in_main test loss : 5.156733\n",
      "06/17 05:00:32 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:00:32 PM |\t  pred_decoded[:2]:['Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die', 'Die city is a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city a city']\n",
      "06/17 05:00:32 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:00:32 PM |\t  computing score...\n",
      "06/17 05:00:32 PM |\t  model_v_in_main sacreBLEU : 0.243946\n",
      "06/17 05:00:32 PM |\t  model_v_in_main test loss : 5.249089\n",
      "06/17 05:00:32 PM |\t  ('Attention Weights A : ', tensor([0.8238, 0.9035, 0.8618, 0.8927, 0.9274, 0.8122, 0.7873, 0.7512, 0.9767,\n",
      "        0.9089, 0.8800, 0.7907, 0.9652, 1.0211, 0.9970, 0.9552, 0.9754, 1.0398,\n",
      "        0.9955, 1.0586, 1.0131, 0.6951, 0.9663, 1.0374, 1.0141, 1.0075, 1.0104,\n",
      "        1.0003, 1.0056, 0.9785, 0.9832, 0.9940, 1.0455, 1.0393, 0.8612, 1.0048,\n",
      "        1.0287, 0.9972, 1.0016, 0.9971, 0.9807, 1.0571, 0.9922, 1.0706, 1.0064,\n",
      "        1.0039, 1.0013, 0.9936], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:00:41 PM |\t   45.5%:\t  W_train_loss:3.2449419\tV_train_syn_loss:0.8485163\tV_train_loss:3.5412315\t  V_star_val_loss:4.7036574\t  V_val_loss:4.6886980\n",
      "06/17 05:00:59 PM |\t   72.7%:\t  W_train_loss:3.2299107\tV_train_syn_loss:0.9774304\tV_train_loss:3.4276892\t  V_star_val_loss:4.6939108\t  V_val_loss:4.6824355\n",
      "06/17 05:01:07 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:01:07 PM |\t  pred_decoded[:2]:['Die Menschen sind nicht mehr wichtig, dass die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, warum die Menschen nicht mehr wichtig, ', 'Die city der Stadt ist in ancient Greek and ancient Egyptian mythologies.']\n",
      "06/17 05:01:07 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:01:07 PM |\t  computing score...\n",
      "06/17 05:01:07 PM |\t  model_w_in_main sacreBLEU : 1.628354\n",
      "06/17 05:01:07 PM |\t  model_w_in_main test loss : 5.243062\n",
      "06/17 05:01:09 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:01:09 PM |\t  pred_decoded[:2]:['Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die', 'Die Stadt ist der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt der Stadt']\n",
      "06/17 05:01:09 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:01:09 PM |\t  computing score...\n",
      "06/17 05:01:09 PM |\t  model_v_in_main sacreBLEU : 0.328958\n",
      "06/17 05:01:09 PM |\t  model_v_in_main test loss : 5.207841\n",
      "06/17 05:01:09 PM |\t  ('Attention Weights A : ', tensor([0.8238, 0.9035, 0.8618, 0.8927, 0.9274, 0.8122, 0.7873, 0.7512, 0.9767,\n",
      "        0.9089, 0.8800, 0.7907, 0.9652, 1.0211, 0.9970, 0.9552, 0.9754, 1.0398,\n",
      "        0.9955, 1.0586, 0.9378, 0.5334, 0.9846, 1.0640, 0.9745, 1.0594, 1.0620,\n",
      "        1.0190, 1.1114, 0.9674, 0.9979, 1.0154, 1.1639, 1.2116, 0.9669, 0.9823,\n",
      "        1.2012, 1.1184, 1.0839, 0.9088, 0.9807, 1.0571, 0.9922, 1.0706, 1.0064,\n",
      "        1.0039, 1.0013, 0.9936], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:01:24 PM |\t  1e+02%:\t  W_train_loss:3.4800168\tV_train_syn_loss:0.8350540\tV_train_loss:3.3424060\t  V_star_val_loss:5.0539460\t  V_val_loss:5.0628161\n",
      "06/17 05:01:24 PM |\t  w_train_loss:36.29957938194275,v_train_loss:24.8253231048584\n",
      "06/17 05:01:24 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.000343,\t\tlr_v:0.000343,\t\tlr_A:34.3----------------\n",
      "06/17 05:01:24 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:01:43 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:01:43 PM |\t  pred_decoded[:2]:['Die Die Aussprache ist es es es es es es es es es es es es es es es es es es es ', 'Die Stadt ist in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:01:43 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:01:43 PM |\t  computing score...\n",
      "06/17 05:01:43 PM |\t  model_w_in_main sacreBLEU : 2.495131\n",
      "06/17 05:01:43 PM |\t  model_w_in_main test loss : 5.189442\n",
      "06/17 05:01:45 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:01:45 PM |\t  pred_decoded[:2]:['Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die Die', 'Die Stadt ist in ancient Greek and ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek and Ancient Greek']\n",
      "06/17 05:01:45 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:01:45 PM |\t  computing score...\n",
      "06/17 05:01:45 PM |\t  model_v_in_main sacreBLEU : 0.364052\n",
      "06/17 05:01:45 PM |\t  model_v_in_main test loss : 5.152539\n",
      "06/17 05:01:45 PM |\t  ('Attention Weights A : ', tensor([0.6923, 0.8474, 0.7795, 0.7960, 0.8804, 0.7550, 0.7870, 0.6650, 0.9344,\n",
      "        0.8646, 0.8384, 0.7600, 0.9652, 1.0211, 0.9970, 0.9552, 0.9754, 1.0398,\n",
      "        0.9955, 1.0586, 0.9378, 0.5334, 0.9846, 1.0640, 0.9745, 1.0594, 1.0620,\n",
      "        1.0190, 1.1114, 0.9674, 0.9979, 1.0154, 1.1639, 1.2116, 0.9669, 0.9823,\n",
      "        1.2012, 1.1184, 1.0839, 0.9088, 0.9264, 1.0695, 0.9907, 1.1544, 0.9748,\n",
      "        1.5850, 1.1018, 1.0655], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:01:48 PM |\t   18.2%:\t  W_train_loss:1.1865324\tV_train_syn_loss:0.8344245\tV_train_loss:1.7455494\t  V_star_val_loss:4.8518535\t  V_val_loss:4.8326483\n",
      "06/17 05:02:06 PM |\t   45.5%:\t  W_train_loss:1.9464562\tV_train_syn_loss:1.2224011\tV_train_loss:2.3564716\t  V_star_val_loss:4.6691834\t  V_val_loss:4.6311462\n",
      "06/17 05:02:20 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:20 PM |\t  pred_decoded[:2]:['Die Die Auschwitz der Psychiatry der Psychiatry ist, die Psychiatry der Psychiatry ist, die Psychiatry ist.', 'Es ist eingar das city der Stadt eingar das Speicherstadt ist.']\n",
      "06/17 05:02:20 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:02:20 PM |\t  computing score...\n",
      "06/17 05:02:20 PM |\t  model_w_in_main sacreBLEU : 1.798079\n",
      "06/17 05:02:20 PM |\t  model_w_in_main test loss : 5.511564\n",
      "06/17 05:02:22 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:22 PM |\t  pred_decoded[:2]:['Die Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psych', 'Das ist ist in Ancient Greek and Ancient Egyptian Symbols.']\n",
      "06/17 05:02:22 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:02:22 PM |\t  computing score...\n",
      "06/17 05:02:22 PM |\t  model_v_in_main sacreBLEU : 0.507302\n",
      "06/17 05:02:22 PM |\t  model_v_in_main test loss : 5.319671\n",
      "06/17 05:02:22 PM |\t  ('Attention Weights A : ', tensor([0.6923, 0.8474, 0.7795, 0.7960, 0.8804, 0.7550, 0.7870, 0.6650, 0.9344,\n",
      "        0.8646, 0.8384, 0.7600, 0.9522, 1.0102, 0.9895, 0.9419, 0.9345, 1.0103,\n",
      "        0.9753, 0.9985, 0.9082, 0.4783, 0.9375, 1.0913, 0.9679, 1.0582, 1.0492,\n",
      "        1.0143, 1.0913, 0.9196, 0.9788, 0.9918, 1.1639, 1.2116, 0.9669, 0.9823,\n",
      "        1.2012, 1.1184, 1.0839, 0.9088, 0.9264, 1.0695, 0.9907, 1.1544, 0.9748,\n",
      "        1.5850, 1.1018, 1.0655], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:02:31 PM |\t   72.7%:\t  W_train_loss:1.9911844\tV_train_syn_loss:1.0719371\tV_train_loss:2.3816795\t  V_star_val_loss:4.9409436\t  V_val_loss:4.8902356\n",
      "06/17 05:02:49 PM |\t  1e+02%:\t  W_train_loss:2.0092245\tV_train_syn_loss:1.2542049\tV_train_loss:2.0962628\t  V_star_val_loss:5.7195288\t  V_val_loss:5.6121141\n",
      "06/17 05:02:49 PM |\t  w_train_loss:21.400192260742188,v_train_loss:19.44439661502838\n",
      "06/17 05:02:49 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:0.00024009999999999998,\t\tlr_v:0.00024009999999999998,\t\tlr_A:24.009999999999998----------------\n",
      "06/17 05:02:49 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:02:57 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:57 PM |\t  pred_decoded[:2]:['Die Auschwitz ist alle eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen er', 'Es gibt das city befindet sich in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:57 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:02:57 PM |\t  computing score...\n",
      "06/17 05:02:57 PM |\t  model_w_in_main sacreBLEU : 2.416851\n",
      "06/17 05:02:57 PM |\t  model_w_in_main test loss : 6.002359\n",
      "06/17 05:02:59 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:59 PM |\t  pred_decoded[:2]:['Die Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die  Die ', 'Es ist das city befindet sich in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:02:59 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:02:59 PM |\t  computing score...\n",
      "06/17 05:02:59 PM |\t  model_v_in_main sacreBLEU : 0.461438\n",
      "06/17 05:02:59 PM |\t  model_v_in_main test loss : 5.616797\n",
      "06/17 05:02:59 PM |\t  ('Attention Weights A : ', tensor([0.6430, 0.8315, 0.7467, 0.7610, 0.8804, 0.7550, 0.7870, 0.6650, 0.9344,\n",
      "        0.8646, 0.8384, 0.7600, 0.9522, 1.0102, 0.9895, 0.9419, 0.9345, 1.0103,\n",
      "        0.9753, 0.9985, 0.9082, 0.4783, 0.9375, 1.0913, 0.9679, 1.0582, 1.0492,\n",
      "        1.0143, 1.0913, 0.9196, 0.9788, 0.9918, 1.1278, 1.2355, 0.9011, 0.9189,\n",
      "        1.1878, 1.0661, 1.0277, 0.8515, 0.9100, 1.0542, 0.9841, 1.1212, 0.8724,\n",
      "        1.6406, 1.1683, 1.0520], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:03:14 PM |\t   18.2%:\t  W_train_loss:0.6627846\tV_train_syn_loss:1.1865447\tV_train_loss:1.1047394\t  V_star_val_loss:5.5879358\t  V_val_loss:5.4774814\n",
      "06/17 05:03:34 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:03:34 PM |\t  pred_decoded[:2]:['Die Auschwitz ist alle eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen er', 'Es ist alle beliebte Stadt gefunden haben.']\n",
      "06/17 05:03:34 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:03:34 PM |\t  computing score...\n",
      "06/17 05:03:34 PM |\t  model_w_in_main sacreBLEU : 2.994950\n",
      "06/17 05:03:34 PM |\t  model_w_in_main test loss : 6.342135\n",
      "06/17 05:03:36 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:03:36 PM |\t  pred_decoded[:2]:['Es gibt verschiedene Therapies aus seinen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen eigenen', 'Es gibt einschließlich alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebtes alle beliebte']\n",
      "06/17 05:03:36 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:03:36 PM |\t  computing score...\n",
      "06/17 05:03:36 PM |\t  model_v_in_main sacreBLEU : 1.781739\n",
      "06/17 05:03:36 PM |\t  model_v_in_main test loss : 5.791107\n",
      "06/17 05:03:36 PM |\t  ('Attention Weights A : ', tensor([0.6430, 0.8315, 0.7467, 0.7610, 0.8843, 0.7207, 0.8005, 0.6629, 0.9283,\n",
      "        0.8524, 0.8435, 0.7576, 0.9458, 0.9983, 0.9782, 0.9442, 0.9684, 1.0102,\n",
      "        0.9537, 0.9372, 0.9004, 0.4816, 0.9337, 1.1126, 0.9679, 1.0582, 1.0492,\n",
      "        1.0143, 1.0913, 0.9196, 0.9788, 0.9918, 1.1278, 1.2355, 0.9011, 0.9189,\n",
      "        1.1878, 1.0661, 1.0277, 0.8515, 0.9100, 1.0542, 0.9841, 1.1212, 0.8724,\n",
      "        1.6406, 1.1683, 1.0520], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:03:38 PM |\t   45.5%:\t  W_train_loss:1.1431306\tV_train_syn_loss:1.1462006\tV_train_loss:1.5520778\t  V_star_val_loss:5.2395983\t  V_val_loss:5.1610374\n",
      "06/17 05:03:56 PM |\t   72.7%:\t  W_train_loss:1.1037055\tV_train_syn_loss:1.0640195\tV_train_loss:1.5713196\t  V_star_val_loss:5.5388892\t  V_val_loss:5.3981032\n",
      "06/17 05:04:10 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:10 PM |\t  pred_decoded[:2]:['Die Austen ist so beliebten Sie man sich selbst zu verstehen, die uns zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu halt zu ', 'Ich möchte Sie eingar georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian ge']\n",
      "06/17 05:04:10 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:04:10 PM |\t  computing score...\n",
      "06/17 05:04:10 PM |\t  model_w_in_main sacreBLEU : 1.808008\n",
      "06/17 05:04:10 PM |\t  model_w_in_main test loss : 6.641009\n",
      "06/17 05:04:12 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:12 PM |\t  pred_decoded[:2]:['Das bedeutet, dass in its extreme it can lead people to take absurd physical risks, gamble or indulge in adequacy as a way to ease it, die er, die er, die er, die er, die er, die er, die er, die', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:12 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:04:12 PM |\t  computing score...\n",
      "06/17 05:04:12 PM |\t  model_v_in_main sacreBLEU : 1.313513\n",
      "06/17 05:04:12 PM |\t  model_v_in_main test loss : 6.013010\n",
      "06/17 05:04:12 PM |\t  ('Attention Weights A : ', tensor([0.6430, 0.8315, 0.7467, 0.7610, 0.8843, 0.7207, 0.8005, 0.6629, 0.9283,\n",
      "        0.8524, 0.8435, 0.7576, 0.9458, 0.9983, 0.9782, 0.9442, 0.9684, 1.0102,\n",
      "        0.9537, 0.9372, 0.9004, 0.4816, 0.9337, 1.1126, 0.9647, 1.0603, 1.0439,\n",
      "        1.0022, 1.0812, 0.8936, 0.9576, 0.9805, 1.1015, 1.2238, 0.8983, 0.9169,\n",
      "        1.1913, 1.0578, 1.0227, 0.8474, 0.9118, 1.0492, 0.9819, 1.1135, 0.8724,\n",
      "        1.6406, 1.1683, 1.0520], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:04:21 PM |\t  1e+02%:\t  W_train_loss:0.9955930\tV_train_syn_loss:1.4961919\tV_train_loss:1.2425944\t  V_star_val_loss:6.4686575\t  V_val_loss:6.3204835\n",
      "06/17 05:04:21 PM |\t  w_train_loss:11.715641170740128,v_train_loss:15.545531868934631\n",
      "06/17 05:04:21 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:0.00016806999999999998,\t\tlr_v:0.00016806999999999998,\t\tlr_A:16.807----------------\n",
      "06/17 05:04:21 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:04:38 PM |\t   18.2%:\t  W_train_loss:0.3191483\tV_train_syn_loss:1.3721112\tV_train_loss:0.6724229\t  V_star_val_loss:5.8131116\t  V_val_loss:5.8426352\n",
      "06/17 05:04:45 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:45 PM |\t  pred_decoded[:2]:['Die Auschwitz ist alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspirierend alle inspir', 'Ich möchte Sie eingar georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian georgian ge']\n",
      "06/17 05:04:45 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:04:45 PM |\t  computing score...\n",
      "06/17 05:04:45 PM |\t  model_w_in_main sacreBLEU : 1.763136\n",
      "06/17 05:04:45 PM |\t  model_w_in_main test loss : 6.894361\n",
      "06/17 05:04:46 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:46 PM |\t  pred_decoded[:2]:['Die Auschwitz ist alle eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen eruvlichen er', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:04:46 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:04:46 PM |\t  computing score...\n",
      "06/17 05:04:46 PM |\t  model_v_in_main sacreBLEU : 1.914603\n",
      "06/17 05:04:46 PM |\t  model_v_in_main test loss : 5.993443\n",
      "06/17 05:04:46 PM |\t  ('Attention Weights A : ', tensor([0.6326, 0.8326, 0.7551, 0.7415, 0.8856, 0.7134, 0.8056, 0.6665, 0.9203,\n",
      "        0.8424, 0.8398, 0.7513, 0.9462, 1.0003, 0.9674, 0.9460, 0.9684, 1.0102,\n",
      "        0.9537, 0.9372, 0.9004, 0.4816, 0.9337, 1.1126, 0.9647, 1.0603, 1.0439,\n",
      "        1.0022, 1.0812, 0.8936, 0.9576, 0.9805, 1.1015, 1.2238, 0.8983, 0.9169,\n",
      "        1.1913, 1.0578, 1.0227, 0.8474, 0.9118, 1.0492, 0.9819, 1.1135, 0.8562,\n",
      "        1.6433, 1.1845, 1.0625], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:05:00 PM |\t   45.5%:\t  W_train_loss:0.6068302\tV_train_syn_loss:1.6207306\tV_train_loss:1.0224345\t  V_star_val_loss:5.4402499\t  V_val_loss:5.3871258\n",
      "06/17 05:05:20 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:20 PM |\t  pred_decoded[:2]:['Die Auschwitz ist Herbs und Herbs der Herbs, die researchers show.', 'Das ist bei Ancient Greek and Ancient Egyptian legends das city alle beliebte alle beliebte Herz.']\n",
      "06/17 05:05:20 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:05:20 PM |\t  computing score...\n",
      "06/17 05:05:20 PM |\t  model_w_in_main sacreBLEU : 1.981326\n",
      "06/17 05:05:20 PM |\t  model_w_in_main test loss : 7.144900\n",
      "06/17 05:05:22 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:22 PM |\t  pred_decoded[:2]:['Die Auschwitz bei Patienten mit dem Weg, die Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psych', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:22 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:05:22 PM |\t  computing score...\n",
      "06/17 05:05:22 PM |\t  model_v_in_main sacreBLEU : 1.284560\n",
      "06/17 05:05:22 PM |\t  model_v_in_main test loss : 6.217361\n",
      "06/17 05:05:22 PM |\t  ('Attention Weights A : ', tensor([0.6326, 0.8326, 0.7551, 0.7415, 0.8856, 0.7134, 0.8056, 0.6665, 0.9203,\n",
      "        0.8424, 0.8398, 0.7513, 0.9462, 1.0003, 0.9674, 0.9460, 0.9546, 1.0185,\n",
      "        0.9366, 0.9406, 0.9010, 0.4824, 0.9288, 1.1154, 0.9668, 1.0611, 1.0424,\n",
      "        0.9976, 1.0780, 0.8821, 0.9516, 0.9797, 1.0925, 1.2285, 0.8908, 0.9124,\n",
      "        1.1913, 1.0578, 1.0227, 0.8474, 0.9118, 1.0492, 0.9819, 1.1135, 0.8562,\n",
      "        1.6433, 1.1845, 1.0625], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:05:25 PM |\t   72.7%:\t  W_train_loss:0.5884184\tV_train_syn_loss:1.2346704\tV_train_loss:1.0502705\t  V_star_val_loss:5.8758688\t  V_val_loss:5.7066325\n",
      "06/17 05:05:42 PM |\t  1e+02%:\t  W_train_loss:0.4590456\tV_train_syn_loss:1.3055685\tV_train_loss:0.8143018\t  V_star_val_loss:7.0786371\t  V_val_loss:6.7411933\n",
      "06/17 05:05:42 PM |\t  w_train_loss:5.920327454805374,v_train_loss:13.638765811920166\n",
      "06/17 05:05:42 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:0.00011764899999999998,\t\tlr_v:0.00011764899999999998,\t\tlr_A:11.764899999999999----------------\n",
      "06/17 05:05:42 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:05:55 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:55 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, dass Sie in its utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost ut', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:05:55 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:05:55 PM |\t  computing score...\n",
      "06/17 05:05:55 PM |\t  model_w_in_main sacreBLEU : 1.809942\n",
      "06/17 05:05:55 PM |\t  model_w_in_main test loss : 7.333084\n",
      "06/17 05:05:57 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:57 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Psychiatry und Psychiatry und Psychiatry und Psychiatry und Psychiatry und Psychiatry und Psychiatry und Psychiatry und Psychiatry und ', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:05:57 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:05:57 PM |\t  computing score...\n",
      "06/17 05:05:57 PM |\t  model_v_in_main sacreBLEU : 1.283719\n",
      "06/17 05:05:57 PM |\t  model_v_in_main test loss : 6.484226\n",
      "06/17 05:05:57 PM |\t  ('Attention Weights A : ', tensor([0.6313, 0.8278, 0.7544, 0.7493, 0.8865, 0.7123, 0.8065, 0.6668, 0.9203,\n",
      "        0.8424, 0.8398, 0.7513, 0.9462, 1.0003, 0.9674, 0.9460, 0.9546, 1.0185,\n",
      "        0.9366, 0.9406, 0.9010, 0.4824, 0.9288, 1.1154, 0.9668, 1.0611, 1.0424,\n",
      "        0.9976, 1.0780, 0.8821, 0.9516, 0.9797, 1.0925, 1.2285, 0.8908, 0.9124,\n",
      "        1.1901, 1.0576, 1.0200, 0.8452, 0.9264, 1.0495, 0.9790, 1.1184, 0.8575,\n",
      "        1.6300, 1.1847, 1.0715], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:06:07 PM |\t   18.2%:\t  W_train_loss:0.1564895\tV_train_syn_loss:1.7066971\tV_train_loss:0.4146382\t  V_star_val_loss:6.4532708\t  V_val_loss:6.3111680\n",
      "06/17 05:06:24 PM |\t   45.5%:\t  W_train_loss:0.3329608\tV_train_syn_loss:1.7004326\tV_train_loss:0.7304145\t  V_star_val_loss:5.8057706\t  V_val_loss:5.7968977\n",
      "06/17 05:06:31 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:06:31 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, dass Sie in its utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost ut', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:06:31 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:06:31 PM |\t  computing score...\n",
      "06/17 05:06:31 PM |\t  model_w_in_main sacreBLEU : 1.760042\n",
      "06/17 05:06:31 PM |\t  model_w_in_main test loss : 7.485651\n",
      "06/17 05:06:33 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:06:33 PM |\t  pred_decoded[:2]:['Diemist ist somber in der Lage, die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage die in der Lage', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:06:33 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:06:33 PM |\t  computing score...\n",
      "06/17 05:06:33 PM |\t  model_v_in_main sacreBLEU : 1.210550\n",
      "06/17 05:06:33 PM |\t  model_v_in_main test loss : 6.498376\n",
      "06/17 05:06:33 PM |\t  ('Attention Weights A : ', tensor([0.6313, 0.8278, 0.7544, 0.7493, 0.8865, 0.7123, 0.8065, 0.6668, 0.9195,\n",
      "        0.8437, 0.8406, 0.7509, 0.9471, 1.0000, 0.9674, 0.9464, 0.9553, 1.0214,\n",
      "        0.9339, 0.9347, 0.9012, 0.4824, 0.9272, 1.1112, 0.9673, 1.0617, 1.0425,\n",
      "        0.9977, 1.0780, 0.8821, 0.9516, 0.9797, 1.0925, 1.2285, 0.8908, 0.9124,\n",
      "        1.1901, 1.0576, 1.0200, 0.8452, 0.9264, 1.0495, 0.9790, 1.1184, 0.8575,\n",
      "        1.6300, 1.1847, 1.0715], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:06:48 PM |\t   72.7%:\t  W_train_loss:0.3196937\tV_train_syn_loss:1.7888863\tV_train_loss:0.7590040\t  V_star_val_loss:5.9925202\t  V_val_loss:5.9934525\n",
      "06/17 05:07:06 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:06 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, dass Sie in its utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost ut', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:07:06 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:07:06 PM |\t  computing score...\n",
      "06/17 05:07:06 PM |\t  model_w_in_main sacreBLEU : 1.565148\n",
      "06/17 05:07:06 PM |\t  model_w_in_main test loss : 7.589240\n",
      "06/17 05:07:08 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:08 PM |\t  pred_decoded[:2]:['Diemist ist somber in der Lage, die in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:08 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:07:08 PM |\t  computing score...\n",
      "06/17 05:07:08 PM |\t  model_v_in_main sacreBLEU : 0.841687\n",
      "06/17 05:07:08 PM |\t  model_v_in_main test loss : 6.465973\n",
      "06/17 05:07:08 PM |\t  ('Attention Weights A : ', tensor([0.6313, 0.8278, 0.7544, 0.7493, 0.8865, 0.7123, 0.8065, 0.6668, 0.9195,\n",
      "        0.8437, 0.8406, 0.7509, 0.9471, 1.0000, 0.9674, 0.9464, 0.9553, 1.0214,\n",
      "        0.9339, 0.9347, 0.9012, 0.4824, 0.9272, 1.1112, 0.9673, 1.0617, 1.0425,\n",
      "        0.9977, 1.0748, 0.8817, 0.9521, 0.9792, 1.0772, 1.2295, 0.8890, 0.9109,\n",
      "        1.1891, 1.0574, 1.0191, 0.8448, 0.9275, 1.0493, 0.9793, 1.1181, 0.8566,\n",
      "        1.6299, 1.1864, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:07:10 PM |\t  1e+02%:\t  W_train_loss:0.2271642\tV_train_syn_loss:1.6365044\tV_train_loss:0.5807156\t  V_star_val_loss:6.8694701\t  V_val_loss:6.8499839\n",
      "06/17 05:07:10 PM |\t  w_train_loss:3.108924590051174,v_train_loss:13.975939154624939\n",
      "06/17 05:07:10 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:8.235429999999999e-05,\t\tlr_v:8.235429999999999e-05,\t\tlr_A:8.23543----------------\n",
      "06/17 05:07:10 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:07:27 PM |\t   18.2%:\t  W_train_loss:0.0946257\tV_train_syn_loss:1.7351367\tV_train_loss:0.3148938\t  V_star_val_loss:6.3828246\t  V_val_loss:6.3096561\n",
      "06/17 05:07:40 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:40 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, dass Sie in its utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost utmost ut', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:07:40 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:07:40 PM |\t  computing score...\n",
      "06/17 05:07:40 PM |\t  model_w_in_main sacreBLEU : 2.359750\n",
      "06/17 05:07:40 PM |\t  model_w_in_main test loss : 7.635140\n",
      "06/17 05:07:42 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:42 PM |\t  pred_decoded[:2]:['Dien, die in its extreme nachdemort, die in its extreme nachdemort, die in its extreme nachdemort, die in its extreme nachdemort, die in its extreme nachdemort, die in its extreme nachdemort, die in its extreme nachdem', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:07:42 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:07:42 PM |\t  computing score...\n",
      "06/17 05:07:42 PM |\t  model_v_in_main sacreBLEU : 1.056522\n",
      "06/17 05:07:42 PM |\t  model_v_in_main test loss : 6.429450\n",
      "06/17 05:07:42 PM |\t  ('Attention Weights A : ', tensor([0.6310, 0.8277, 0.7538, 0.7501, 0.8867, 0.7119, 0.8069, 0.6672, 0.9197,\n",
      "        0.8432, 0.8400, 0.7506, 0.9469, 1.0000, 0.9671, 0.9467, 0.9554, 1.0243,\n",
      "        0.9362, 0.9359, 0.9012, 0.4824, 0.9272, 1.1112, 0.9673, 1.0617, 1.0425,\n",
      "        0.9977, 1.0748, 0.8817, 0.9521, 0.9792, 1.0772, 1.2295, 0.8890, 0.9109,\n",
      "        1.1891, 1.0574, 1.0191, 0.8448, 0.9275, 1.0493, 0.9793, 1.1181, 0.8566,\n",
      "        1.6299, 1.1864, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:07:51 PM |\t   45.5%:\t  W_train_loss:0.2021298\tV_train_syn_loss:1.5651970\tV_train_loss:0.5751113\t  V_star_val_loss:5.8569857\t  V_val_loss:5.7520299\n",
      "06/17 05:08:09 PM |\t   72.7%:\t  W_train_loss:0.2026921\tV_train_syn_loss:1.3155661\tV_train_loss:0.6023657\t  V_star_val_loss:6.1998549\t  V_val_loss:6.0444477\n",
      "06/17 05:08:17 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:17 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:08:17 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:08:17 PM |\t  computing score...\n",
      "06/17 05:08:17 PM |\t  model_w_in_main sacreBLEU : 1.506283\n",
      "06/17 05:08:17 PM |\t  model_w_in_main test loss : 7.672504\n",
      "06/17 05:08:19 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:19 PM |\t  pred_decoded[:2]:['Diemist ist somber in der Lage, die in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:19 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:08:19 PM |\t  computing score...\n",
      "06/17 05:08:19 PM |\t  model_v_in_main sacreBLEU : 0.957141\n",
      "06/17 05:08:19 PM |\t  model_v_in_main test loss : 6.495415\n",
      "06/17 05:08:19 PM |\t  ('Attention Weights A : ', tensor([0.6310, 0.8277, 0.7538, 0.7501, 0.8867, 0.7119, 0.8069, 0.6672, 0.9197,\n",
      "        0.8432, 0.8400, 0.7506, 0.9469, 1.0000, 0.9671, 0.9467, 0.9554, 1.0243,\n",
      "        0.9362, 0.9359, 0.9014, 0.4825, 0.9276, 1.1139, 0.9675, 1.0614, 1.0419,\n",
      "        0.9976, 1.0734, 0.8811, 0.9524, 0.9793, 1.0772, 1.2290, 0.8885, 0.9106,\n",
      "        1.1897, 1.0576, 1.0186, 0.8450, 0.9275, 1.0493, 0.9793, 1.1181, 0.8566,\n",
      "        1.6299, 1.1864, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:08:33 PM |\t  1e+02%:\t  W_train_loss:0.1498727\tV_train_syn_loss:1.3020094\tV_train_loss:0.4548933\t  V_star_val_loss:7.3680727\t  V_val_loss:7.0799901\n",
      "06/17 05:08:33 PM |\t  w_train_loss:1.947960965335369,v_train_loss:11.797759890556335\n",
      "06/17 05:08:33 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:5.764800999999999e-05,\t\tlr_v:5.764800999999999e-05,\t\tlr_A:5.764800999999999----------------\n",
      "06/17 05:08:33 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:08:51 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:51 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:08:51 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:08:51 PM |\t  computing score...\n",
      "06/17 05:08:51 PM |\t  model_w_in_main sacreBLEU : 1.809544\n",
      "06/17 05:08:51 PM |\t  model_w_in_main test loss : 7.704166\n",
      "06/17 05:08:52 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:52 PM |\t  pred_decoded[:2]:['Diemist ist somber in der Lage, die in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage auch in der Lage', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:08:52 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:08:52 PM |\t  computing score...\n",
      "06/17 05:08:52 PM |\t  model_v_in_main sacreBLEU : 1.154181\n",
      "06/17 05:08:52 PM |\t  model_v_in_main test loss : 6.598997\n",
      "06/17 05:08:52 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8277, 0.7536, 0.7499, 0.8867, 0.7116, 0.8069, 0.6671, 0.9197,\n",
      "        0.8427, 0.8401, 0.7508, 0.9469, 1.0000, 0.9671, 0.9467, 0.9554, 1.0243,\n",
      "        0.9362, 0.9359, 0.9014, 0.4825, 0.9276, 1.1139, 0.9675, 1.0614, 1.0419,\n",
      "        0.9976, 1.0734, 0.8811, 0.9524, 0.9793, 1.0772, 1.2290, 0.8885, 0.9106,\n",
      "        1.1897, 1.0576, 1.0186, 0.8450, 0.9273, 1.0499, 0.9791, 1.1183, 0.8568,\n",
      "        1.6292, 1.1865, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:08:55 PM |\t   18.2%:\t  W_train_loss:0.0680143\tV_train_syn_loss:1.7374518\tV_train_loss:0.2396037\t  V_star_val_loss:6.7209188\t  V_val_loss:6.5372183\n",
      "06/17 05:09:13 PM |\t   45.5%:\t  W_train_loss:0.1435792\tV_train_syn_loss:1.2219252\tV_train_loss:0.4734383\t  V_star_val_loss:6.2360829\t  V_val_loss:5.9973790\n",
      "06/17 05:09:26 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:09:26 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Es ist die Stadt ist in Ancient Greek and Ancient Egyptian']\n",
      "06/17 05:09:26 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:09:26 PM |\t  computing score...\n",
      "06/17 05:09:26 PM |\t  model_w_in_main sacreBLEU : 1.809544\n",
      "06/17 05:09:26 PM |\t  model_w_in_main test loss : 7.734988\n",
      "06/17 05:09:28 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:09:28 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen, die in its utmost esoteric esoteric esoteric esoteric esoteric esoteric esoteric e', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:09:28 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:09:28 PM |\t  computing score...\n",
      "06/17 05:09:28 PM |\t  model_v_in_main sacreBLEU : 1.661504\n",
      "06/17 05:09:28 PM |\t  model_v_in_main test loss : 6.673559\n",
      "06/17 05:09:28 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8277, 0.7536, 0.7499, 0.8867, 0.7116, 0.8069, 0.6671, 0.9197,\n",
      "        0.8427, 0.8401, 0.7508, 0.9468, 1.0000, 0.9671, 0.9468, 0.9556, 1.0240,\n",
      "        0.9369, 0.9362, 0.9010, 0.4825, 0.9283, 1.1165, 0.9676, 1.0614, 1.0419,\n",
      "        0.9977, 1.0730, 0.8809, 0.9524, 0.9793, 1.0772, 1.2290, 0.8885, 0.9106,\n",
      "        1.1897, 1.0576, 1.0186, 0.8450, 0.9273, 1.0499, 0.9791, 1.1183, 0.8568,\n",
      "        1.6292, 1.1865, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:09:36 PM |\t   72.7%:\t  W_train_loss:0.1491966\tV_train_syn_loss:1.6030791\tV_train_loss:0.4922608\t  V_star_val_loss:6.3765532\t  V_val_loss:6.2621228\n",
      "06/17 05:09:52 PM |\t  1e+02%:\t  W_train_loss:0.1129523\tV_train_syn_loss:1.3573367\tV_train_loss:0.3675321\t  V_star_val_loss:7.3163352\t  V_val_loss:7.2320533\n",
      "06/17 05:09:52 PM |\t  w_train_loss:1.4212269112467766,v_train_loss:11.238941490650177\n",
      "06/17 05:09:52 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:4.035360699999999e-05,\t\tlr_v:4.035360699999999e-05,\t\tlr_A:4.035360699999999----------------\n",
      "06/17 05:09:52 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:10:00 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:00 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greek and Ancient Egyptologies.']\n",
      "06/17 05:10:00 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:10:00 PM |\t  computing score...\n",
      "06/17 05:10:00 PM |\t  model_w_in_main sacreBLEU : 1.830201\n",
      "06/17 05:10:00 PM |\t  model_w_in_main test loss : 7.762429\n",
      "06/17 05:10:01 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:01 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen, die in its utmost esoteric esophageal esophageal esophageal esophageal esophageal esophageal e', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:01 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:10:01 PM |\t  computing score...\n",
      "06/17 05:10:01 PM |\t  model_v_in_main sacreBLEU : 1.935656\n",
      "06/17 05:10:01 PM |\t  model_v_in_main test loss : 6.686763\n",
      "06/17 05:10:01 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7499, 0.8867, 0.7116, 0.8069, 0.6671, 0.9197,\n",
      "        0.8427, 0.8401, 0.7508, 0.9468, 1.0000, 0.9671, 0.9468, 0.9556, 1.0240,\n",
      "        0.9369, 0.9362, 0.9010, 0.4825, 0.9283, 1.1165, 0.9676, 1.0614, 1.0419,\n",
      "        0.9977, 1.0730, 0.8809, 0.9524, 0.9793, 1.0772, 1.2284, 0.8883, 0.9103,\n",
      "        1.1896, 1.0576, 1.0185, 0.8452, 0.9274, 1.0497, 0.9791, 1.1186, 0.8568,\n",
      "        1.6290, 1.1865, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:10:15 PM |\t   18.2%:\t  W_train_loss:0.0548515\tV_train_syn_loss:1.4588619\tV_train_loss:0.1981601\t  V_star_val_loss:6.6327155\t  V_val_loss:6.6320093\n",
      "06/17 05:10:34 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:34 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greek and Ancient Egyptologies.']\n",
      "06/17 05:10:34 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:10:34 PM |\t  computing score...\n",
      "06/17 05:10:34 PM |\t  model_w_in_main sacreBLEU : 1.565465\n",
      "06/17 05:10:34 PM |\t  model_w_in_main test loss : 7.780983\n",
      "06/17 05:10:35 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:35 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen, die in its utmost esoteric esophageal esophageal esophageal esophageal esophageal esophageal e', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:10:35 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:10:35 PM |\t  computing score...\n",
      "06/17 05:10:35 PM |\t  model_v_in_main sacreBLEU : 1.935656\n",
      "06/17 05:10:35 PM |\t  model_v_in_main test loss : 6.686216\n",
      "06/17 05:10:35 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7499, 0.8868, 0.7115, 0.8069, 0.6670, 0.9197,\n",
      "        0.8427, 0.8399, 0.7507, 0.9468, 1.0001, 0.9670, 0.9468, 0.9558, 1.0238,\n",
      "        0.9370, 0.9364, 0.9010, 0.4825, 0.9283, 1.1160, 0.9676, 1.0614, 1.0419,\n",
      "        0.9977, 1.0730, 0.8809, 0.9524, 0.9793, 1.0772, 1.2284, 0.8883, 0.9103,\n",
      "        1.1896, 1.0576, 1.0185, 0.8452, 0.9274, 1.0497, 0.9791, 1.1186, 0.8568,\n",
      "        1.6290, 1.1865, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:10:38 PM |\t   45.5%:\t  W_train_loss:0.1120448\tV_train_syn_loss:1.2337172\tV_train_loss:0.4092741\t  V_star_val_loss:6.0896826\t  V_val_loss:6.0499794\n",
      "06/17 05:10:56 PM |\t   72.7%:\t  W_train_loss:0.1218111\tV_train_syn_loss:1.2421103\tV_train_loss:0.4254617\t  V_star_val_loss:6.4588881\t  V_val_loss:6.3166431\n",
      "06/17 05:11:09 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:09 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greek and Ancient Egyptologies.']\n",
      "06/17 05:11:09 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:11:09 PM |\t  computing score...\n",
      "06/17 05:11:09 PM |\t  model_w_in_main sacreBLEU : 1.603163\n",
      "06/17 05:11:09 PM |\t  model_w_in_main test loss : 7.803775\n",
      "06/17 05:11:10 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:10 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen zu halt, die er, die er, die er, die er, die er, die er, die er, die er, die er, die er, die er, die er,', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:10 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:11:10 PM |\t  computing score...\n",
      "06/17 05:11:10 PM |\t  model_v_in_main sacreBLEU : 1.612911\n",
      "06/17 05:11:10 PM |\t  model_v_in_main test loss : 6.735255\n",
      "06/17 05:11:10 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7499, 0.8868, 0.7115, 0.8069, 0.6670, 0.9197,\n",
      "        0.8427, 0.8399, 0.7507, 0.9468, 1.0001, 0.9670, 0.9468, 0.9558, 1.0238,\n",
      "        0.9370, 0.9364, 0.9010, 0.4825, 0.9283, 1.1160, 0.9676, 1.0614, 1.0419,\n",
      "        0.9977, 1.0726, 0.8806, 0.9524, 0.9794, 1.0772, 1.2282, 0.8884, 0.9102,\n",
      "        1.1895, 1.0576, 1.0184, 0.8453, 0.9268, 1.0500, 0.9792, 1.1192, 0.8568,\n",
      "        1.6290, 1.1865, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:11:19 PM |\t  1e+02%:\t  W_train_loss:0.0920969\tV_train_syn_loss:1.1400690\tV_train_loss:0.3187864\t  V_star_val_loss:7.5068124\t  V_val_loss:7.3257705\n",
      "06/17 05:11:19 PM |\t  w_train_loss:1.142413206398487,v_train_loss:9.639661133289337\n",
      "06/17 05:11:19 PM |\t  \n",
      "\n",
      "  ----------------epoch:8,\t\tlr_w:2.8247524899999994e-05,\t\tlr_v:2.8247524899999994e-05,\t\tlr_A:2.8247524899999994----------------\n",
      "06/17 05:11:19 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:11:35 PM |\t   18.2%:\t  W_train_loss:0.0476605\tV_train_syn_loss:1.3984289\tV_train_loss:0.1726056\t  V_star_val_loss:6.8079379\t  V_val_loss:6.7101612\n",
      "06/17 05:11:43 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:43 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greece and Ancient Egyptologies.']\n",
      "06/17 05:11:43 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:11:43 PM |\t  computing score...\n",
      "06/17 05:11:43 PM |\t  model_w_in_main sacreBLEU : 1.939153\n",
      "06/17 05:11:43 PM |\t  model_w_in_main test loss : 7.819658\n",
      "06/17 05:11:44 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:44 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen, die in its utmost esoteric esophageal esophageal esophageal esophageal esophageal esophageal e', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:11:44 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:11:44 PM |\t  computing score...\n",
      "06/17 05:11:44 PM |\t  model_v_in_main sacreBLEU : 1.251954\n",
      "06/17 05:11:44 PM |\t  model_v_in_main test loss : 6.768919\n",
      "06/17 05:11:44 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7499, 0.8868, 0.7115, 0.8069, 0.6670, 0.9197,\n",
      "        0.8427, 0.8398, 0.7507, 0.9468, 1.0001, 0.9670, 0.9469, 0.9558, 1.0238,\n",
      "        0.9370, 0.9364, 0.9010, 0.4825, 0.9283, 1.1160, 0.9676, 1.0614, 1.0419,\n",
      "        0.9977, 1.0726, 0.8806, 0.9524, 0.9794, 1.0772, 1.2282, 0.8884, 0.9102,\n",
      "        1.1895, 1.0576, 1.0184, 0.8453, 0.9268, 1.0500, 0.9792, 1.1192, 0.8568,\n",
      "        1.6290, 1.1864, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:12:00 PM |\t   45.5%:\t  W_train_loss:0.0912852\tV_train_syn_loss:1.1514287\tV_train_loss:0.3675769\t  V_star_val_loss:6.2334382\t  V_val_loss:6.1063738\n",
      "06/17 05:12:19 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:19 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greece and Ancient Egyptologies.']\n",
      "06/17 05:12:19 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:12:19 PM |\t  computing score...\n",
      "06/17 05:12:19 PM |\t  model_w_in_main sacreBLEU : 1.830201\n",
      "06/17 05:12:19 PM |\t  model_w_in_main test loss : 7.836367\n",
      "06/17 05:12:21 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:21 PM |\t  pred_decoded[:2]:['Die Auschwitz ist so beliebten Erkrankungen, die in its utmost esoteric esophageal esophageal esophageal esophageal esophageal esophageal e', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:21 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:12:21 PM |\t  computing score...\n",
      "06/17 05:12:21 PM |\t  model_v_in_main sacreBLEU : 1.251954\n",
      "06/17 05:12:21 PM |\t  model_v_in_main test loss : 6.806715\n",
      "06/17 05:12:21 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7499, 0.8868, 0.7115, 0.8069, 0.6670, 0.9197,\n",
      "        0.8427, 0.8398, 0.7507, 0.9468, 1.0001, 0.9670, 0.9469, 0.9559, 1.0235,\n",
      "        0.9372, 0.9364, 0.9011, 0.4825, 0.9283, 1.1160, 0.9676, 1.0614, 1.0420,\n",
      "        0.9977, 1.0725, 0.8805, 0.9525, 0.9794, 1.0772, 1.2281, 0.8884, 0.9101,\n",
      "        1.1895, 1.0576, 1.0184, 0.8453, 0.9268, 1.0500, 0.9792, 1.1192, 0.8568,\n",
      "        1.6290, 1.1864, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:12:23 PM |\t   72.7%:\t  W_train_loss:0.1056026\tV_train_syn_loss:1.1424202\tV_train_loss:0.3842071\t  V_star_val_loss:6.5891407\t  V_val_loss:6.4003661\n",
      "06/17 05:12:39 PM |\t  1e+02%:\t  W_train_loss:0.0786941\tV_train_syn_loss:0.9455016\tV_train_loss:0.2865599\t  V_star_val_loss:7.6519408\t  V_val_loss:7.4002474\n",
      "06/17 05:12:39 PM |\t  w_train_loss:0.9697269946336746,v_train_loss:8.773093104362488\n",
      "06/17 05:12:39 PM |\t  \n",
      "\n",
      "  ----------------epoch:9,\t\tlr_w:1.9773267429999995e-05,\t\tlr_v:1.9773267429999995e-05,\t\tlr_A:1.9773267429999994----------------\n",
      "06/17 05:12:39 PM |\t  split size:[4, 8, 0, 4]\n",
      "06/17 05:12:52 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:52 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greece and Ancient Egyptologies.']\n",
      "06/17 05:12:52 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:12:52 PM |\t  computing score...\n",
      "06/17 05:12:52 PM |\t  model_w_in_main sacreBLEU : 1.981326\n",
      "06/17 05:12:52 PM |\t  model_w_in_main test loss : 7.849734\n",
      "06/17 05:12:54 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:54 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, die in its utmost errand nicht sogar, Die Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychia', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:12:54 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:12:54 PM |\t  computing score...\n",
      "06/17 05:12:54 PM |\t  model_v_in_main sacreBLEU : 1.303031\n",
      "06/17 05:12:54 PM |\t  model_v_in_main test loss : 6.840327\n",
      "06/17 05:12:54 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7498, 0.8868, 0.7115, 0.8069, 0.6670, 0.9197,\n",
      "        0.8427, 0.8398, 0.7507, 0.9468, 1.0001, 0.9670, 0.9469, 0.9559, 1.0235,\n",
      "        0.9372, 0.9364, 0.9011, 0.4825, 0.9283, 1.1160, 0.9676, 1.0614, 1.0420,\n",
      "        0.9977, 1.0725, 0.8805, 0.9525, 0.9794, 1.0772, 1.2281, 0.8884, 0.9101,\n",
      "        1.1895, 1.0576, 1.0183, 0.8452, 0.9268, 1.0500, 0.9792, 1.1192, 0.8568,\n",
      "        1.6289, 1.1863, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/17 05:13:03 PM |\t   18.2%:\t  W_train_loss:0.0433752\tV_train_syn_loss:1.4545847\tV_train_loss:0.1557498\t  V_star_val_loss:6.9938162\t  V_val_loss:6.7999153\n",
      "06/17 05:13:21 PM |\t   45.5%:\t  W_train_loss:0.0801080\tV_train_syn_loss:1.0835840\tV_train_loss:0.3384834\t  V_star_val_loss:6.3185582\t  V_val_loss:6.1781882\n",
      "06/17 05:13:29 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:13:29 PM |\t  pred_decoded[:2]:['Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C Dien Hepatitis C', 'Über die Stadt ist in Ancient Greece and Ancient Egyptologies.']\n",
      "06/17 05:13:29 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:13:29 PM |\t  computing score...\n",
      "06/17 05:13:29 PM |\t  model_w_in_main sacreBLEU : 2.964165\n",
      "06/17 05:13:29 PM |\t  model_w_in_main test loss : 7.856561\n",
      "06/17 05:13:31 PM |\t  x_decoded[:2]:['translate English to German: Sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows.', 'translate English to German: The city is mentioned in ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:13:31 PM |\t  pred_decoded[:2]:['Die Auschwitz ist sogar, die in its utmost errand nicht sogar, Die Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychiatry Psychia', 'Das ist bei ancient Greek and ancient Egyptian legends.']\n",
      "06/17 05:13:31 PM |\t  label_decoded[:2]:['Manchmal nicht, denn im Extremfall führt sie dazu, dass Menschen extreme körperliche Risiken auf sich nehmen und dem Glücksspiel oder Drogen verfallen, um sie zu lindern - das haben Studien gezeigt.', 'Die Stadt wird in altgriechischen und altägyptischen Legenden erwähnt.']\n",
      "06/17 05:13:31 PM |\t  computing score...\n",
      "06/17 05:13:31 PM |\t  model_v_in_main sacreBLEU : 1.312914\n",
      "06/17 05:13:31 PM |\t  model_v_in_main test loss : 6.859508\n",
      "06/17 05:13:31 PM |\t  ('Attention Weights A : ', tensor([0.6309, 0.8276, 0.7535, 0.7498, 0.8868, 0.7115, 0.8069, 0.6670, 0.9198,\n",
      "        0.8427, 0.8399, 0.7508, 0.9468, 1.0001, 0.9670, 0.9469, 0.9560, 1.0235,\n",
      "        0.9373, 0.9364, 0.9012, 0.4824, 0.9283, 1.1161, 0.9676, 1.0614, 1.0420,\n",
      "        0.9976, 1.0725, 0.8805, 0.9525, 0.9794, 1.0772, 1.2281, 0.8884, 0.9101,\n",
      "        1.1895, 1.0576, 1.0183, 0.8452, 0.9268, 1.0500, 0.9792, 1.1192, 0.8568,\n",
      "        1.6289, 1.1863, 1.0709], device='cuda:0', grad_fn=<ReluBackward0>))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21264/877182758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21264/3556225481.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mv_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mloss_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         loss = my_loss2(input_v, input_v_attn, output_v,\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5_scratch\\losses.py\u001b[0m in \u001b[0;36mcalc_loss_aug\u001b[1;34m(input_syn_ids, input_syn_attn, w_model, v_model)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mw_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mturnoff_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;31m# print(output_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0matt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput_ids\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5_scratch\\T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m## sampling with top_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[1;31m# 10. run greedy search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m             return self.greedy_search(\n\u001b[0m\u001b[0;32m   1255\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m             \u001b[1;31m# forward pass to get next token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   1639\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[1;31m# Decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1639\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 )\n\u001b[0;32m   1032\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1034\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[0;32m    669\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    572\u001b[0m     ):\n\u001b[0;32m    573\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[0;32m    575\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# get query states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[0mquery_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# get key/value states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(states)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;34m\"\"\"projection\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_value_proj_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0munshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin==1):\n",
    "#     my_test(valid_dataloader,model_w,-1) #before train\n",
    "#     my_test(valid_dataloader,model_v,-1)  \n",
    "\n",
    "tot_iter = [0]\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "    lr_A = architect.scheduler_A.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{lr_A}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v,tot_iter)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "    architect.scheduler_A.step()\n",
    "\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    # wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6276, 0.9817, 0.8305], device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.ReLU(A.alpha[[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
