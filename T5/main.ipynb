{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=20,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=8,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=4,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=4,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--model_name', type=str,                   default='t5-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='withlr large',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=25,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                      default=4,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=64,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=6e-4,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=6e-4,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-4,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.7,    help='momentum')\n",
    "parser.add_argument('--smoothing', type=float,                   default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.9,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5\\wandb\\run-20220409_191045-23kmteha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/500K/runs/23kmteha\" target=\"_blank\">withlr large</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/500K\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/500K/runs/23kmteha?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1e8a1a5b0a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"500K\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09 07:10:53 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 44.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09 07:10:53 PM |\t  Namespace(A_lr=0.0001, batch_size=20, decay=0.001, epochs=50, exp_name='withlr large', gpu=0, grad_acc_count=64, grad_clip=1, learning_rate_min=1e-08, model_name='t5-small', momentum=0.7, pre_epochs=0, rep_num=25, smoothing=0.1, syndata_loss_ratio=0.1, test_num=2, train_A=0, train_A_num_points=4, train_num_points=1000, train_v_num_points=4, train_v_synthetic_num_points=4, train_w_num_points=8, traindata_loss_ratio=0.9, v_lr=0.0001, valid_begin=1, valid_num_points=100, w_lr=0.0001)\n",
      "04/09 07:10:53 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "04/09 07:10:53 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09 07:11:04 PM |\t  modelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,modelname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09 07:12:03 PM |\t  train len: 1000\n",
      "04/09 07:12:03 PM |\t  train_w_num_points_len: 400\n",
      "04/09 07:12:03 PM |\t  train_v_synthetic_num_points_len: 200\n",
      "04/09 07:12:03 PM |\t  train_v_num_points_len: 200\n",
      "04/09 07:12:03 PM |\t  train_A_num_points_len: 200\n",
      "04/09 07:12:03 PM |\t  valid len: 1000\n",
      "04/09 07:12:03 PM |\t  test len: 3003\n",
      "04/09 07:12:03 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "04/09 07:12:03 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "04/09 07:12:03 PM |\t  {'de': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'en': 'translate English to German: Two sets of lights so close to one another: intentional or just a silly error?'}\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data, make a dataloader\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelname)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#teacher shouldn't have label smoothing, especially when student got same size.\n",
    "criterion_v = torch.nn.CrossEntropyLoss( reduction='none',label_smoothing=args.smoothing) #without LS, V may be too confident to that syn data, and LS do well for real data also.\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['train']['translation'][:args.train_num_points]#TODO:change dataset['validation']['translation'][:args.valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en']  #needed for T5\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "#TODO: Syn_input should be monolingual data, should try en-fo's en. cuz wmt may align\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "\n",
    "'''\n",
    "each mini batch consist of : \n",
    "1. data to train W\n",
    "2. monolingual data to generate parallel data\n",
    "3. data to train V\n",
    "4. data to train A\n",
    "'''\n",
    "\n",
    "\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "04/09 07:12:04 PM |\t  train data get\n",
      "04/09 07:12:04 PM |\t  train data loader get\n",
      "04/09 07:12:04 PM |\t  valid data loader get\n",
      "04/09 07:12:05 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=4)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=16, pin_memory=True, num_workers=4)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=16, pin_memory=True, num_workers=4)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0)\n",
    "scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion_v, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False, warmup_init=False, clip_threshold=1,beta1=0)\n",
    "scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    metric_bleu =  load_metric('bleu')\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        if(step==150):\n",
    "            break\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        with torch.no_grad():\n",
    "            ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "            acc+= ls\n",
    "            counter+= 1\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "            pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "            label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "            \n",
    "            pred_str = [x.replace('.', '')  for x in pred_decoded]\n",
    "            label_str = [[x.replace('.', '')] for x in label_decoded]\n",
    "            pred_list = [x.replace('.', '').split()  for x in pred_decoded]\n",
    "            label_list = [[x.replace('.', '').split()] for x in label_decoded]\n",
    "            metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "            metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "            if  step%100==0:\n",
    "                logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            \n",
    "            \n",
    "    logging.info('computing score...')            \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    writer.add_scalar(model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/sacreBLEU\",sacrebleu_score['score'], global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/BLEU\",bleu_score['bleu'], global_step=epoch)\n",
    "    \n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    \n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    wsize = args.train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points \n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points \n",
    "    grad_acc_count = args.grad_acc_count\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size=[wsize,synsize,vsize,Asize]\n",
    "    for step, batch in enumerate(_dataloader) :\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=True) \n",
    "        (input_w,input_syn,input_v,input_A_v) = torch.split(train_x,split_size)\n",
    "        (input_w_attn,input_syn_attn,input_v_attn,input_A_v_attn) = torch.split(train_x_attn,split_size)\n",
    "        (output_w,_,output_v,output_A_v) = torch.split(train_y,split_size)\n",
    "        (output_w_attn,_,output_v_attn,output_A_v_attn) = torch.split(train_y_attn,split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "       \n",
    "\n",
    "        if (epoch <= args.epochs) and (args.train_A == 1) and epoch >= args.pre_epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "        \n",
    "        \n",
    "        if  epoch <= args.epochs:\n",
    "            for p in w_model.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            \n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "            loss_w.backward()\n",
    "            objs_w.update(loss_w.item(), wsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "                w_optimizer.step()\n",
    "                w_optimizer.zero_grad()\n",
    "            for p in w_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        if epoch >= args.pre_epochs and epoch <= args.epochs:\n",
    "            \n",
    "            for p in v_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "            loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            v_loss =  (args.traindata_loss_ratio*loss+loss_aug*args.syndata_loss_ratio)/num_batch\n",
    "            v_trainloss_acc+=v_loss.item()\n",
    "            v_loss.backward()\n",
    "            objs_v.update(v_loss.item(), vtrainsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "                v_optimizer.step()  \n",
    "                v_optimizer.zero_grad() \n",
    "            for p in v_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        rep_fre = (loader_len//args.rep_num)\n",
    "        test_fre = (loader_len//args.test_num)\n",
    "\n",
    "        if((step)%test_fre == 0 and step!=0):\n",
    "            my_test(valid_dataloader,model_w,epoch)\n",
    "            my_test(valid_dataloader,model_v,epoch)\n",
    "        \n",
    "\n",
    "\n",
    "        if((step)%rep_fre == 0 or (step)==(loader_len-1)):\n",
    "            logging.info(f\"{progress:5.3}% \\t w_loss_avg:{objs_w.avg*train_w_num_points_len:^.7f}\\t v_loss_avg:{objs_v.avg*vtrainsize_total:^.7f}\")\n",
    "  \n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    \n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09 07:12:24 PM |\t  x_decoded[:2]:['translate English to German: This is a pity, in a sense.', 'translate English to German: I shall summarise two of those highlighted by the rapporteur, one positive and one negative.']\n",
      "04/09 07:12:24 PM |\t  pred_decoded[:2]:['Das ist in gewisser Weise schade.', 'Ich möchte zwei davon zusammenfassen, die der Berichterstatter hervorgehoben hat, einen positiven und einen negativen.']\n",
      "04/09 07:12:24 PM |\t  label_decoded[:2]:['In gewissem Sinne bedauere ich das.', 'Ich möchte zwei davon kurz zusammenfassen, der Berichterstatter hat sie schon aufgegriffen, eine positive und eine negative.']\n",
      "04/09 07:16:40 PM |\t  computing score...\n",
      "04/09 07:16:40 PM |\t  model_w_in_main sacreBLEU : 24.320994\n",
      "04/09 07:16:40 PM |\t  model_w_in_main BLEU : 0.208031\n",
      "04/09 07:16:40 PM |\t  model_w_in_main test loss : 1.108963\n",
      "04/09 07:16:40 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.0001,\t\tlr_v:0.0001----------------\n",
      "04/09 07:16:43 PM |\t    0.0% \t w_loss_avg:0.9891585\t v_loss_avg:25.0704646\n",
      "04/09 07:16:46 PM |\t   4.08% \t w_loss_avg:1.1879937\t v_loss_avg:21.0136722\n",
      "04/09 07:16:47 PM |\t   8.16% \t w_loss_avg:1.2850100\t v_loss_avg:23.0125754\n",
      "04/09 07:16:50 PM |\t   12.2% \t w_loss_avg:1.2148668\t v_loss_avg:25.4221970\n",
      "04/09 07:16:52 PM |\t   16.3% \t w_loss_avg:1.1940625\t v_loss_avg:24.5664114\n",
      "04/09 07:16:55 PM |\t   20.4% \t w_loss_avg:1.2302412\t v_loss_avg:23.3968584\n",
      "04/09 07:16:57 PM |\t   24.5% \t w_loss_avg:1.1885839\t v_loss_avg:21.5599128\n",
      "04/09 07:16:59 PM |\t   28.6% \t w_loss_avg:1.1770666\t v_loss_avg:20.5509086\n",
      "04/09 07:17:01 PM |\t   32.7% \t w_loss_avg:1.1648072\t v_loss_avg:19.5483475\n",
      "04/09 07:17:03 PM |\t   36.7% \t w_loss_avg:1.2290534\t v_loss_avg:18.8089763\n",
      "04/09 07:17:06 PM |\t   40.8% \t w_loss_avg:1.2399298\t v_loss_avg:18.3642280\n",
      "04/09 07:17:08 PM |\t   44.9% \t w_loss_avg:1.2297737\t v_loss_avg:18.1352832\n",
      "04/09 07:17:10 PM |\t   49.0% \t w_loss_avg:1.2417163\t v_loss_avg:17.6997274\n",
      "04/09 07:17:21 PM |\t  x_decoded[:2]:[\"translate English to German: A systematic survey of the various national regulations is needed if barriers to competition are to be dismantled. The EU' s own regulations too may therefore need to be analysed.\", 'translate English to German: Consumers see their standard of living being eroded, poverty spreading and the public sector and production base in most countries in the Union being dismantled and dissolved in the name of unadulterated and catastrophic competition, in the name of the absolute market economy and the promotion of the monopolistic interests of big business.']\n",
      "04/09 07:17:21 PM |\t  pred_decoded[:2]:['Eine systematische Untersuchung der verschiedenen nationalen Regelungen ist erforderlich, wenn Wettbewerbshemmnisse abgebaut werden sollen, und deshalb müssen auch die eigenen Regelungen der EU analysiert werden.', 'Die Verbraucher sehen, dass ihr Lebensstandard untergraben wird, die Armut sich ausbreitet und der öffentliche Sektor und die Produktionsbasis in den meisten Ländern der Union im Namen eines unverfälschten und katastrophalen Wettbewerbs, im Namen der absoluten Marktwirtschaft und der Förderung der monopolistischen Interessen des Großkapitals demontiert und aufgelöst wird.']\n",
      "04/09 07:17:21 PM |\t  label_decoded[:2]:['Zum Abbau von Wettbewerbsbeschränkungen sollten die nationalen Regelwerke systematisch überprüft werden, und auch die Rechtsvorschriften der EU bedürfen einer Analyse.', 'Die Verbraucher erleben, wie ihr Lebensstandard sinkt, die Armut um sich greift und der öffentliche Sektor sowie die produktive Basis in den meisten Ländern der Union im Namen eines zügellosen und zerstörerischen Wettbewerbs zugunsten der uneingeschränkten Marktwirtschaft und der Durchsetzung der Monopolinteressen des Großkapitals ruiniert und aufgelöst werden.']\n",
      "04/09 07:21:36 PM |\t  computing score...\n",
      "04/09 07:21:36 PM |\t  model_w_in_main sacreBLEU : 24.320994\n",
      "04/09 07:21:36 PM |\t  model_w_in_main BLEU : 0.208031\n",
      "04/09 07:21:36 PM |\t  model_w_in_main test loss : 1.111476\n",
      "04/09 07:21:44 PM |\t  x_decoded[:2]:['translate English to German: Allow me to make two fundamental comments at this juncture. As the competent authority, the Commission, with its logically consistent approach, has again and again served the cause of freedom of competition, not always to the delight of the Member States or enterprises concerned.', \"translate English to German: This being the case, various amendments to the rapporteur' s initial draft report were adopted in committee, particularly highlighting the need for effective reimbursement of aid found to be illegal as well as the establishment of a league table of results.\"]\n",
      "04/09 07:21:44 PM |\t  pred_decoded[:2]:['Lassen Sie mich an dieser Stelle zwei grundlegende Bemerkungen machen: Als zuständige Behörde hat die Kommission mit ihrem logischerweise konsequenten Ansatz immer wieder der Sache der Wettbewerbsfreiheit gedient, nicht immer zum Wohle der betroffenen Mitgliedstaaten oder Unternehmen.', 'Daher wurden im Ausschuß verschiedene nderungsanträge zum ursprünglichen Berichtsentwurf der Berichterstatterin angenommen, die insbesondere die Notwendigkeit einer wirksamen Rückerstattung der für illegal erachteten Beihilfen sowie die Einrichtung eines Bilanztabellen hervorheben.']\n",
      "04/09 07:21:44 PM |\t  label_decoded[:2]:['Lassen Sie mich zwei grundsätzliche Anmerkungen dazu machen: Die Kommission als zuständige Behörde hat sich mit ihrer konsequenten Haltung immer wieder um die Wettbewerbsfreiheit verdient gemacht, nicht immer zur Freude der betroffenen Mitgliedstaaten oder Unternehmen.', 'Auf dieser Grundlage wurden im Ausschuß mehrere nderungsvorschläge zum ursprünglichen Entwurf des Berichterstatters beschlossen. Das Schwergewicht lag dabei auf der Notwendigkeit, die als unrechtmäßig eingeschätzten Beihilfen tatsächlich zurückzufordern, sowie eine Liste der Ergebnisse zu erstellen.']\n",
      "04/09 07:26:00 PM |\t  computing score...\n",
      "04/09 07:26:00 PM |\t  model_v_in_main sacreBLEU : 24.320994\n",
      "04/09 07:26:00 PM |\t  model_v_in_main BLEU : 0.208031\n",
      "04/09 07:26:00 PM |\t  model_v_in_main test loss : 18.578054\n",
      "04/09 07:26:01 PM |\t   53.1% \t w_loss_avg:1.2797981\t v_loss_avg:17.1642194\n",
      "04/09 07:26:05 PM |\t   57.1% \t w_loss_avg:1.2817067\t v_loss_avg:16.5729477\n",
      "04/09 07:26:07 PM |\t   61.2% \t w_loss_avg:1.3018672\t v_loss_avg:16.1944514\n",
      "04/09 07:26:09 PM |\t   65.3% \t w_loss_avg:1.3230326\t v_loss_avg:16.0789917\n",
      "04/09 07:26:11 PM |\t   69.4% \t w_loss_avg:1.3154422\t v_loss_avg:16.4355684\n",
      "04/09 07:26:14 PM |\t   73.5% \t w_loss_avg:1.2855552\t v_loss_avg:16.4115192\n",
      "04/09 07:26:16 PM |\t   77.6% \t w_loss_avg:1.2736357\t v_loss_avg:16.8553766\n",
      "04/09 07:26:18 PM |\t   81.6% \t w_loss_avg:1.2791689\t v_loss_avg:16.6290705\n",
      "04/09 07:26:20 PM |\t   85.7% \t w_loss_avg:1.2734147\t v_loss_avg:16.6173816\n",
      "04/09 07:26:22 PM |\t   89.8% \t w_loss_avg:1.2629171\t v_loss_avg:16.7706267\n",
      "04/09 07:26:24 PM |\t   93.9% \t w_loss_avg:1.2596943\t v_loss_avg:16.7321282\n",
      "04/09 07:26:26 PM |\t   98.0% \t w_loss_avg:1.2684905\t v_loss_avg:16.6189345\n",
      "04/09 07:26:28 PM |\t  1e+02% \t w_loss_avg:1.2809111\t v_loss_avg:16.4517204\n",
      "04/09 07:26:28 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/09 07:26:28 PM |\t  w_train_loss:1.2809111354872584,v_train_loss:16.45172044634819\n",
      "04/09 07:26:37 PM |\t  x_decoded[:2]:['translate English to German: Mr President, on numerous occasions in the past I have disagreed with the rapporteur on her approach to regional policy issues. This time, however, I actually agree with her.', 'translate English to German: To this end, Europe as a whole, and each Member State individually, will have to make optimum use of all available resources and capacities, including the Structural Funds.']\n",
      "04/09 07:26:37 PM |\t  pred_decoded[:2]:['Herr Präsident, in der Vergangenheit habe ich mehrfach mit der Berichterstatterin in Bezug auf ihre Herangehensweise an regionalpolitische Fragen nicht einverstanden, aber diesmal stimme ich ihr zu.', 'Dazu muss Europa als Ganzes und jeder einzelne Mitgliedstaat alle verfügbaren Ressourcen und Kapazitäten, einschließlich der Strukturfonds, optimal nutzen.']\n",
      "04/09 07:26:37 PM |\t  label_decoded[:2]:['Herr Präsident, bislang war ich in bezug auf regionalpolitische Ansätze oft anderer Meinung als die Berichterstatterin, diesmal jedoch stimme ich mit ihr überein.', 'In diesem Sinne müssen die Europäische Union als Ganzes und jeder Mitgliedstaat im besonderen von sämtlichen zu Gebote stehenden Mitteln und Möglichkeiten und damit auch von den vorhandenen Strukturfonds optimalen Gebrauch machen.']\n",
      "04/09 07:30:51 PM |\t  computing score...\n",
      "04/09 07:30:52 PM |\t  model_w_in_main sacreBLEU : 24.355476\n",
      "04/09 07:30:52 PM |\t  model_w_in_main BLEU : 0.208051\n",
      "04/09 07:30:52 PM |\t  model_w_in_main test loss : 1.097606\n",
      "04/09 07:30:59 PM |\t  x_decoded[:2]:['translate English to German: Mr President, Commissioner, the guidelines are intended to help steer the Member States towards achieving the reform objectives contained in the programmes.', 'translate English to German: We would encourage a continuation of the open debate which has been strengthened in the course of consideration of the reports by Mr von Wogau and Mr Rapkay.']\n",
      "04/09 07:30:59 PM |\t  pred_decoded[:2]:['Herr Präsident, Herr Kommissar! Die Leitlinien sollen dazu beitragen, die Mitgliedstaaten auf die Erreichung der in den Programmen enthaltenen Reformziele zu lenken.', 'Wir ermutigen die Fortsetzung der offenen Debatte, die im Zuge der Prüfung der Berichte von Herrn von Wogau und Herrn Rapkay verstärkt wurde.']\n",
      "04/09 07:30:59 PM |\t  label_decoded[:2]:['Herr Präsident, meine sehr verehrten Damen und Herren, Herr Kommissar! Mit Hilfe von Leitlinien soll den Mitgliedstaaten eine Orientierung zum Erreichen der Reformziele im Rahmen der Programmierung angeboten werden.', 'Wir befürworten die Fortsetzung der offenen Debatte, wie sie sich bei der Erörterung der Berichte von Wogau und Rapkay manifestiert hat.']\n",
      "04/09 07:35:18 PM |\t  computing score...\n",
      "04/09 07:35:18 PM |\t  model_v_in_main sacreBLEU : 24.517120\n",
      "04/09 07:35:18 PM |\t  model_v_in_main BLEU : 0.209957\n",
      "04/09 07:35:18 PM |\t  model_v_in_main test loss : 18.428528\n",
      "04/09 07:35:18 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.0001,\t\tlr_v:0.0001----------------\n",
      "04/09 07:35:21 PM |\t    0.0% \t w_loss_avg:0.9651765\t v_loss_avg:23.2523099\n",
      "04/09 07:35:24 PM |\t   4.08% \t w_loss_avg:1.1771696\t v_loss_avg:19.6140195\n",
      "04/09 07:35:25 PM |\t   8.16% \t w_loss_avg:1.2771942\t v_loss_avg:21.4421476\n",
      "04/09 07:35:28 PM |\t   12.2% \t w_loss_avg:1.1903693\t v_loss_avg:23.5185158\n",
      "04/09 07:35:30 PM |\t   16.3% \t w_loss_avg:1.1523587\t v_loss_avg:22.8145519\n",
      "04/09 07:35:33 PM |\t   20.4% \t w_loss_avg:1.1920408\t v_loss_avg:21.7212816\n",
      "04/09 07:35:35 PM |\t   24.5% \t w_loss_avg:1.1592741\t v_loss_avg:20.0501507\n",
      "04/09 07:35:37 PM |\t   28.6% \t w_loss_avg:1.1485152\t v_loss_avg:19.1599174\n",
      "04/09 07:35:39 PM |\t   32.7% \t w_loss_avg:1.1402322\t v_loss_avg:18.2340327\n",
      "04/09 07:35:42 PM |\t   36.7% \t w_loss_avg:1.1946081\t v_loss_avg:17.5671343\n",
      "04/09 07:35:44 PM |\t   40.8% \t w_loss_avg:1.2074688\t v_loss_avg:17.1634635\n",
      "04/09 07:35:46 PM |\t   44.9% \t w_loss_avg:1.1988845\t v_loss_avg:16.9527618\n",
      "04/09 07:35:48 PM |\t   49.0% \t w_loss_avg:1.2159889\t v_loss_avg:16.5504033\n",
      "04/09 07:35:57 PM |\t  x_decoded[:2]:['translate English to German: We have to remember that rural areas represent almost four fifths of the territory of the European Union.', 'translate English to German: As regards state aid, it is essential to ensure that regulations are not made more complex, and the introduction of a public register, where all aid would be recorded, does not seem advisable to us since this onerous commitment would quite naturally run counter to the attempts to simplify bureaucratic constraints.']\n",
      "04/09 07:35:57 PM |\t  pred_decoded[:2]:['Wir dürfen nicht vergessen, dass die ländlichen Gebiete fast vier Fünftel des Territoriums der Europäischen Union darstellen.', 'Was die staatlichen Beihilfen betrifft, so ist es unerläßlich, dafür zu sorgen, daß die Vorschriften nicht komplizierter werden, und die Einführung eines öffentlichen Registers, in dem alle Beihilfen registriert werden, erscheint uns nicht ratsam, da diese ernste Verpflichtung natürlich den Versuchen zur Vereinfachung der bürokratischen Zwänge zuwiderlaufen würde.']\n",
      "04/09 07:35:57 PM |\t  label_decoded[:2]:['Man muß sich vor Augen halten, daß der ländliche Raum nahezu vier Fünftel des Territoriums der Europäischen Union ausmacht.', 'Was die staatlichen Beihilfen betrifft, so gilt es, darauf zu achten, daß dieses Instrumentarium nicht zu schwerfällig wird. Die Einführung eines öffentlichen Registers, in dem alle Beihilfen erfaßt werden, scheint uns nicht wünschenswert, denn diese aufwendige Verpflichtung würde automatisch allen Versuchen zum Abbau der bürokratischen Zwänge zuwiderlaufen.']\n",
      "04/09 07:40:16 PM |\t  computing score...\n",
      "04/09 07:40:16 PM |\t  model_w_in_main sacreBLEU : 24.355476\n",
      "04/09 07:40:16 PM |\t  model_w_in_main BLEU : 0.208051\n",
      "04/09 07:40:16 PM |\t  model_w_in_main test loss : 1.098836\n",
      "04/09 07:40:54 PM |\t  x_decoded[:2]:['translate English to German: As you know, each chairman has the same number of votes as his Group has Members.', 'translate English to German: Thank you very much.']\n",
      "04/09 07:40:54 PM |\t  pred_decoded[:2]:['Wie Sie wissen, hat jeder Vorsitzende die gleiche Stimmenzahl wie seine Fraktion.', 'Vielen Dank.']\n",
      "04/09 07:40:54 PM |\t  label_decoded[:2]:['Jeder Vorsitzende bzw. jede Vorsitzende hat ja so viele Stimmen, wie die Fraktion Mitglieder hat.', 'Vielen Dank!']\n",
      "04/09 07:45:14 PM |\t  computing score...\n",
      "04/09 07:45:15 PM |\t  model_v_in_main sacreBLEU : 24.517120\n",
      "04/09 07:45:15 PM |\t  model_v_in_main BLEU : 0.209957\n",
      "04/09 07:45:15 PM |\t  model_v_in_main test loss : 18.311872\n",
      "04/09 07:45:16 PM |\t   53.1% \t w_loss_avg:1.2493681\t v_loss_avg:16.0797113\n",
      "04/09 07:45:19 PM |\t   57.1% \t w_loss_avg:1.2555069\t v_loss_avg:15.5486442\n",
      "04/09 07:45:21 PM |\t   61.2% \t w_loss_avg:1.2750112\t v_loss_avg:15.2345999\n",
      "04/09 07:45:24 PM |\t   65.3% \t w_loss_avg:1.2897834\t v_loss_avg:15.1737009\n",
      "04/09 07:45:26 PM |\t   69.4% \t w_loss_avg:1.2844349\t v_loss_avg:15.5839100\n",
      "04/09 07:45:28 PM |\t   73.5% \t w_loss_avg:1.2553975\t v_loss_avg:15.6070186\n",
      "04/09 07:45:30 PM |\t   77.6% \t w_loss_avg:1.2414722\t v_loss_avg:16.0552488\n",
      "04/09 07:45:32 PM |\t   81.6% \t w_loss_avg:1.2446531\t v_loss_avg:15.8545149\n",
      "04/09 07:45:34 PM |\t   85.7% \t w_loss_avg:1.2410793\t v_loss_avg:15.8767955\n",
      "04/09 07:45:37 PM |\t   89.8% \t w_loss_avg:1.2316768\t v_loss_avg:16.0405634\n",
      "04/09 07:45:38 PM |\t   93.9% \t w_loss_avg:1.2292347\t v_loss_avg:16.0356993\n",
      "04/09 07:45:41 PM |\t   98.0% \t w_loss_avg:1.2370820\t v_loss_avg:15.9474085\n",
      "04/09 07:45:42 PM |\t  1e+02% \t w_loss_avg:1.2515528\t v_loss_avg:15.7929550\n",
      "04/09 07:45:42 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/09 07:45:42 PM |\t  w_train_loss:1.2515528174117208,v_train_loss:15.792955040931702\n",
      "04/09 07:45:49 PM |\t  x_decoded[:2]:['translate English to German: The consequences do not inspire hope.', 'translate English to German: I congratulate him on his excellent report.']\n",
      "04/09 07:45:49 PM |\t  pred_decoded[:2]:['Die Folgen wecken keine Hoffnung.', 'Ich beglückwünsche ihn zu seinem ausgezeichneten Bericht.']\n",
      "04/09 07:45:49 PM |\t  label_decoded[:2]:['Die Folgen lassen nicht auf sich warten.', 'Ich beglückwünsche ihn zu seinem ausgezeichneten Bericht.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27200/2317287350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27200/3778181570.py\u001b[0m in \u001b[0;36mmy_test\u001b[1;34m(_dataloader, model, epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0macc\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaderx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mx_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaderx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mpred_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m## sampling with top_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             )\n\u001b[0;32m   1250\u001b[0m             \u001b[1;31m# 12. run beam search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1252\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m             \u001b[1;31m# stateless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2087\u001b[1;33m             beam_outputs = beam_scorer.process(\n\u001b[0m\u001b[0;32m   2088\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2089\u001b[0m                 \u001b[0mnext_token_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_beam_search.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mbeam_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             ):\n\u001b[0;32m    251\u001b[0m                 \u001b[0mbatch_beam_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnext_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                           'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,model_w,-1) #before train\n",
    "    # my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "    \n",
    "    my_test(valid_dataloader,model_w,epoch) \n",
    "    my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
