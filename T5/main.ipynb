{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03 02:31:12 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 33.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03 02:31:12 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "03/03 02:31:12 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(0)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "torch.save(pretrained,'T5BASE.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03 02:31:44 PM |\t  train len: 4992\n",
      "03/03 02:31:44 PM |\t  train_w_num_points_len: 1248\n",
      "03/03 02:31:44 PM |\t  train_w_synthetic_num_points_len: 1248\n",
      "03/03 02:31:44 PM |\t  train_v_num_points_len: 1248\n",
      "03/03 02:31:44 PM |\t  train_A_num_points_len: 1248\n",
      "03/03 02:31:44 PM |\t  valid len: 1000\n",
      "03/03 02:31:44 PM |\t  test len: 3003\n",
      "03/03 02:31:44 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#ignore_index = tokenizer.pad_token_id,\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:train_num_points]\n",
    "valid = dataset['validation']['translation'][:valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = 'translate English to German: ' + t['en'] \n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "num_batch = train_num_points//batch_size\n",
    "train = train[:batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "train_w_num_points_len = num_batch * train_w_num_points\n",
    "train_w_synthetic_num_points_len = num_batch * train_w_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * train_v_num_points\n",
    "train_A_num_points_len = num_batch * train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_w_synthetic_num_points_len: %d\",train_w_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), \n",
    "                        batch_size=batch_size, pin_memory=True, num_workers=1)\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=RandomSampler(valid_data), \n",
    "                        batch_size=batch_size, pin_memory=True, num_workers=1)\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=RandomSampler(test_data),\n",
    "                        batch_size=batch_size, pin_memory=True, num_workers=1)#, sampler=RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.SGD(model_w.parameters(),w_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_w  = torch.optim.lr_scheduler.CosineAnnealingLR(w_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.SGD(model_v.parameters(),v_lr,momentum=momentum,weight_decay=decay)\n",
    "scheduler_v  = torch.optim.lr_scheduler.CosineAnnealingLR(v_optimizer, float(epochs), eta_min=learning_rate_min)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Ich werde jetzt essen</s> <pad> <pad> <pad>',\n",
       " '<pad> es ist mein Nameit ist</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['im going to eat now ','it is my nameit is']\n",
    "for index,i in enumerate(x) :\n",
    "    x[index] = 'translate Enlgish to German:' + x[index]\n",
    "y= tokenize(x, tokenizer, max_length = max_length)\n",
    "input = y[0].cuda()\n",
    "output  = model_v.generate(input,max_length=max_length)\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "def my_test(test_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric =  load_metric('sacrebleu')\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).cuda()\n",
    "        n = test_dataloaderx.size(0)   \n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).cuda()\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).cuda()\n",
    "        ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "        with torch.no_grad():\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            try:\n",
    "                x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "                pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "                label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "                \n",
    "                pred_ = [x.lower().replace('.', '')  for x in pred_decoded]\n",
    "                label_ = [[x.lower().replace('.', '')] for x in label_decoded]\n",
    "                if  step%100==0:\n",
    "                    logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                    logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                    logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "                metric.add_batch(predictions=pred_, references=label_)\n",
    "                \n",
    "               \n",
    "            except Exception as ex:\n",
    "                print(tokenizer.batch_decode(pre),[[x] for x in tokenizer.batch_decode(test_dataloadery)])\n",
    "                raise Exception(ex)\n",
    "        # logging.info(f\"loss:{ls}\")\n",
    "        \n",
    "        acc+= ls\n",
    "        counter+= 1\n",
    "    final_score = metric.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,final_score['score'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter*n))\n",
    "    writer.add_scalar(\"MT/\"+model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/\"+model.name+\"/sacreBLEU\",final_score['score'], global_step=epoch)\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, train_dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    counter = 0\n",
    "    wsize = train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = train_w_synthetic_num_points\n",
    "    vsize = train_v_num_points \n",
    "    Asize = train_A_num_points \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        counter+=1\n",
    "        batch_loss_w, batch_loss_v = 0, 0\n",
    "        \n",
    "        train_x = Variable(batch[0], requires_grad=False).cuda()\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).cuda()\n",
    "        train_y = Variable(batch[2], requires_grad=False).cuda()\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).cuda() \n",
    "\n",
    "        input_w = train_x[:wsize]\n",
    "        input_w_attn = train_x_attn[:wsize]\n",
    "        output_w = train_y[:wsize]\n",
    "        output_w_attn = train_y_attn[:wsize]\n",
    "        attn_idx = attn_idx_list[train_w_num_points*step:(train_w_num_points*step+train_w_num_points)]\n",
    "           \n",
    "        input_syn = train_x[wsize:wsize+synsize]\n",
    "        input_syn_attn = train_x_attn[wsize:wsize+synsize]\n",
    "\n",
    "        input_v = train_x[wsize+synsize:wsize+synsize+vsize]\n",
    "        input_v_attn = train_x_attn[wsize+synsize:wsize+synsize+vsize]\n",
    "        output_v = train_y[wsize+synsize:wsize+synsize+vsize]\n",
    "        output_v_attn = train_y_attn[wsize+synsize:wsize+synsize+vsize]\n",
    "\n",
    "        input_A_v      = train_x[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        input_A_v_attn = train_x_attn[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        output_A_v      = train_y[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "        output_A_v_attn = train_y_attn[wsize+synsize+vsize:wsize+synsize+vsize+Asize]\n",
    "       \n",
    "\n",
    "        if epoch <= epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "\n",
    "        if epoch <= epochs:\n",
    "            \n",
    "            w_optimizer.zero_grad()\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            batch_loss_w += loss_w.item()\n",
    "            loss_w.backward()\n",
    "            # nn.utils.clip_grad_norm(w_model.parameters(), grad_clip)\n",
    "            w_optimizer.step()\n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "        if epoch >= pre_epochs:\n",
    "            v_optimizer.zero_grad()\n",
    "            loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)#,input_v,input_v_attn,output_v,output_v_attn)\n",
    "            loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            \n",
    "            v_loss =  (syndata_loss_ratio*loss_aug+traindata_loss_ratio*loss)/num_batch\n",
    "            \n",
    "            batch_loss_v += v_loss.item()\n",
    "            v_loss.backward()\n",
    "            # nn.utils.clip_grad_norm(v_model.parameters(), grad_clip)\n",
    "            v_optimizer.step()     \n",
    "                \n",
    "            v_trainloss_acc+=v_loss.item()\n",
    "            \n",
    "        if(step*batch_size%5==0):\n",
    "            logging.info(f\"{step*batch_size*100/(train_num_points)}%\")\n",
    "    \n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/03 02:32:00 PM |\t  x_decoded[:2]:['translate English to German: Repercussions', 'translate English to German: The Quebec government proposes to limit donations to parties to 100 dollars, but this will not change the situation, he says: \"Until election expenses are strictly limited, there will be dirty money in politics.\"']\n",
      "03/03 02:32:00 PM |\t  pred_decoded[:2]:['Auswirkungen', 'Die Quebec-Regierung schlägt vor, Spenden an Parteien auf 100 Dollar zu beschränken, aber dies wird die Situation nicht ändern, sagt er: \"Ehe die Wahlkosten streng begrenzt sind, wird es schmutziges Geld in der Politik geben.\"']\n",
      "03/03 02:32:00 PM |\t  label_decoded[:2]:['Auswirkungen', 'Die Regierung Quebecs schlägt vor, die Spenden an Parteien auf 100 Dollar zu beschränken, was ihm zufolge nichts an der Situation ändern würde: \"Solange die Wahlkampfausgaben nicht streng begrenzt werden, wird Schwarzgeld in der Politik im Umlauf sein.\"']\n",
      "03/03 02:37:32 PM |\t  model_w_in_main bleu : 23.027894\n",
      "03/03 02:37:32 PM |\t  model_w_in_main test loss : 0.737425\n",
      "03/03 02:37:39 PM |\t  x_decoded[:2]:['translate English to German: Former commanders of the anti-Soviet struggle are already thinking about restoring provincial militias, which will escape the central power.', 'translate English to German: Financially, PSG has the means to make it happen.']\n",
      "03/03 02:37:39 PM |\t  pred_decoded[:2]:['Ehemalige Kommandanten des antisowjetischen Kampfes denken bereits an die Wiederherstellung von Provinzmilizen, die der Zentralmacht entkommen werden.', 'Finanziell verfügt PSG über die Mittel, um dies zu erreichen.']\n",
      "03/03 02:37:39 PM |\t  label_decoded[:2]:['Ehemalige Kommandeure des antisowjetischen Kampfes bemühen sich bereits um die Wiederherstellung der Milizen in der Provinz, die sich der Macht der Zentralregierung entziehen werden.', 'Finanziell gibt sich der PSG die Mittel, damit dieses Vorhaben konkrete Formen annimmt.']\n",
      "03/03 02:43:04 PM |\t  model_v_in_main bleu : 23.027894\n",
      "03/03 02:43:04 PM |\t  model_v_in_main test loss : 0.737389\n",
      "03/03 02:43:04 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.001,\t\tlr_v:0.001----------------\n",
      "03/03 02:43:15 PM |\t  0.0%\n",
      "03/03 02:44:04 PM |\t  1.6%\n",
      "03/03 02:44:52 PM |\t  3.2%\n",
      "03/03 02:45:41 PM |\t  4.8%\n",
      "03/03 02:46:31 PM |\t  6.4%\n",
      "03/03 02:47:28 PM |\t  8.0%\n",
      "03/03 02:48:21 PM |\t  9.6%\n",
      "03/03 02:49:16 PM |\t  11.2%\n",
      "03/03 02:50:07 PM |\t  12.8%\n",
      "03/03 02:50:56 PM |\t  14.4%\n",
      "03/03 02:51:46 PM |\t  16.0%\n",
      "03/03 02:52:40 PM |\t  17.6%\n",
      "03/03 02:53:26 PM |\t  19.2%\n",
      "03/03 02:54:28 PM |\t  20.8%\n",
      "03/03 02:55:09 PM |\t  22.4%\n",
      "03/03 02:56:10 PM |\t  24.0%\n",
      "03/03 02:56:57 PM |\t  25.6%\n",
      "03/03 02:57:48 PM |\t  27.2%\n",
      "03/03 02:58:37 PM |\t  28.8%\n",
      "03/03 02:59:31 PM |\t  30.4%\n",
      "03/03 03:00:21 PM |\t  32.0%\n",
      "03/03 03:01:14 PM |\t  33.6%\n",
      "03/03 03:02:16 PM |\t  35.2%\n",
      "03/03 03:03:17 PM |\t  36.8%\n",
      "03/03 03:04:14 PM |\t  38.4%\n",
      "03/03 03:05:09 PM |\t  40.0%\n",
      "03/03 03:05:57 PM |\t  41.6%\n",
      "03/03 03:06:54 PM |\t  43.2%\n",
      "03/03 03:07:37 PM |\t  44.8%\n",
      "03/03 03:08:24 PM |\t  46.4%\n",
      "03/03 03:09:17 PM |\t  48.0%\n",
      "03/03 03:10:10 PM |\t  49.6%\n",
      "03/03 03:11:03 PM |\t  51.2%\n",
      "03/03 03:11:50 PM |\t  52.8%\n",
      "03/03 03:12:46 PM |\t  54.4%\n",
      "03/03 03:13:56 PM |\t  56.0%\n",
      "03/03 03:14:56 PM |\t  57.6%\n",
      "03/03 03:15:51 PM |\t  59.2%\n",
      "03/03 03:16:56 PM |\t  60.8%\n",
      "03/03 03:18:06 PM |\t  62.4%\n",
      "03/03 03:19:07 PM |\t  64.0%\n",
      "03/03 03:20:21 PM |\t  65.6%\n",
      "03/03 03:21:30 PM |\t  67.2%\n",
      "03/03 03:22:34 PM |\t  68.8%\n",
      "03/03 03:23:34 PM |\t  70.4%\n",
      "03/03 03:24:35 PM |\t  72.0%\n",
      "03/03 03:25:46 PM |\t  73.6%\n",
      "03/03 03:26:49 PM |\t  75.2%\n",
      "03/03 03:27:47 PM |\t  76.8%\n",
      "03/03 03:28:56 PM |\t  78.4%\n",
      "03/03 03:30:04 PM |\t  80.0%\n",
      "03/03 03:31:01 PM |\t  81.6%\n",
      "03/03 03:32:10 PM |\t  83.2%\n",
      "03/03 03:33:06 PM |\t  84.8%\n",
      "03/03 03:34:03 PM |\t  86.4%\n",
      "03/03 03:35:06 PM |\t  88.0%\n",
      "03/03 03:36:22 PM |\t  89.6%\n",
      "03/03 03:37:20 PM |\t  91.2%\n",
      "03/03 03:38:19 PM |\t  92.8%\n",
      "03/03 03:39:25 PM |\t  94.4%\n",
      "03/03 03:40:37 PM |\t  96.0%\n",
      "03/03 03:41:29 PM |\t  97.6%\n",
      "03/03 03:42:25 PM |\t  99.2%\n",
      "03/03 03:42:36 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0006, 0.0010, 0.0010,  ..., 0.0017, 0.0014, 0.0016], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 03:42:36 PM |\t  w_train_loss:1.9934293653350323,v_train_loss:0\n",
      "03/03 03:42:43 PM |\t  x_decoded[:2]:['translate English to German: Thiago Silva, who is one of the best defenders in the world, also helps everyone else progress.', 'translate English to German: \"Roll up, don\\'t be cheap, get your artwork\"']\n",
      "03/03 03:42:43 PM |\t  pred_decoded[:2]:['Thiago Silva, der einer der besten Verteidiger der Welt ist, hilft auch allen anderen Fortschritte.', '\"Roll up, don\\'t be cheap, get your artwork\"']\n",
      "03/03 03:42:43 PM |\t  label_decoded[:2]:['Thiago Silva, der weltweit einer der besten Verteidiger ist, ermöglicht es allen Anderen ebenfalls, Fortschritte zu machen.', '\"Bloß keine Kosten für falsche Papiere scheuen\"']\n",
      "03/03 03:48:46 PM |\t  model_w_in_main bleu : 22.474571\n",
      "03/03 03:48:46 PM |\t  model_w_in_main test loss : 0.136166\n",
      "03/03 03:48:55 PM |\t  x_decoded[:2]:['translate English to German: The project is supported by the St. Petersburg grant.', 'translate English to German: And death is part of everyday life.']\n",
      "03/03 03:48:55 PM |\t  pred_decoded[:2]:['Das Projekt wird durch das St. Petersburger Grant unterstützt.', 'Und der Tod ist Teil des Alltags.']\n",
      "03/03 03:48:55 PM |\t  label_decoded[:2]:['Das Projekt wird aus Zuschüssen der Stadt Sankt Petersburg finanziert', 'Und der Tod ist Teil des Alltags.']\n",
      "03/03 03:55:30 PM |\t  model_v_in_main bleu : 23.024272\n",
      "03/03 03:55:30 PM |\t  model_v_in_main test loss : 0.737796\n",
      "03/03 03:55:30 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.0009999802609535456,\t\tlr_v:0.0009999802609535456----------------\n",
      "03/03 03:55:47 PM |\t  0.0%\n",
      "03/03 03:56:55 PM |\t  1.6%\n",
      "03/03 03:58:22 PM |\t  3.2%\n",
      "03/03 03:59:46 PM |\t  4.8%\n",
      "03/03 04:01:01 PM |\t  6.4%\n",
      "03/03 04:02:13 PM |\t  8.0%\n",
      "03/03 04:03:28 PM |\t  9.6%\n",
      "03/03 04:04:56 PM |\t  11.2%\n",
      "03/03 04:06:26 PM |\t  12.8%\n",
      "03/03 04:07:42 PM |\t  14.4%\n",
      "03/03 04:08:52 PM |\t  16.0%\n",
      "03/03 04:10:04 PM |\t  17.6%\n",
      "03/03 04:11:27 PM |\t  19.2%\n",
      "03/03 04:12:37 PM |\t  20.8%\n",
      "03/03 04:13:52 PM |\t  22.4%\n",
      "03/03 04:15:20 PM |\t  24.0%\n",
      "03/03 04:16:38 PM |\t  25.6%\n",
      "03/03 04:17:45 PM |\t  27.2%\n",
      "03/03 04:18:52 PM |\t  28.8%\n",
      "03/03 04:20:04 PM |\t  30.4%\n",
      "03/03 04:21:18 PM |\t  32.0%\n",
      "03/03 04:22:44 PM |\t  33.6%\n",
      "03/03 04:23:52 PM |\t  35.2%\n",
      "03/03 04:25:07 PM |\t  36.8%\n",
      "03/03 04:26:09 PM |\t  38.4%\n",
      "03/03 04:27:15 PM |\t  40.0%\n",
      "03/03 04:28:28 PM |\t  41.6%\n",
      "03/03 04:29:37 PM |\t  43.2%\n",
      "03/03 04:30:34 PM |\t  44.8%\n",
      "03/03 04:31:42 PM |\t  46.4%\n",
      "03/03 04:33:13 PM |\t  48.0%\n",
      "03/03 04:34:09 PM |\t  49.6%\n",
      "03/03 04:35:10 PM |\t  51.2%\n",
      "03/03 04:36:17 PM |\t  52.8%\n",
      "03/03 04:37:25 PM |\t  54.4%\n",
      "03/03 04:38:24 PM |\t  56.0%\n",
      "03/03 04:39:26 PM |\t  57.6%\n",
      "03/03 04:40:38 PM |\t  59.2%\n",
      "03/03 04:41:47 PM |\t  60.8%\n",
      "03/03 04:42:47 PM |\t  62.4%\n",
      "03/03 04:43:54 PM |\t  64.0%\n",
      "03/03 04:45:05 PM |\t  65.6%\n",
      "03/03 04:46:20 PM |\t  67.2%\n",
      "03/03 04:47:23 PM |\t  68.8%\n",
      "03/03 04:48:33 PM |\t  70.4%\n",
      "03/03 04:49:34 PM |\t  72.0%\n",
      "03/03 04:50:27 PM |\t  73.6%\n",
      "03/03 04:51:45 PM |\t  75.2%\n",
      "03/03 04:52:54 PM |\t  76.8%\n",
      "03/03 04:54:04 PM |\t  78.4%\n",
      "03/03 04:55:10 PM |\t  80.0%\n",
      "03/03 04:56:26 PM |\t  81.6%\n",
      "03/03 04:57:46 PM |\t  83.2%\n",
      "03/03 04:58:37 PM |\t  84.8%\n",
      "03/03 04:59:47 PM |\t  86.4%\n",
      "03/03 05:00:54 PM |\t  88.0%\n",
      "03/03 05:01:43 PM |\t  89.6%\n",
      "03/03 05:02:33 PM |\t  91.2%\n",
      "03/03 05:03:44 PM |\t  92.8%\n",
      "03/03 05:04:40 PM |\t  94.4%\n",
      "03/03 05:05:42 PM |\t  96.0%\n",
      "03/03 05:06:46 PM |\t  97.6%\n",
      "03/03 05:07:46 PM |\t  99.2%\n",
      "03/03 05:07:58 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0011, 0.0011, 0.0016,  ..., 0.0025, 0.0023, 0.0026], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 05:07:58 PM |\t  w_train_loss:0.7190538253635168,v_train_loss:3.4233027598820627\n",
      "03/03 05:08:07 PM |\t  x_decoded[:2]:[\"translate English to German: McDonald's says that it sells billions of portions, and despite this it doesn't even give you sick days or pay you for honest work!\", 'translate English to German: The United States Attorney General intervened to suspend the most controversial laws.']\n",
      "03/03 05:08:07 PM |\t  pred_decoded[:2]:[\"McDonald's sagt, dass es Milliarden von Teilen verkauft, und trotz dieser Tatsache erbringt sie nicht einmal kranke Tage oder bezahlt Sie für ehrliche Arbeit!\", 'Der Generalanwalt der Vereinigten Staaten intervenierte, um die umstrittensten Gesetze auszusetzen.']\n",
      "03/03 05:08:07 PM |\t  label_decoded[:2]:[\"McDonald's sagt, das Unternehmen verkaufe Milliarden Portionen, und trotzdem gewährt man euch keine Krankentage, und ihr werdet für eure ehrliche Arbeit nicht bezahlt!\", 'Der Generalanwalt der USA hat eingegriffen, um die umstrittensten Gesetze auszusetzen.']\n",
      "03/03 05:13:33 PM |\t  model_w_in_main bleu : 22.417429\n",
      "03/03 05:13:33 PM |\t  model_w_in_main test loss : 0.111552\n",
      "03/03 05:13:40 PM |\t  x_decoded[:2]:['translate English to German: The \"Duchesneau report\" established a direct link between industry, under-the-table financing of parties and bribery of officials.', \"translate English to German: - This is a good idea and we haven't tried this yet.\"]\n",
      "03/03 05:13:40 PM |\t  pred_decoded[:2]:['Der \"Duchesneau-Bericht\" bildete eine direkte Verbindung zwischen der Industrie, der Unterfinanzierung von Parteien und der Bestechung von Beamten.', '- Dies ist eine gute Idee und wir haben dies noch nicht versucht.']\n",
      "03/03 05:13:40 PM |\t  label_decoded[:2]:['Der \"Duchesneau-Bericht\" etablierte eine direkte Verbindung zwischen Industrie, geheimer Finanzierung von Parteien und Bestechung von Beamten.', '- Das ist eine gute Idee, so haben wir es noch nicht versucht.']\n",
      "03/03 05:19:16 PM |\t  model_v_in_main bleu : 22.466820\n",
      "03/03 05:19:16 PM |\t  model_v_in_main test loss : 0.136050\n",
      "03/03 05:19:16 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:0.0009999309146524094,\t\tlr_v:0.0009999309146524094----------------\n",
      "03/03 05:19:37 PM |\t  0.0%\n",
      "03/03 05:20:54 PM |\t  1.6%\n",
      "03/03 05:22:18 PM |\t  3.2%\n",
      "03/03 05:23:26 PM |\t  4.8%\n",
      "03/03 05:24:35 PM |\t  6.4%\n",
      "03/03 05:25:37 PM |\t  8.0%\n",
      "03/03 05:26:43 PM |\t  9.6%\n",
      "03/03 05:27:42 PM |\t  11.2%\n",
      "03/03 05:28:43 PM |\t  12.8%\n",
      "03/03 05:29:51 PM |\t  14.4%\n",
      "03/03 05:31:04 PM |\t  16.0%\n",
      "03/03 05:32:05 PM |\t  17.6%\n",
      "03/03 05:33:11 PM |\t  19.2%\n",
      "03/03 05:34:13 PM |\t  20.8%\n",
      "03/03 05:35:10 PM |\t  22.4%\n",
      "03/03 05:36:25 PM |\t  24.0%\n",
      "03/03 05:37:23 PM |\t  25.6%\n",
      "03/03 05:38:31 PM |\t  27.2%\n",
      "03/03 05:39:29 PM |\t  28.8%\n",
      "03/03 05:40:44 PM |\t  30.4%\n",
      "03/03 05:41:56 PM |\t  32.0%\n",
      "03/03 05:42:58 PM |\t  33.6%\n",
      "03/03 05:43:50 PM |\t  35.2%\n",
      "03/03 05:44:50 PM |\t  36.8%\n",
      "03/03 05:45:42 PM |\t  38.4%\n",
      "03/03 05:46:52 PM |\t  40.0%\n",
      "03/03 05:47:55 PM |\t  41.6%\n",
      "03/03 05:48:59 PM |\t  43.2%\n",
      "03/03 05:50:03 PM |\t  44.8%\n",
      "03/03 05:51:02 PM |\t  46.4%\n",
      "03/03 05:52:25 PM |\t  48.0%\n",
      "03/03 05:53:32 PM |\t  49.6%\n",
      "03/03 05:54:43 PM |\t  51.2%\n",
      "03/03 05:55:59 PM |\t  52.8%\n",
      "03/03 05:57:07 PM |\t  54.4%\n",
      "03/03 05:58:17 PM |\t  56.0%\n",
      "03/03 05:59:04 PM |\t  57.6%\n",
      "03/03 06:00:06 PM |\t  59.2%\n",
      "03/03 06:01:14 PM |\t  60.8%\n",
      "03/03 06:02:28 PM |\t  62.4%\n",
      "03/03 06:03:35 PM |\t  64.0%\n",
      "03/03 06:04:47 PM |\t  65.6%\n",
      "03/03 06:05:41 PM |\t  67.2%\n",
      "03/03 06:06:43 PM |\t  68.8%\n",
      "03/03 06:07:51 PM |\t  70.4%\n",
      "03/03 06:09:03 PM |\t  72.0%\n",
      "03/03 06:10:25 PM |\t  73.6%\n",
      "03/03 06:11:50 PM |\t  75.2%\n",
      "03/03 06:13:04 PM |\t  76.8%\n",
      "03/03 06:14:00 PM |\t  78.4%\n",
      "03/03 06:15:09 PM |\t  80.0%\n",
      "03/03 06:16:06 PM |\t  81.6%\n",
      "03/03 06:17:16 PM |\t  83.2%\n",
      "03/03 06:18:26 PM |\t  84.8%\n",
      "03/03 06:19:29 PM |\t  86.4%\n",
      "03/03 06:20:22 PM |\t  88.0%\n",
      "03/03 06:21:23 PM |\t  89.6%\n",
      "03/03 06:22:38 PM |\t  91.2%\n",
      "03/03 06:23:43 PM |\t  92.8%\n",
      "03/03 06:24:54 PM |\t  94.4%\n",
      "03/03 06:26:03 PM |\t  96.0%\n",
      "03/03 06:26:54 PM |\t  97.6%\n",
      "03/03 06:27:45 PM |\t  99.2%\n",
      "03/03 06:27:58 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0011, 0.0011, 0.0016,  ..., 0.0025, 0.0024, 0.0027], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 06:27:58 PM |\t  w_train_loss:0.5691172404913232,v_train_loss:1.996978247538209\n",
      "03/03 06:28:05 PM |\t  x_decoded[:2]:['translate English to German: - What does St. Petersburg mean to you?', \"translate English to German: The ECB, in its report, says it will reassess the various risks, currently regarded as high, in the event of the currency's success.\"]\n",
      "03/03 06:28:05 PM |\t  pred_decoded[:2]:['- Was bedeutet St. Petersburg für Sie?', 'Die EZB erklärt in ihrem Bericht, dass sie die verschiedenen Risiken, die gegenwärtig als hoch angesehen werden, im Falle des Erfolgs der Währung neu bewerten wird.']\n",
      "03/03 06:28:05 PM |\t  label_decoded[:2]:['- Was ist für dich Petersburg?', 'Die EZB sieht in ihrem Bericht übrigens vor, die verschiedenen, heute als hoch angesehenen Risiken im Falle eines Erfolgs dieser Währung neu zu bewerten.']\n",
      "03/03 06:33:35 PM |\t  model_w_in_main bleu : 22.378597\n",
      "03/03 06:33:35 PM |\t  model_w_in_main test loss : 0.103743\n",
      "03/03 06:33:42 PM |\t  x_decoded[:2]:['translate English to German: They also know that every training session is crucial.', 'translate English to German: He will simply not have what he needs to survive.']\n",
      "03/03 06:33:42 PM |\t  pred_decoded[:2]:['Sie wissen auch, dass jede Ausbildung von entscheidender Bedeutung ist.', 'Er wird einfach nicht das haben, was er braucht, um überleben zu können.']\n",
      "03/03 06:33:42 PM |\t  label_decoded[:2]:['Sie wissen auch, dass jede Trainingseinheit ausschlaggebend ist.', 'Ihm wird einfach das fehlen, was für sein Leben von elementarer Bedeutung ist.']\n",
      "03/03 06:39:15 PM |\t  model_v_in_main bleu : 22.481754\n",
      "03/03 06:39:15 PM |\t  model_v_in_main test loss : 0.108390\n",
      "03/03 06:39:16 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:0.0009998618327140161,\t\tlr_v:0.0009998618327140161----------------\n",
      "03/03 06:39:30 PM |\t  0.0%\n",
      "03/03 06:40:31 PM |\t  1.6%\n",
      "03/03 06:41:30 PM |\t  3.2%\n",
      "03/03 06:42:28 PM |\t  4.8%\n",
      "03/03 06:43:37 PM |\t  6.4%\n",
      "03/03 06:44:48 PM |\t  8.0%\n",
      "03/03 06:46:06 PM |\t  9.6%\n",
      "03/03 06:47:04 PM |\t  11.2%\n",
      "03/03 06:48:12 PM |\t  12.8%\n",
      "03/03 06:49:07 PM |\t  14.4%\n",
      "03/03 06:50:05 PM |\t  16.0%\n",
      "03/03 06:51:12 PM |\t  17.6%\n",
      "03/03 06:52:22 PM |\t  19.2%\n",
      "03/03 06:53:23 PM |\t  20.8%\n",
      "03/03 06:54:26 PM |\t  22.4%\n",
      "03/03 06:55:32 PM |\t  24.0%\n",
      "03/03 06:56:26 PM |\t  25.6%\n",
      "03/03 06:57:23 PM |\t  27.2%\n",
      "03/03 06:58:16 PM |\t  28.8%\n",
      "03/03 06:59:04 PM |\t  30.4%\n",
      "03/03 07:00:02 PM |\t  32.0%\n",
      "03/03 07:00:56 PM |\t  33.6%\n",
      "03/03 07:01:54 PM |\t  35.2%\n",
      "03/03 07:03:11 PM |\t  36.8%\n",
      "03/03 07:04:13 PM |\t  38.4%\n",
      "03/03 07:05:28 PM |\t  40.0%\n",
      "03/03 07:06:23 PM |\t  41.6%\n",
      "03/03 07:07:26 PM |\t  43.2%\n",
      "03/03 07:08:34 PM |\t  44.8%\n",
      "03/03 07:09:46 PM |\t  46.4%\n",
      "03/03 07:10:44 PM |\t  48.0%\n",
      "03/03 07:11:48 PM |\t  49.6%\n",
      "03/03 07:12:45 PM |\t  51.2%\n",
      "03/03 07:14:03 PM |\t  52.8%\n",
      "03/03 07:15:19 PM |\t  54.4%\n",
      "03/03 07:16:24 PM |\t  56.0%\n",
      "03/03 07:17:23 PM |\t  57.6%\n",
      "03/03 07:18:34 PM |\t  59.2%\n",
      "03/03 07:19:35 PM |\t  60.8%\n",
      "03/03 07:20:31 PM |\t  62.4%\n",
      "03/03 07:21:34 PM |\t  64.0%\n",
      "03/03 07:22:37 PM |\t  65.6%\n",
      "03/03 07:23:42 PM |\t  67.2%\n",
      "03/03 07:24:52 PM |\t  68.8%\n",
      "03/03 07:25:53 PM |\t  70.4%\n",
      "03/03 07:26:49 PM |\t  72.0%\n",
      "03/03 07:27:52 PM |\t  73.6%\n",
      "03/03 07:28:54 PM |\t  75.2%\n",
      "03/03 07:29:48 PM |\t  76.8%\n",
      "03/03 07:31:22 PM |\t  78.4%\n",
      "03/03 07:32:38 PM |\t  80.0%\n",
      "03/03 07:33:41 PM |\t  81.6%\n",
      "03/03 07:35:03 PM |\t  83.2%\n",
      "03/03 07:36:16 PM |\t  84.8%\n",
      "03/03 07:37:23 PM |\t  86.4%\n",
      "03/03 07:38:41 PM |\t  88.0%\n",
      "03/03 07:39:48 PM |\t  89.6%\n",
      "03/03 07:40:54 PM |\t  91.2%\n",
      "03/03 07:42:07 PM |\t  92.8%\n",
      "03/03 07:43:30 PM |\t  94.4%\n",
      "03/03 07:44:39 PM |\t  96.0%\n",
      "03/03 07:45:51 PM |\t  97.6%\n",
      "03/03 07:47:07 PM |\t  99.2%\n",
      "03/03 07:47:20 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0011, 0.0011, 0.0011,  ..., 0.0025, 0.0024, 0.0027], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 07:47:20 PM |\t  w_train_loss:0.5148793339612894,v_train_loss:1.7549430318176746\n",
      "03/03 07:47:28 PM |\t  x_decoded[:2]:['translate English to German: Despite the progress in research, the adoption of healthy living habits remains the best way to reduce the risk of suffering from it.', 'translate English to German: You can download the document (in English for the time being, a [French] translation will be available shortly) at this address: http://ca.movember.com/fr/mens-health/prostate-cancer-screening']\n",
      "03/03 07:47:28 PM |\t  pred_decoded[:2]:['Trotz der Fortschritte in der Forschung bleibt die Annahme gesunder Lebensgewohnheiten der beste Weg, um das Leid zu verringern.', 'Sie können das Dokument herunterladen (im Moment in Englisch, eine [französische] Übersetzung wird in Kürze verfügbar sein) an dieser Adresse: http://ca.movember.com/fr/mens-health/prostate-cancer-screening']\n",
      "03/03 07:47:28 PM |\t  label_decoded[:2]:['Trotz der Fortschritte in der Forschung bleibt die Annahme einer gesunden Lebensweise der beste Weg, um das Risiko zu verringern, an ihm zu erkranken.', 'Dieses Dokument (derzeit nur auf Englisch, aber eine Übersetzung wird in Kürze verfügbar sein) kann unter folgender Adresse heruntergeladen werden: http://ca.movember.com/fr/mens-health/prostate-cancer-screening']\n",
      "03/03 07:53:10 PM |\t  model_w_in_main bleu : 22.389688\n",
      "03/03 07:53:10 PM |\t  model_w_in_main test loss : 0.099308\n",
      "03/03 07:53:17 PM |\t  x_decoded[:2]:['translate English to German: But not everyone can be engaged in culture.', 'translate English to German: Many young people want to leave, as those who were able to benefit from American largesse will leave: the flight of capital is considerable.']\n",
      "03/03 07:53:17 PM |\t  pred_decoded[:2]:['Aber nicht jeder kann sich an Kultur beteiligen.', 'Viele junge Menschen wollen ausscheiden, denn diejenigen, die von der amerikanischen Großmacht profitieren konnten, werden verlassen: die Kapitalflucht ist beträchtlich.']\n",
      "03/03 07:53:17 PM |\t  label_decoded[:2]:['Doch nicht jeder von ihnen kann sich mit Kultur befassen.', 'Viele junge Leute wollen das Land verlassen, so wie diejenigen, die sich die amerikanischen Zuwendungen zu Nutze machen konnten: die Kapitalflucht ist beträchtlich.']\n",
      "03/03 07:58:42 PM |\t  model_v_in_main bleu : 22.320697\n",
      "03/03 07:58:42 PM |\t  model_v_in_main test loss : 0.099271\n",
      "03/03 07:58:42 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:0.0009997730178655797,\t\tlr_v:0.0009997730178655797----------------\n",
      "03/03 07:58:57 PM |\t  0.0%\n",
      "03/03 08:00:10 PM |\t  1.6%\n",
      "03/03 08:01:14 PM |\t  3.2%\n",
      "03/03 08:02:15 PM |\t  4.8%\n",
      "03/03 08:03:15 PM |\t  6.4%\n",
      "03/03 08:04:22 PM |\t  8.0%\n",
      "03/03 08:05:46 PM |\t  9.6%\n",
      "03/03 08:06:56 PM |\t  11.2%\n",
      "03/03 08:07:52 PM |\t  12.8%\n",
      "03/03 08:09:10 PM |\t  14.4%\n",
      "03/03 08:10:13 PM |\t  16.0%\n",
      "03/03 08:11:22 PM |\t  17.6%\n",
      "03/03 08:12:32 PM |\t  19.2%\n",
      "03/03 08:13:55 PM |\t  20.8%\n",
      "03/03 08:15:44 PM |\t  22.4%\n",
      "03/03 08:17:17 PM |\t  24.0%\n",
      "03/03 08:18:45 PM |\t  25.6%\n",
      "03/03 08:20:36 PM |\t  27.2%\n",
      "03/03 08:22:26 PM |\t  28.8%\n",
      "03/03 08:24:18 PM |\t  30.4%\n",
      "03/03 08:26:08 PM |\t  32.0%\n",
      "03/03 08:27:40 PM |\t  33.6%\n",
      "03/03 08:29:25 PM |\t  35.2%\n",
      "03/03 08:31:03 PM |\t  36.8%\n",
      "03/03 08:32:50 PM |\t  38.4%\n",
      "03/03 08:34:44 PM |\t  40.0%\n",
      "03/03 08:36:19 PM |\t  41.6%\n",
      "03/03 08:37:33 PM |\t  43.2%\n",
      "03/03 08:39:27 PM |\t  44.8%\n",
      "03/03 08:40:38 PM |\t  46.4%\n",
      "03/03 08:41:50 PM |\t  48.0%\n",
      "03/03 08:43:15 PM |\t  49.6%\n",
      "03/03 08:44:14 PM |\t  51.2%\n",
      "03/03 08:45:11 PM |\t  52.8%\n",
      "03/03 08:46:31 PM |\t  54.4%\n",
      "03/03 08:47:30 PM |\t  56.0%\n",
      "03/03 08:48:40 PM |\t  57.6%\n",
      "03/03 08:50:01 PM |\t  59.2%\n",
      "03/03 08:51:04 PM |\t  60.8%\n",
      "03/03 08:52:02 PM |\t  62.4%\n",
      "03/03 08:53:06 PM |\t  64.0%\n",
      "03/03 08:54:06 PM |\t  65.6%\n",
      "03/03 08:55:16 PM |\t  67.2%\n",
      "03/03 08:56:25 PM |\t  68.8%\n",
      "03/03 08:57:33 PM |\t  70.4%\n",
      "03/03 08:58:49 PM |\t  72.0%\n",
      "03/03 08:59:56 PM |\t  73.6%\n",
      "03/03 09:01:01 PM |\t  75.2%\n",
      "03/03 09:02:12 PM |\t  76.8%\n",
      "03/03 09:03:15 PM |\t  78.4%\n",
      "03/03 09:04:18 PM |\t  80.0%\n",
      "03/03 09:05:20 PM |\t  81.6%\n",
      "03/03 09:06:35 PM |\t  83.2%\n",
      "03/03 09:07:47 PM |\t  84.8%\n",
      "03/03 09:08:56 PM |\t  86.4%\n",
      "03/03 09:10:05 PM |\t  88.0%\n",
      "03/03 09:11:15 PM |\t  89.6%\n",
      "03/03 09:12:33 PM |\t  91.2%\n",
      "03/03 09:13:49 PM |\t  92.8%\n",
      "03/03 09:14:47 PM |\t  94.4%\n",
      "03/03 09:15:51 PM |\t  96.0%\n",
      "03/03 09:16:53 PM |\t  97.6%\n",
      "03/03 09:17:53 PM |\t  99.2%\n",
      "03/03 09:18:04 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0011, 0.0011, 0.0011,  ..., 0.0025, 0.0024, 0.0027], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 09:18:04 PM |\t  w_train_loss:0.49154934601392597,v_train_loss:1.669733706396073\n",
      "03/03 09:18:19 PM |\t  x_decoded[:2]:['translate English to German: According to the New York Times, this was the biggest action of this kind in the history of the American fast food industry.', 'translate English to German: City transportation is mainly buses, but Jerusalem has a high-speed tram, and Haifa has the only subway line in the country, comprising six stops and connecting upper town with lower.']\n",
      "03/03 09:18:19 PM |\t  pred_decoded[:2]:['Laut New York Times war dies die größte Aktion dieser Art in der Geschichte der amerikanischen Fast Food-Industrie.', 'In Jerusalem gibt es eine große Anzahl von Bussen, aber Jerusalem hat eine Hochgeschwindigkeitsbahn, und Haifa verfügt über die einzige U-Bahn-Linie im Land, die sechs Stationen umfasst und die obere Stadt mit der unteren verbindet.']\n",
      "03/03 09:18:19 PM |\t  label_decoded[:2]:['Laut New York Times war dies die größte Aktion ihrer Art in der Geschichte der US-amerikanischen Fast-Food-Industrie.', 'In der Stadt kommen Sie zumeist mit dem Bus voran, doch in Jerusalem gibt es auch eine Schnellstraßenbahn und in Haifa die einzige U-Bahn-Linie des Landes mit sechs Stationen, die die Unterstadt mit der Oberstadt verbindet.']\n",
      "03/03 09:23:52 PM |\t  model_w_in_main bleu : 22.301595\n",
      "03/03 09:23:52 PM |\t  model_w_in_main test loss : 0.096205\n",
      "03/03 09:24:00 PM |\t  x_decoded[:2]:['translate English to German: Arizona was the first to introduce such a requirement.', 'translate English to German: The key step is entering the new transaction in the book.']\n",
      "03/03 09:24:00 PM |\t  pred_decoded[:2]:['Arizona war der erste, der eine solche Forderung einführte.', 'Der Schlüsselschritt ist die Eingabe der neuen Transaktion in das Buch.']\n",
      "03/03 09:24:00 PM |\t  label_decoded[:2]:['Arizona war der erste Bundesstaat, der eine derartige Forderung einführte.', 'Der entscheidende Schritt besteht darin, diese neue Transaktion in dieses Buch einzutragen.']\n",
      "03/03 09:29:27 PM |\t  model_v_in_main bleu : 22.068110\n",
      "03/03 09:29:27 PM |\t  model_v_in_main test loss : 0.093532\n",
      "03/03 09:29:28 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:0.0009996644736133356,\t\tlr_v:0.0009996644736133356----------------\n",
      "03/03 09:29:41 PM |\t  0.0%\n",
      "03/03 09:30:54 PM |\t  1.6%\n",
      "03/03 09:32:15 PM |\t  3.2%\n",
      "03/03 09:33:40 PM |\t  4.8%\n",
      "03/03 09:34:43 PM |\t  6.4%\n",
      "03/03 09:35:48 PM |\t  8.0%\n",
      "03/03 09:37:06 PM |\t  9.6%\n",
      "03/03 09:38:46 PM |\t  11.2%\n",
      "03/03 09:40:15 PM |\t  12.8%\n",
      "03/03 09:41:24 PM |\t  14.4%\n",
      "03/03 09:42:21 PM |\t  16.0%\n",
      "03/03 09:43:30 PM |\t  17.6%\n",
      "03/03 09:44:39 PM |\t  19.2%\n",
      "03/03 09:45:43 PM |\t  20.8%\n",
      "03/03 09:46:42 PM |\t  22.4%\n",
      "03/03 09:47:47 PM |\t  24.0%\n",
      "03/03 09:49:09 PM |\t  25.6%\n",
      "03/03 09:50:15 PM |\t  27.2%\n",
      "03/03 09:51:23 PM |\t  28.8%\n",
      "03/03 09:52:23 PM |\t  30.4%\n",
      "03/03 09:53:28 PM |\t  32.0%\n",
      "03/03 09:54:24 PM |\t  33.6%\n",
      "03/03 09:55:47 PM |\t  35.2%\n",
      "03/03 09:57:19 PM |\t  36.8%\n",
      "03/03 09:58:39 PM |\t  38.4%\n",
      "03/03 09:59:45 PM |\t  40.0%\n",
      "03/03 10:00:52 PM |\t  41.6%\n",
      "03/03 10:02:01 PM |\t  43.2%\n",
      "03/03 10:03:12 PM |\t  44.8%\n",
      "03/03 10:04:25 PM |\t  46.4%\n",
      "03/03 10:05:38 PM |\t  48.0%\n",
      "03/03 10:06:54 PM |\t  49.6%\n",
      "03/03 10:08:01 PM |\t  51.2%\n",
      "03/03 10:09:03 PM |\t  52.8%\n",
      "03/03 10:10:27 PM |\t  54.4%\n",
      "03/03 10:12:30 PM |\t  56.0%\n",
      "03/03 10:14:20 PM |\t  57.6%\n",
      "03/03 10:15:39 PM |\t  59.2%\n",
      "03/03 10:16:46 PM |\t  60.8%\n",
      "03/03 10:17:57 PM |\t  62.4%\n",
      "03/03 10:18:54 PM |\t  64.0%\n",
      "03/03 10:19:55 PM |\t  65.6%\n",
      "03/03 10:21:49 PM |\t  67.2%\n",
      "03/03 10:23:14 PM |\t  68.8%\n",
      "03/03 10:25:01 PM |\t  70.4%\n",
      "03/03 10:26:23 PM |\t  72.0%\n",
      "03/03 10:28:19 PM |\t  73.6%\n",
      "03/03 10:30:08 PM |\t  75.2%\n",
      "03/03 10:32:04 PM |\t  76.8%\n",
      "03/03 10:33:45 PM |\t  78.4%\n",
      "03/03 10:35:12 PM |\t  80.0%\n",
      "03/03 10:36:49 PM |\t  81.6%\n",
      "03/03 10:38:20 PM |\t  83.2%\n",
      "03/03 10:40:11 PM |\t  84.8%\n",
      "03/03 10:41:47 PM |\t  86.4%\n",
      "03/03 10:42:56 PM |\t  88.0%\n",
      "03/03 10:43:48 PM |\t  89.6%\n",
      "03/03 10:44:39 PM |\t  91.2%\n",
      "03/03 10:45:33 PM |\t  92.8%\n",
      "03/03 10:46:40 PM |\t  94.4%\n",
      "03/03 10:47:40 PM |\t  96.0%\n",
      "03/03 10:48:42 PM |\t  97.6%\n",
      "03/03 10:49:44 PM |\t  99.2%\n",
      "03/03 10:50:00 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0011, 0.0011, 0.0011,  ..., 0.0025, 0.0024, 0.0027], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "03/03 10:50:00 PM |\t  w_train_loss:0.46425679262029007,v_train_loss:1.6039225219283253\n",
      "03/03 10:50:09 PM |\t  x_decoded[:2]:['translate English to German: What is more, in 2011 Republican legislators sponsored laws abolishing the registration of voters on election day in eight States.', 'translate English to German: This phenomenon gained momentum following the November 2010 elections, which saw 675 new Republican representatives added in 26 States.']\n",
      "03/03 10:50:09 PM |\t  pred_decoded[:2]:['Darüber hinaus haben republikanische Gesetzgeber 2011 Gesetze gesponsert, die die Registrierung von Wählern am Wahltag in acht Staaten abschaffen.', 'Dieses Phänomen gewinnt an Dynamik nach den Wahlen im November 2010, bei denen 675 neue republikanische Vertreter in 26 Staaten hinzugefügt wurden.']\n",
      "03/03 10:50:09 PM |\t  label_decoded[:2]:['Darüber hinaus haben republikanische Gesetzgeber 2011 in acht Bundesstaaten Gesetze gefördert, mit denen die Registrierung von Wählern am Wahltag abgeschafft wurde.', 'Dieses Phänomen hat nach den Wahlen vom November 2010 an Bedeutung gewonnen, bei denen 675 neue republikanische Vertreter in 26 Staaten verzeichnet werden konnten.']\n",
      "03/03 10:55:15 PM |\t  model_w_in_main bleu : 22.153478\n",
      "03/03 10:55:15 PM |\t  model_w_in_main test loss : 0.093536\n",
      "03/03 10:55:41 PM |\t  x_decoded[:2]:['translate English to German: Rights protection should begin before their departure.', 'translate English to German: What now?']\n",
      "03/03 10:55:41 PM |\t  pred_decoded[:2]:['Der Schutz der Rechte sollte vor ihrer Abreise beginnen.', 'Was jetzt?']\n",
      "03/03 10:55:41 PM |\t  label_decoded[:2]:['Ihre Rechte müssen jedoch bereits vor ihrer Abreise geschützt werden.', 'Und danach?']\n",
      "03/03 11:00:27 PM |\t  model_v_in_main bleu : 22.103392\n",
      "03/03 11:00:27 PM |\t  model_v_in_main test loss : 0.089775\n",
      "03/03 11:00:28 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:0.0009995362042424009,\t\tlr_v:0.0009995362042424009----------------\n",
      "03/03 11:00:40 PM |\t  0.0%\n",
      "03/03 11:01:24 PM |\t  1.6%\n",
      "03/03 11:02:22 PM |\t  3.2%\n",
      "03/03 11:03:27 PM |\t  4.8%\n",
      "03/03 11:04:38 PM |\t  6.4%\n",
      "03/03 11:05:32 PM |\t  8.0%\n",
      "03/03 11:06:32 PM |\t  9.6%\n",
      "03/03 11:07:13 PM |\t  11.2%\n",
      "03/03 11:08:09 PM |\t  12.8%\n",
      "03/03 11:08:56 PM |\t  14.4%\n",
      "03/03 11:09:52 PM |\t  16.0%\n",
      "03/03 11:10:46 PM |\t  17.6%\n",
      "03/03 11:11:40 PM |\t  19.2%\n",
      "03/03 11:12:31 PM |\t  20.8%\n",
      "03/03 11:13:25 PM |\t  22.4%\n",
      "03/03 11:14:16 PM |\t  24.0%\n",
      "03/03 11:15:18 PM |\t  25.6%\n",
      "03/03 11:16:24 PM |\t  27.2%\n",
      "03/03 11:17:35 PM |\t  28.8%\n",
      "03/03 11:18:38 PM |\t  30.4%\n",
      "03/03 11:19:28 PM |\t  32.0%\n",
      "03/03 11:20:27 PM |\t  33.6%\n",
      "03/03 11:21:16 PM |\t  35.2%\n",
      "03/03 11:22:14 PM |\t  36.8%\n",
      "03/03 11:23:10 PM |\t  38.4%\n",
      "03/03 11:24:32 PM |\t  40.0%\n",
      "03/03 11:25:57 PM |\t  41.6%\n",
      "03/03 11:27:18 PM |\t  43.2%\n",
      "03/03 11:28:52 PM |\t  44.8%\n",
      "03/03 11:30:31 PM |\t  46.4%\n",
      "03/03 11:32:13 PM |\t  48.0%\n",
      "03/03 11:33:52 PM |\t  49.6%\n",
      "03/03 11:35:28 PM |\t  51.2%\n",
      "03/03 11:37:10 PM |\t  52.8%\n",
      "03/03 11:39:08 PM |\t  54.4%\n",
      "03/03 11:41:05 PM |\t  56.0%\n",
      "03/03 11:43:18 PM |\t  57.6%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_test(valid_dataloader,model_w,-1) #before train\n",
    "my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(epochs):\n",
    "\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "\n",
    "    \n",
    "    my_test(valid_dataloader,model_w,epoch) \n",
    "    my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "    torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "    torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "     \n",
    "   \n",
    "   \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
