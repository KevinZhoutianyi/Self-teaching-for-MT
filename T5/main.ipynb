{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=20,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=8,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=4,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=4,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--model_name', type=str,                   default='t5-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='smooth64',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default='25',      help='howmany step report once')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=128,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-4,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.7,    help='momentum')\n",
    "parser.add_argument('--smoothing', type=float,                   default=0.0,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.9,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=0,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5\\wandb\\run-20220406_182859-1s6widy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/BeforeA/runs/1s6widy5\" target=\"_blank\">smooth64</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/BeforeA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/BeforeA/runs/1s6widy5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ad9c35fbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "# os.environ['WANDB_NOTES']='train without A,withoutAandt5smallandbatch64 '\n",
    "wandb.init(project=\"BeforeA\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 06:29:08 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 06:29:08 PM |\t  Namespace(A_lr=0.0001, batch_size=20, decay=0.001, epochs=50, exp_name='smooth64', gpu=0, grad_acc_count=128, grad_clip=1, learning_rate_min=1e-08, model_name='t5-small', momentum=0.7, pre_epochs=0, rep_num=25, smoothing=0.0, syndata_loss_ratio=0.1, train_A=0, train_A_num_points=4, train_num_points=1000, train_v_num_points=4, train_v_synthetic_num_points=4, train_w_num_points=8, traindata_loss_ratio=0.9, v_lr=0.001, valid_begin=0, valid_num_points=100, w_lr=0.001)\n",
      "04/06 06:29:08 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "04/06 06:29:08 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 06:29:10 PM |\t  modelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,modelname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 06:29:42 PM |\t  train len: 1000\n",
      "04/06 06:29:42 PM |\t  train_w_num_points_len: 400\n",
      "04/06 06:29:42 PM |\t  train_v_synthetic_num_points_len: 200\n",
      "04/06 06:29:42 PM |\t  train_v_num_points_len: 200\n",
      "04/06 06:29:42 PM |\t  train_A_num_points_len: 200\n",
      "04/06 06:29:42 PM |\t  valid len: 100\n",
      "04/06 06:29:42 PM |\t  test len: 3003\n",
      "04/06 06:29:42 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "04/06 06:29:42 PM |\t  {'de': 'Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.', 'en': 'translate English to German: However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.'}\n",
      "04/06 06:29:42 PM |\t  {'de': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'en': 'translate English to German: Two sets of lights so close to one another: intentional or just a silly error?'}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelname)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none',label_smoothing=args.smoothing)#,ignore_index = tokenizer.pad_token_id)#\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][:args.valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en'] \n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "04/06 06:29:42 PM |\t  train data get\n",
      "04/06 06:29:42 PM |\t  train data loader get\n",
      "04/06 06:29:42 PM |\t  valid data loader get\n",
      "04/06 06:29:44 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler=SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=4)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=16, pin_memory=True, num_workers=4)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=16, pin_memory=True, num_workers=4)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = Adafactor(model_w.parameters(), scale_parameter=True, relative_step=True, warmup_init=False,clip_threshold=1,beta1=0)\n",
    "scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer =Adafactor(model_v.parameters(), scale_parameter=True, relative_step=True, warmup_init=False, clip_threshold=1,beta1=0)\n",
    "scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    metric_bleu =  load_metric('bleu')\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        with torch.no_grad():\n",
    "            ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "            acc+= ls\n",
    "            counter+= 1\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "            pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "            label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "            \n",
    "            pred_str = [x.replace('.', '')  for x in pred_decoded]\n",
    "            label_str = [[x.replace('.', '')] for x in label_decoded]\n",
    "            pred_list = [x.replace('.', '').split()  for x in pred_decoded]\n",
    "            label_list = [[x.replace('.', '').split()] for x in label_decoded]\n",
    "            if  step%100==0:\n",
    "                logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "            metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "            \n",
    "    logging.info('computing score...')            \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    writer.add_scalar(model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/sacreBLEU\",sacrebleu_score['score'], global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/BLEU\",bleu_score['bleu'], global_step=epoch)\n",
    "    \n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    \n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    wsize = args.train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points \n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points \n",
    "    grad_acc_count = args.grad_acc_count\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size=[wsize,synsize,vsize,Asize]\n",
    "    for step, batch in enumerate(_dataloader) :\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=True) \n",
    "        (input_w,input_syn,input_v,input_A_v) = torch.split(train_x,split_size)\n",
    "        (input_w_attn,input_syn_attn,input_v_attn,input_A_v_attn) = torch.split(train_x_attn,split_size)\n",
    "        (output_w,_,output_v,output_A_v) = torch.split(train_y,split_size)\n",
    "        (output_w_attn,_,output_v_attn,output_A_v_attn) = torch.split(train_y_attn,split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "       \n",
    "\n",
    "        if (epoch <= args.epochs) and (args.train_A == 1) and epoch >= args.pre_epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "        \n",
    "        \n",
    "        if  epoch <= args.epochs:\n",
    "            for p in w_model.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            \n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "            loss_w.backward()\n",
    "            objs_w.update(loss_w.item(), wsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "                w_optimizer.step()\n",
    "                w_optimizer.zero_grad()\n",
    "            for p in w_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        if epoch >= args.pre_epochs and epoch <= args.epochs:\n",
    "            \n",
    "            for p in v_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "            loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            v_loss =  (args.traindata_loss_ratio*loss+loss_aug*args.syndata_loss_ratio)/num_batch\n",
    "            v_trainloss_acc+=v_loss.item()\n",
    "            v_loss.backward()\n",
    "            objs_v.update(v_loss.item(), vtrainsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "                v_optimizer.step()  \n",
    "                v_optimizer.zero_grad() \n",
    "            for p in v_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        fre = (loader_len//args.rep_num)\n",
    "        if((step)%fre == 0 or (step)==(loader_len-1)):\n",
    "            logging.info(f\"{progress:5.3}% \\t w_loss_avg:{objs_w.avg*train_w_num_points_len:^.7f}\\t v_loss_avg:{objs_v.avg*vtrainsize_total:^.7f}\")\n",
    "  \n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    \n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 06:29:44 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 976195072\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:29:51 PM |\t    0.0% \t w_loss_avg:0.8071423\t v_loss_avg:0.5683495\n",
      "5 976195072\n",
      "step 1\n",
      "fre 2\n",
      "5 976195072\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:29:54 PM |\t   4.08% \t w_loss_avg:1.0335226\t v_loss_avg:0.8821828\n",
      "5 976195072\n",
      "step 3\n",
      "fre 2\n",
      "5 976195072\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:29:56 PM |\t   8.16% \t w_loss_avg:1.1525814\t v_loss_avg:0.9598331\n",
      "5 976195072\n",
      "step 5\n",
      "fre 2\n",
      "5 976195072\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:30:00 PM |\t   12.2% \t w_loss_avg:1.0834214\t v_loss_avg:0.8089365\n",
      "5 976195072\n",
      "step 7\n",
      "fre 2\n",
      "5 976195072\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:30:02 PM |\t   16.3% \t w_loss_avg:1.0367713\t v_loss_avg:0.8720013\n",
      "5 976195072\n",
      "step 9\n",
      "fre 2\n",
      "5 976195072\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:30:06 PM |\t   20.4% \t w_loss_avg:1.0625971\t v_loss_avg:0.8967040\n",
      "5 976195072\n",
      "step 11\n",
      "fre 2\n",
      "5 976195072\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:30:09 PM |\t   24.5% \t w_loss_avg:1.0282095\t v_loss_avg:0.8602834\n",
      "5 976195072\n",
      "step 13\n",
      "fre 2\n",
      "5 976195072\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:30:11 PM |\t   28.6% \t w_loss_avg:1.0092556\t v_loss_avg:0.9151338\n",
      "5 976195072\n",
      "step 15\n",
      "fre 2\n",
      "5 976195072\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:30:14 PM |\t   32.7% \t w_loss_avg:0.9993890\t v_loss_avg:0.9193297\n",
      "5 976195072\n",
      "step 17\n",
      "fre 2\n",
      "5 976195072\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:30:16 PM |\t   36.7% \t w_loss_avg:1.0595784\t v_loss_avg:0.9215244\n",
      "5 976195072\n",
      "step 19\n",
      "fre 2\n",
      "5 976195072\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:30:19 PM |\t   40.8% \t w_loss_avg:1.0677685\t v_loss_avg:0.9155134\n",
      "5 976195072\n",
      "step 21\n",
      "fre 2\n",
      "5 976195072\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:30:22 PM |\t   44.9% \t w_loss_avg:1.0560158\t v_loss_avg:0.9224014\n",
      "5 976195072\n",
      "step 23\n",
      "fre 2\n",
      "5 976195072\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:30:24 PM |\t   49.0% \t w_loss_avg:1.0710307\t v_loss_avg:0.9716510\n",
      "5 976195072\n",
      "step 25\n",
      "fre 2\n",
      "5 976195072\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:30:27 PM |\t   53.1% \t w_loss_avg:1.1047437\t v_loss_avg:0.9844702\n",
      "5 976195072\n",
      "step 27\n",
      "fre 2\n",
      "5 976195072\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:30:31 PM |\t   57.1% \t w_loss_avg:1.1097460\t v_loss_avg:1.0413771\n",
      "5 976195072\n",
      "step 29\n",
      "fre 2\n",
      "5 976195072\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:30:34 PM |\t   61.2% \t w_loss_avg:1.1317889\t v_loss_avg:1.0419895\n",
      "5 976195072\n",
      "step 31\n",
      "fre 2\n",
      "5 976195072\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:30:36 PM |\t   65.3% \t w_loss_avg:1.1574289\t v_loss_avg:1.0315281\n",
      "5 976195072\n",
      "step 33\n",
      "fre 2\n",
      "5 976195072\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:30:39 PM |\t   69.4% \t w_loss_avg:1.1513765\t v_loss_avg:1.0497546\n",
      "5 976195072\n",
      "step 35\n",
      "fre 2\n",
      "5 976195072\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:30:42 PM |\t   73.5% \t w_loss_avg:1.1233672\t v_loss_avg:1.0472641\n",
      "5 976195072\n",
      "step 37\n",
      "fre 2\n",
      "5 976195072\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:30:44 PM |\t   77.6% \t w_loss_avg:1.1107500\t v_loss_avg:1.0268360\n",
      "5 976195072\n",
      "step 39\n",
      "fre 2\n",
      "5 976195072\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:30:47 PM |\t   81.6% \t w_loss_avg:1.1153288\t v_loss_avg:1.0246668\n",
      "5 976195072\n",
      "step 41\n",
      "fre 2\n",
      "5 976195072\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:30:49 PM |\t   85.7% \t w_loss_avg:1.1093231\t v_loss_avg:1.0131324\n",
      "5 976195072\n",
      "step 43\n",
      "fre 2\n",
      "5 976195072\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:30:52 PM |\t   89.8% \t w_loss_avg:1.0991941\t v_loss_avg:1.0116016\n",
      "5 976195072\n",
      "step 45\n",
      "fre 2\n",
      "5 976195072\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:30:54 PM |\t   93.9% \t w_loss_avg:1.0962669\t v_loss_avg:0.9973123\n",
      "5 976195072\n",
      "step 47\n",
      "fre 2\n",
      "5 976195072\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:30:57 PM |\t   98.0% \t w_loss_avg:1.1040308\t v_loss_avg:0.9878763\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:30:59 PM |\t  1e+02% \t w_loss_avg:1.1169611\t v_loss_avg:0.9858638\n",
      "04/06 06:30:59 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:30:59 PM |\t  w_train_loss:1.1169611006043851,v_train_loss:0.9858637750148773\n",
      "04/06 06:31:05 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:31:05 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanische Führung hat die Wahlbekämpfung gerechtfertigt.']\n",
      "04/06 06:31:05 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:31:20 PM |\t  computing score...\n",
      "04/06 06:31:20 PM |\t  model_w_in_main sacreBLEU : 9.586488\n",
      "04/06 06:31:20 PM |\t  model_w_in_main BLEU : 0.072908\n",
      "04/06 06:31:20 PM |\t  model_w_in_main test loss : 1.389931\n",
      "04/06 06:31:27 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:31:27 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Führer haben ihre Politik durch die Bekämpfung von Wahlbetrug begründet.']\n",
      "04/06 06:31:27 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:31:43 PM |\t  computing score...\n",
      "04/06 06:31:43 PM |\t  model_v_in_main sacreBLEU : 11.887770\n",
      "04/06 06:31:43 PM |\t  model_v_in_main BLEU : 0.098066\n",
      "04/06 06:31:43 PM |\t  model_v_in_main test loss : 1.312482\n",
      "04/06 06:31:43 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:31:47 PM |\t    0.0% \t w_loss_avg:1.1633261\t v_loss_avg:0.9900553\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:31:54 PM |\t   4.08% \t w_loss_avg:1.2922687\t v_loss_avg:1.1870565\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:31:59 PM |\t   8.16% \t w_loss_avg:1.3628923\t v_loss_avg:1.1554078\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:32:11 PM |\t   12.2% \t w_loss_avg:1.3225131\t v_loss_avg:1.0195453\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:32:14 PM |\t   16.3% \t w_loss_avg:1.2942180\t v_loss_avg:1.0128503\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:32:23 PM |\t   20.4% \t w_loss_avg:1.3406322\t v_loss_avg:0.9977984\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:32:29 PM |\t   24.5% \t w_loss_avg:1.3325090\t v_loss_avg:1.0098860\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:32:34 PM |\t   28.6% \t w_loss_avg:1.3262747\t v_loss_avg:1.0510036\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:32:37 PM |\t   32.7% \t w_loss_avg:1.3132594\t v_loss_avg:1.0538355\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:32:48 PM |\t   36.7% \t w_loss_avg:1.3400391\t v_loss_avg:1.0695946\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:32:51 PM |\t   40.8% \t w_loss_avg:1.3574428\t v_loss_avg:1.0691743\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:32:55 PM |\t   44.9% \t w_loss_avg:1.3539279\t v_loss_avg:1.0741460\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:33:01 PM |\t   49.0% \t w_loss_avg:1.3661476\t v_loss_avg:1.0988691\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:33:09 PM |\t   53.1% \t w_loss_avg:1.4022622\t v_loss_avg:1.1090315\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:33:22 PM |\t   57.1% \t w_loss_avg:1.4172384\t v_loss_avg:1.1305780\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:33:28 PM |\t   61.2% \t w_loss_avg:1.4364703\t v_loss_avg:1.1282494\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:33:33 PM |\t   65.3% \t w_loss_avg:1.4405326\t v_loss_avg:1.1277032\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:33:36 PM |\t   69.4% \t w_loss_avg:1.4427732\t v_loss_avg:1.1310936\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:33:44 PM |\t   73.5% \t w_loss_avg:1.4138209\t v_loss_avg:1.1311133\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:33:51 PM |\t   77.6% \t w_loss_avg:1.4009493\t v_loss_avg:1.1200553\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:33:54 PM |\t   81.6% \t w_loss_avg:1.4087514\t v_loss_avg:1.1229574\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:34:05 PM |\t   85.7% \t w_loss_avg:1.4087472\t v_loss_avg:1.1148456\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:34:08 PM |\t   89.8% \t w_loss_avg:1.3990077\t v_loss_avg:1.1118925\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:34:11 PM |\t   93.9% \t w_loss_avg:1.4005095\t v_loss_avg:1.1033224\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:34:15 PM |\t   98.0% \t w_loss_avg:1.4085362\t v_loss_avg:1.0968740\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:34:17 PM |\t  1e+02% \t w_loss_avg:1.4170141\t v_loss_avg:1.0992384\n",
      "04/06 06:34:17 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:34:17 PM |\t  w_train_loss:1.4170141275972128,v_train_loss:1.0992384450510144\n",
      "04/06 06:34:22 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:34:22 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Staats- und Regierungschefs haben ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug gerechtfertigt.']\n",
      "04/06 06:34:22 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:34:35 PM |\t  computing score...\n",
      "04/06 06:34:35 PM |\t  model_w_in_main sacreBLEU : 23.304934\n",
      "04/06 06:34:35 PM |\t  model_w_in_main BLEU : 0.195874\n",
      "04/06 06:34:35 PM |\t  model_w_in_main test loss : 1.136988\n",
      "04/06 06:34:50 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:34:50 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Staats- und Regierungschefs rechtfertigten ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug.']\n",
      "04/06 06:34:50 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:35:04 PM |\t  computing score...\n",
      "04/06 06:35:04 PM |\t  model_v_in_main sacreBLEU : 23.960325\n",
      "04/06 06:35:04 PM |\t  model_v_in_main BLEU : 0.203077\n",
      "04/06 06:35:04 PM |\t  model_v_in_main test loss : 1.133852\n",
      "04/06 06:35:04 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:35:07 PM |\t    0.0% \t w_loss_avg:0.8067073\t v_loss_avg:0.3495602\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:35:10 PM |\t   4.08% \t w_loss_avg:0.8110176\t v_loss_avg:0.5325512\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:35:13 PM |\t   8.16% \t w_loss_avg:0.8470831\t v_loss_avg:0.5659957\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:35:23 PM |\t   12.2% \t w_loss_avg:0.8345710\t v_loss_avg:0.4950008\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:35:25 PM |\t   16.3% \t w_loss_avg:0.8000427\t v_loss_avg:0.5048521\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:35:35 PM |\t   20.4% \t w_loss_avg:0.8261001\t v_loss_avg:0.5206970\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:35:42 PM |\t   24.5% \t w_loss_avg:0.8101929\t v_loss_avg:0.5136435\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:35:45 PM |\t   28.6% \t w_loss_avg:0.7947970\t v_loss_avg:0.5800037\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:35:47 PM |\t   32.7% \t w_loss_avg:0.7986797\t v_loss_avg:0.5860754\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:35:51 PM |\t   36.7% \t w_loss_avg:0.8311929\t v_loss_avg:0.6018828\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:35:54 PM |\t   40.8% \t w_loss_avg:0.8466840\t v_loss_avg:0.6004756\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:35:57 PM |\t   44.9% \t w_loss_avg:0.8484505\t v_loss_avg:0.6075749\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:35:59 PM |\t   49.0% \t w_loss_avg:0.8688599\t v_loss_avg:0.6444869\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:36:09 PM |\t   53.1% \t w_loss_avg:0.9065156\t v_loss_avg:0.6629117\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:36:26 PM |\t   57.1% \t w_loss_avg:0.9255037\t v_loss_avg:0.6920799\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:36:29 PM |\t   61.2% \t w_loss_avg:0.9503959\t v_loss_avg:0.7049295\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:36:32 PM |\t   65.3% \t w_loss_avg:0.9531363\t v_loss_avg:0.7020311\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:36:35 PM |\t   69.4% \t w_loss_avg:0.9527232\t v_loss_avg:0.7202804\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:36:41 PM |\t   73.5% \t w_loss_avg:0.9253878\t v_loss_avg:0.7195186\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:36:44 PM |\t   77.6% \t w_loss_avg:0.9147765\t v_loss_avg:0.7082006\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:36:46 PM |\t   81.6% \t w_loss_avg:0.9263164\t v_loss_avg:0.7095981\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:36:49 PM |\t   85.7% \t w_loss_avg:0.9222317\t v_loss_avg:0.6963446\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:36:52 PM |\t   89.8% \t w_loss_avg:0.9120863\t v_loss_avg:0.6936623\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:36:54 PM |\t   93.9% \t w_loss_avg:0.9107070\t v_loss_avg:0.6804698\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:36:58 PM |\t   98.0% \t w_loss_avg:0.9232993\t v_loss_avg:0.6777158\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:36:59 PM |\t  1e+02% \t w_loss_avg:0.9335569\t v_loss_avg:0.6777753\n",
      "04/06 06:36:59 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:36:59 PM |\t  w_train_loss:0.9335568591486663,v_train_loss:0.6777752814814448\n",
      "04/06 06:37:04 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:37:04 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie', 'Die republikanischen Führer rechtfertigen ihre Politik.']\n",
      "04/06 06:37:04 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:37:13 PM |\t  computing score...\n",
      "04/06 06:37:13 PM |\t  model_w_in_main sacreBLEU : 8.248934\n",
      "04/06 06:37:13 PM |\t  model_w_in_main BLEU : 0.076035\n",
      "04/06 06:37:13 PM |\t  model_w_in_main test loss : 1.591103\n",
      "04/06 06:37:18 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:37:18 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama zu entziehen', 'Die republikanische Führung hat ihre Politik dadurch rechtfertigt, dass es notwendig ist, Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:37:18 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:37:29 PM |\t  computing score...\n",
      "04/06 06:37:29 PM |\t  model_v_in_main sacreBLEU : 13.916492\n",
      "04/06 06:37:29 PM |\t  model_v_in_main BLEU : 0.123049\n",
      "04/06 06:37:30 PM |\t  model_v_in_main test loss : 1.364832\n",
      "04/06 06:37:30 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:37:32 PM |\t    0.0% \t w_loss_avg:0.9734402\t v_loss_avg:0.2679562\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:37:34 PM |\t   4.08% \t w_loss_avg:1.0303685\t v_loss_avg:0.4788319\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:37:35 PM |\t   8.16% \t w_loss_avg:1.1546764\t v_loss_avg:0.5163791\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:37:37 PM |\t   12.2% \t w_loss_avg:1.0917132\t v_loss_avg:0.4520570\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:37:39 PM |\t   16.3% \t w_loss_avg:1.0461983\t v_loss_avg:0.4727110\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:37:40 PM |\t   20.4% \t w_loss_avg:1.0474478\t v_loss_avg:0.4652532\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:37:42 PM |\t   24.5% \t w_loss_avg:1.0255793\t v_loss_avg:0.4825222\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:37:44 PM |\t   28.6% \t w_loss_avg:1.0398750\t v_loss_avg:0.5419739\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:37:45 PM |\t   32.7% \t w_loss_avg:1.0449214\t v_loss_avg:0.5559094\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:37:47 PM |\t   36.7% \t w_loss_avg:1.0752025\t v_loss_avg:0.5719685\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:37:49 PM |\t   40.8% \t w_loss_avg:1.0767275\t v_loss_avg:0.5656153\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:37:51 PM |\t   44.9% \t w_loss_avg:1.0889356\t v_loss_avg:0.5705384\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:37:52 PM |\t   49.0% \t w_loss_avg:1.1113881\t v_loss_avg:0.5928134\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:37:54 PM |\t   53.1% \t w_loss_avg:1.1575616\t v_loss_avg:0.6122083\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:37:56 PM |\t   57.1% \t w_loss_avg:1.1797337\t v_loss_avg:0.6401270\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:37:57 PM |\t   61.2% \t w_loss_avg:1.1977007\t v_loss_avg:0.6455818\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:37:59 PM |\t   65.3% \t w_loss_avg:1.2048071\t v_loss_avg:0.6454882\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:38:01 PM |\t   69.4% \t w_loss_avg:1.2133770\t v_loss_avg:0.6525337\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:38:03 PM |\t   73.5% \t w_loss_avg:1.1852614\t v_loss_avg:0.6580360\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:38:04 PM |\t   77.6% \t w_loss_avg:1.1758050\t v_loss_avg:0.6468705\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:38:06 PM |\t   81.6% \t w_loss_avg:1.1889588\t v_loss_avg:0.6506331\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:38:08 PM |\t   85.7% \t w_loss_avg:1.1915817\t v_loss_avg:0.6471909\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:38:10 PM |\t   89.8% \t w_loss_avg:1.1757780\t v_loss_avg:0.6438780\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:38:11 PM |\t   93.9% \t w_loss_avg:1.1636135\t v_loss_avg:0.6363480\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:38:13 PM |\t   98.0% \t w_loss_avg:1.1708033\t v_loss_avg:0.6296238\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:38:22 PM |\t  1e+02% \t w_loss_avg:1.1809278\t v_loss_avg:0.6319209\n",
      "04/06 06:38:23 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:38:23 PM |\t  w_train_loss:1.1809277590364218,v_train_loss:0.6319208620116115\n",
      "04/06 06:38:42 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:38:42 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl Obamas durch Obama durch die Wiedereingliederung von Obama in die Wahlen von Barack Obama', 'Die republikanischen Staats- und Regierungschefs rechtfertigten ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug, die notwendigen Maßnahmen gegen Wahlfälschungen zu bekämpfen.']\n",
      "04/06 06:38:42 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:40:12 PM |\t  computing score...\n",
      "04/06 06:40:12 PM |\t  model_w_in_main sacreBLEU : 7.257138\n",
      "04/06 06:40:12 PM |\t  model_w_in_main BLEU : 0.057422\n",
      "04/06 06:40:12 PM |\t  model_w_in_main test loss : 1.300846\n",
      "04/06 06:40:33 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:40:33 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama durch die Republiken, die der Rewahl Obamas entgegenstehen,', 'Die führenden republikanischen Politiker haben ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug durch den Kampf gegen Wahlfälschungen in den Wahlkreisen in ihrer Heimatregion rechtfertigt, Wahltäuschungen zu bekämpfen.']\n",
      "04/06 06:40:33 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:42:10 PM |\t  computing score...\n",
      "04/06 06:42:10 PM |\t  model_v_in_main sacreBLEU : 3.302847\n",
      "04/06 06:42:10 PM |\t  model_v_in_main BLEU : 0.025799\n",
      "04/06 06:42:10 PM |\t  model_v_in_main test loss : 1.513196\n",
      "04/06 06:42:10 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:42:21 PM |\t    0.0% \t w_loss_avg:0.9322966\t v_loss_avg:0.6175735\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:42:39 PM |\t   4.08% \t w_loss_avg:0.8810892\t v_loss_avg:0.7835334\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:42:56 PM |\t   8.16% \t w_loss_avg:0.9263699\t v_loss_avg:0.7744699\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:43:14 PM |\t   12.2% \t w_loss_avg:0.9253409\t v_loss_avg:0.7325283\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:43:31 PM |\t   16.3% \t w_loss_avg:0.9010274\t v_loss_avg:0.7503763\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:43:49 PM |\t   20.4% \t w_loss_avg:0.9168955\t v_loss_avg:0.7485984\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:44:07 PM |\t   24.5% \t w_loss_avg:0.9032866\t v_loss_avg:0.7439971\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:44:25 PM |\t   28.6% \t w_loss_avg:0.9001102\t v_loss_avg:0.8001386\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:44:43 PM |\t   32.7% \t w_loss_avg:0.8988667\t v_loss_avg:0.8100077\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:45:01 PM |\t   36.7% \t w_loss_avg:0.9216980\t v_loss_avg:0.8179589\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:45:19 PM |\t   40.8% \t w_loss_avg:0.9265785\t v_loss_avg:0.8208143\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:45:37 PM |\t   44.9% \t w_loss_avg:0.9328764\t v_loss_avg:0.8391478\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:45:55 PM |\t   49.0% \t w_loss_avg:0.9423106\t v_loss_avg:0.8562522\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:46:12 PM |\t   53.1% \t w_loss_avg:0.9709127\t v_loss_avg:0.8654531\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:46:30 PM |\t   57.1% \t w_loss_avg:0.9801967\t v_loss_avg:0.8865055\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:46:48 PM |\t   61.2% \t w_loss_avg:0.9954379\t v_loss_avg:0.8937496\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:47:06 PM |\t   65.3% \t w_loss_avg:0.9951667\t v_loss_avg:0.8954632\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:47:24 PM |\t   69.4% \t w_loss_avg:0.9898053\t v_loss_avg:0.9024977\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:47:42 PM |\t   73.5% \t w_loss_avg:0.9771608\t v_loss_avg:0.8994199\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:48:00 PM |\t   77.6% \t w_loss_avg:0.9742933\t v_loss_avg:0.8912466\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:48:17 PM |\t   81.6% \t w_loss_avg:0.9786175\t v_loss_avg:0.8946100\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:48:35 PM |\t   85.7% \t w_loss_avg:0.9778285\t v_loss_avg:0.8901922\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:48:53 PM |\t   89.8% \t w_loss_avg:0.9656522\t v_loss_avg:0.8919075\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:49:11 PM |\t   93.9% \t w_loss_avg:0.9615309\t v_loss_avg:0.8801215\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:49:29 PM |\t   98.0% \t w_loss_avg:0.9700442\t v_loss_avg:0.8708741\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:49:30 PM |\t  1e+02% \t w_loss_avg:0.9768195\t v_loss_avg:0.8713894\n",
      "04/06 06:49:30 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:49:30 PM |\t  w_train_loss:0.9768195003271103,v_train_loss:0.8713893564417958\n",
      "04/06 06:49:40 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:49:40 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie gegen die Wiederwahl von Obama', 'Die republikanischen Führer rechtfertigten ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug.']\n",
      "04/06 06:49:40 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:49:51 PM |\t  computing score...\n",
      "04/06 06:49:51 PM |\t  model_w_in_main sacreBLEU : 13.859257\n",
      "04/06 06:49:51 PM |\t  model_w_in_main BLEU : 0.119638\n",
      "04/06 06:49:51 PM |\t  model_w_in_main test loss : 1.313473\n",
      "04/06 06:49:56 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:49:56 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie gegen die Wiederwahl von Obama', 'Die republikanischen Politiker rechtfertigten ihre Politik durch die Notwendigkeit, Wahlbetrug abzubauen.']\n",
      "04/06 06:49:56 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:50:07 PM |\t  computing score...\n",
      "04/06 06:50:07 PM |\t  model_v_in_main sacreBLEU : 15.424639\n",
      "04/06 06:50:07 PM |\t  model_v_in_main BLEU : 0.133536\n",
      "04/06 06:50:07 PM |\t  model_v_in_main test loss : 1.299998\n",
      "04/06 06:50:07 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:50:10 PM |\t    0.0% \t w_loss_avg:0.6513210\t v_loss_avg:0.2969422\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:50:12 PM |\t   4.08% \t w_loss_avg:0.6779897\t v_loss_avg:0.4648031\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:50:13 PM |\t   8.16% \t w_loss_avg:0.7556661\t v_loss_avg:0.4726676\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:50:15 PM |\t   12.2% \t w_loss_avg:0.7426568\t v_loss_avg:0.4188796\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:50:18 PM |\t   16.3% \t w_loss_avg:0.7047593\t v_loss_avg:0.4496805\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:50:19 PM |\t   20.4% \t w_loss_avg:0.7131393\t v_loss_avg:0.4350205\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:50:22 PM |\t   24.5% \t w_loss_avg:0.7008297\t v_loss_avg:0.4448510\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:50:24 PM |\t   28.6% \t w_loss_avg:0.6885776\t v_loss_avg:0.4932979\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:50:26 PM |\t   32.7% \t w_loss_avg:0.6849988\t v_loss_avg:0.4974611\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:50:28 PM |\t   36.7% \t w_loss_avg:0.7147772\t v_loss_avg:0.5080147\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:50:30 PM |\t   40.8% \t w_loss_avg:0.7295553\t v_loss_avg:0.5098390\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:50:32 PM |\t   44.9% \t w_loss_avg:0.7274165\t v_loss_avg:0.5177561\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:50:34 PM |\t   49.0% \t w_loss_avg:0.7407796\t v_loss_avg:0.5433839\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:50:36 PM |\t   53.1% \t w_loss_avg:0.7721467\t v_loss_avg:0.5637797\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:50:39 PM |\t   57.1% \t w_loss_avg:0.7936798\t v_loss_avg:0.5884438\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:50:40 PM |\t   61.2% \t w_loss_avg:0.8141681\t v_loss_avg:0.5963789\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:50:43 PM |\t   65.3% \t w_loss_avg:0.8214047\t v_loss_avg:0.5889246\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:50:45 PM |\t   69.4% \t w_loss_avg:0.8216034\t v_loss_avg:0.5981219\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:50:47 PM |\t   73.5% \t w_loss_avg:0.7989956\t v_loss_avg:0.5949480\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:50:49 PM |\t   77.6% \t w_loss_avg:0.7923159\t v_loss_avg:0.5842972\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:50:51 PM |\t   81.6% \t w_loss_avg:0.7998373\t v_loss_avg:0.5891344\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:50:52 PM |\t   85.7% \t w_loss_avg:0.7984729\t v_loss_avg:0.5810881\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:50:54 PM |\t   89.8% \t w_loss_avg:0.7853250\t v_loss_avg:0.5722087\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:50:56 PM |\t   93.9% \t w_loss_avg:0.7776609\t v_loss_avg:0.5664581\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:50:58 PM |\t   98.0% \t w_loss_avg:0.7881902\t v_loss_avg:0.5615563\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:50:59 PM |\t  1e+02% \t w_loss_avg:0.7932142\t v_loss_avg:0.5641250\n",
      "04/06 06:51:00 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:51:00 PM |\t  w_train_loss:0.7932141821365803,v_train_loss:0.5641250014305115\n",
      "04/06 06:51:05 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:51:05 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die führenden republikanischen Politiker haben ihre Politik durch die Notwendigkeit der Bekämpfung des Wahlbetrugs gerechtfertigt.']\n",
      "04/06 06:51:05 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:51:18 PM |\t  computing score...\n",
      "04/06 06:51:18 PM |\t  model_w_in_main sacreBLEU : 21.839279\n",
      "04/06 06:51:18 PM |\t  model_w_in_main BLEU : 0.181970\n",
      "04/06 06:51:18 PM |\t  model_w_in_main test loss : 1.248582\n",
      "04/06 06:51:26 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:51:26 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Politiker haben ihre Politik durch die Notwendigkeit der Bekämpfung des Wahlbetrugs gerechtfertigt.']\n",
      "04/06 06:51:26 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:51:40 PM |\t  computing score...\n",
      "04/06 06:51:40 PM |\t  model_v_in_main sacreBLEU : 22.135838\n",
      "04/06 06:51:40 PM |\t  model_v_in_main BLEU : 0.187923\n",
      "04/06 06:51:40 PM |\t  model_v_in_main test loss : 1.241183\n",
      "04/06 06:51:40 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:51:44 PM |\t    0.0% \t w_loss_avg:0.5108463\t v_loss_avg:0.1039032\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:51:47 PM |\t   4.08% \t w_loss_avg:0.4893541\t v_loss_avg:0.2196303\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:51:49 PM |\t   8.16% \t w_loss_avg:0.5359560\t v_loss_avg:0.2456752\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:51:53 PM |\t   12.2% \t w_loss_avg:0.5039411\t v_loss_avg:0.2302957\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:51:56 PM |\t   16.3% \t w_loss_avg:0.4973168\t v_loss_avg:0.2402438\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:51:59 PM |\t   20.4% \t w_loss_avg:0.5043817\t v_loss_avg:0.2291245\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:52:03 PM |\t   24.5% \t w_loss_avg:0.5041888\t v_loss_avg:0.2364565\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:52:07 PM |\t   28.6% \t w_loss_avg:0.4923853\t v_loss_avg:0.2751846\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:52:09 PM |\t   32.7% \t w_loss_avg:0.4878052\t v_loss_avg:0.2770052\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:52:12 PM |\t   36.7% \t w_loss_avg:0.5082727\t v_loss_avg:0.2927948\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:52:15 PM |\t   40.8% \t w_loss_avg:0.5202133\t v_loss_avg:0.2872020\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:52:18 PM |\t   44.9% \t w_loss_avg:0.5217155\t v_loss_avg:0.2925617\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:52:21 PM |\t   49.0% \t w_loss_avg:0.5286801\t v_loss_avg:0.3069810\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:52:24 PM |\t   53.1% \t w_loss_avg:0.5536230\t v_loss_avg:0.3169180\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:52:28 PM |\t   57.1% \t w_loss_avg:0.5700651\t v_loss_avg:0.3402799\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:52:31 PM |\t   61.2% \t w_loss_avg:0.5855228\t v_loss_avg:0.3467790\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:52:34 PM |\t   65.3% \t w_loss_avg:0.5923403\t v_loss_avg:0.3420985\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:52:36 PM |\t   69.4% \t w_loss_avg:0.5936205\t v_loss_avg:0.3499908\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:52:39 PM |\t   73.5% \t w_loss_avg:0.5763658\t v_loss_avg:0.3469594\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:52:43 PM |\t   77.6% \t w_loss_avg:0.5701187\t v_loss_avg:0.3424386\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:52:46 PM |\t   81.6% \t w_loss_avg:0.5758594\t v_loss_avg:0.3460566\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:52:49 PM |\t   85.7% \t w_loss_avg:0.5741637\t v_loss_avg:0.3382501\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:52:52 PM |\t   89.8% \t w_loss_avg:0.5634038\t v_loss_avg:0.3324342\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:52:54 PM |\t   93.9% \t w_loss_avg:0.5593071\t v_loss_avg:0.3251777\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:52:58 PM |\t   98.0% \t w_loss_avg:0.5660153\t v_loss_avg:0.3222074\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:52:59 PM |\t  1e+02% \t w_loss_avg:0.5687210\t v_loss_avg:0.3230392\n",
      "04/06 06:52:59 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:52:59 PM |\t  w_train_loss:0.5687210208270699,v_train_loss:0.3230392402037978\n",
      "04/06 06:53:05 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:53:05 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie gegen die Wiederwahl von Obama', 'Die republikanischen Staats- und Regierungschefs rechtfertigten ihre Politik aufgrund der Notwendigkeit, Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:53:05 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:53:17 PM |\t  computing score...\n",
      "04/06 06:53:17 PM |\t  model_w_in_main sacreBLEU : 19.159233\n",
      "04/06 06:53:17 PM |\t  model_w_in_main BLEU : 0.155662\n",
      "04/06 06:53:17 PM |\t  model_w_in_main test loss : 1.256237\n",
      "04/06 06:53:22 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:53:22 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Politiker rechtfertigten ihre Politik durch die Notwendigkeit, Wahlbetrug abzubauen.']\n",
      "04/06 06:53:22 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:53:34 PM |\t  computing score...\n",
      "04/06 06:53:34 PM |\t  model_v_in_main sacreBLEU : 19.297534\n",
      "04/06 06:53:34 PM |\t  model_v_in_main BLEU : 0.155268\n",
      "04/06 06:53:34 PM |\t  model_v_in_main test loss : 1.352430\n",
      "04/06 06:53:34 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:53:38 PM |\t    0.0% \t w_loss_avg:0.4144043\t v_loss_avg:0.0857000\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:53:41 PM |\t   4.08% \t w_loss_avg:0.4032874\t v_loss_avg:0.1545212\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:53:43 PM |\t   8.16% \t w_loss_avg:0.4500414\t v_loss_avg:0.1952257\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:53:48 PM |\t   12.2% \t w_loss_avg:0.4302181\t v_loss_avg:0.1974542\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:53:50 PM |\t   16.3% \t w_loss_avg:0.4041203\t v_loss_avg:0.2027510\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:53:54 PM |\t   20.4% \t w_loss_avg:0.4056202\t v_loss_avg:0.1810829\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:53:58 PM |\t   24.5% \t w_loss_avg:0.4042350\t v_loss_avg:0.1863073\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:54:00 PM |\t   28.6% \t w_loss_avg:0.4040753\t v_loss_avg:0.2081269\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:54:03 PM |\t   32.7% \t w_loss_avg:0.4052996\t v_loss_avg:0.2086798\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:54:06 PM |\t   36.7% \t w_loss_avg:0.4260414\t v_loss_avg:0.2176052\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:54:09 PM |\t   40.8% \t w_loss_avg:0.4339939\t v_loss_avg:0.2169624\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:54:11 PM |\t   44.9% \t w_loss_avg:0.4363241\t v_loss_avg:0.2223585\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:54:14 PM |\t   49.0% \t w_loss_avg:0.4445135\t v_loss_avg:0.2371944\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:54:16 PM |\t   53.1% \t w_loss_avg:0.4643025\t v_loss_avg:0.2433254\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:54:20 PM |\t   57.1% \t w_loss_avg:0.4791969\t v_loss_avg:0.2663078\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:54:22 PM |\t   61.2% \t w_loss_avg:0.4953485\t v_loss_avg:0.2685668\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:54:25 PM |\t   65.3% \t w_loss_avg:0.5014869\t v_loss_avg:0.2638866\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:54:28 PM |\t   69.4% \t w_loss_avg:0.5009564\t v_loss_avg:0.2730499\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:54:31 PM |\t   73.5% \t w_loss_avg:0.4837903\t v_loss_avg:0.2693156\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:54:34 PM |\t   77.6% \t w_loss_avg:0.4791379\t v_loss_avg:0.2653651\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:54:36 PM |\t   81.6% \t w_loss_avg:0.4834033\t v_loss_avg:0.2670140\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:54:39 PM |\t   85.7% \t w_loss_avg:0.4849573\t v_loss_avg:0.2618203\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:54:41 PM |\t   89.8% \t w_loss_avg:0.4755605\t v_loss_avg:0.2573126\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:54:43 PM |\t   93.9% \t w_loss_avg:0.4707994\t v_loss_avg:0.2514339\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:54:46 PM |\t   98.0% \t w_loss_avg:0.4767093\t v_loss_avg:0.2489639\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:54:47 PM |\t  1e+02% \t w_loss_avg:0.4789034\t v_loss_avg:0.2509492\n",
      "04/06 06:54:48 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:54:48 PM |\t  w_train_loss:0.47890336718410254,v_train_loss:0.25094920478295535\n",
      "04/06 06:54:54 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:54:54 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Staats- und Regierungschefs haben ihre Politik durch die Notwendigkeit der Bekämpfung des Wahlbetrugs gerechtfertigt.']\n",
      "04/06 06:54:54 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:55:07 PM |\t  computing score...\n",
      "04/06 06:55:07 PM |\t  model_w_in_main sacreBLEU : 20.044916\n",
      "04/06 06:55:07 PM |\t  model_w_in_main BLEU : 0.170347\n",
      "04/06 06:55:07 PM |\t  model_w_in_main test loss : 1.314256\n",
      "04/06 06:55:12 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:55:12 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Politiker rechtfertigten ihre Politik durch die Notwendigkeit, den Wahlbetrug abzubauen.']\n",
      "04/06 06:55:12 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:55:25 PM |\t  computing score...\n",
      "04/06 06:55:25 PM |\t  model_v_in_main sacreBLEU : 21.042529\n",
      "04/06 06:55:25 PM |\t  model_v_in_main BLEU : 0.178086\n",
      "04/06 06:55:25 PM |\t  model_v_in_main test loss : 1.352177\n",
      "04/06 06:55:25 PM |\t  \n",
      "\n",
      "  ----------------epoch:8,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:55:29 PM |\t    0.0% \t w_loss_avg:0.3120606\t v_loss_avg:0.0981274\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:55:32 PM |\t   4.08% \t w_loss_avg:0.3047136\t v_loss_avg:0.1690610\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:55:34 PM |\t   8.16% \t w_loss_avg:0.3642428\t v_loss_avg:0.2069384\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:55:37 PM |\t   12.2% \t w_loss_avg:0.3523209\t v_loss_avg:0.1762340\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:55:40 PM |\t   16.3% \t w_loss_avg:0.3348917\t v_loss_avg:0.1774694\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:55:43 PM |\t   20.4% \t w_loss_avg:0.3448013\t v_loss_avg:0.1613864\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:55:46 PM |\t   24.5% \t w_loss_avg:0.3376867\t v_loss_avg:0.1680438\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:55:50 PM |\t   28.6% \t w_loss_avg:0.3358300\t v_loss_avg:0.1827588\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:55:52 PM |\t   32.7% \t w_loss_avg:0.3359177\t v_loss_avg:0.1824081\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:55:55 PM |\t   36.7% \t w_loss_avg:0.3554124\t v_loss_avg:0.1875853\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:55:58 PM |\t   40.8% \t w_loss_avg:0.3627130\t v_loss_avg:0.1832377\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:56:01 PM |\t   44.9% \t w_loss_avg:0.3636632\t v_loss_avg:0.1841426\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:56:04 PM |\t   49.0% \t w_loss_avg:0.3724959\t v_loss_avg:0.1978050\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:56:06 PM |\t   53.1% \t w_loss_avg:0.3934516\t v_loss_avg:0.2030457\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:56:10 PM |\t   57.1% \t w_loss_avg:0.4097519\t v_loss_avg:0.2206211\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:56:13 PM |\t   61.2% \t w_loss_avg:0.4218955\t v_loss_avg:0.2233522\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:56:15 PM |\t   65.3% \t w_loss_avg:0.4285302\t v_loss_avg:0.2195004\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:56:18 PM |\t   69.4% \t w_loss_avg:0.4279734\t v_loss_avg:0.2224615\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:56:21 PM |\t   73.5% \t w_loss_avg:0.4143904\t v_loss_avg:0.2196788\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:56:24 PM |\t   77.6% \t w_loss_avg:0.4082359\t v_loss_avg:0.2167833\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:56:26 PM |\t   81.6% \t w_loss_avg:0.4138213\t v_loss_avg:0.2191486\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:56:29 PM |\t   85.7% \t w_loss_avg:0.4122429\t v_loss_avg:0.2176925\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:56:31 PM |\t   89.8% \t w_loss_avg:0.4056849\t v_loss_avg:0.2142149\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:56:34 PM |\t   93.9% \t w_loss_avg:0.4027653\t v_loss_avg:0.2096088\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:56:36 PM |\t   98.0% \t w_loss_avg:0.4089187\t v_loss_avg:0.2074876\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:56:38 PM |\t  1e+02% \t w_loss_avg:0.4123711\t v_loss_avg:0.2076552\n",
      "04/06 06:56:38 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n",
      "04/06 06:56:38 PM |\t  w_train_loss:0.4123711002757773,v_train_loss:0.20765516895335168\n",
      "04/06 06:56:44 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:56:44 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie gegen die Wiederwahl von Obama', 'Die republikanischen Staats- und Regierungschefs rechtfertigten ihre Politik dadurch, ob Wahlbetrug bekämpft werden muß.']\n",
      "04/06 06:56:44 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:56:56 PM |\t  computing score...\n",
      "04/06 06:56:56 PM |\t  model_w_in_main sacreBLEU : 20.238693\n",
      "04/06 06:56:56 PM |\t  model_w_in_main BLEU : 0.165387\n",
      "04/06 06:56:56 PM |\t  model_w_in_main test loss : 1.371860\n",
      "04/06 06:57:02 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 06:57:02 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Politiker haben ihre Politik durch die Bekämpfung von Wahlbetrug begründet.']\n",
      "04/06 06:57:02 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 06:57:14 PM |\t  computing score...\n",
      "04/06 06:57:14 PM |\t  model_v_in_main sacreBLEU : 20.254333\n",
      "04/06 06:57:14 PM |\t  model_v_in_main BLEU : 0.165302\n",
      "04/06 06:57:14 PM |\t  model_v_in_main test loss : 1.448793\n",
      "04/06 06:57:14 PM |\t  \n",
      "\n",
      "  ----------------epoch:9,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "5 1463956992\n",
      "step 0\n",
      "fre 2\n",
      "04/06 06:57:17 PM |\t    0.0% \t w_loss_avg:0.2652688\t v_loss_avg:0.0667763\n",
      "5 1463956992\n",
      "step 1\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 2\n",
      "fre 2\n",
      "04/06 06:57:20 PM |\t   4.08% \t w_loss_avg:0.2953903\t v_loss_avg:0.1333385\n",
      "5 1463956992\n",
      "step 3\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 4\n",
      "fre 2\n",
      "04/06 06:57:22 PM |\t   8.16% \t w_loss_avg:0.3541767\t v_loss_avg:0.1474815\n",
      "5 1463956992\n",
      "step 5\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 6\n",
      "fre 2\n",
      "04/06 06:57:26 PM |\t   12.2% \t w_loss_avg:0.3280083\t v_loss_avg:0.1383195\n",
      "5 1463956992\n",
      "step 7\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 8\n",
      "fre 2\n",
      "04/06 06:57:29 PM |\t   16.3% \t w_loss_avg:0.3174438\t v_loss_avg:0.1568780\n",
      "5 1463956992\n",
      "step 9\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 10\n",
      "fre 2\n",
      "04/06 06:57:33 PM |\t   20.4% \t w_loss_avg:0.3130263\t v_loss_avg:0.1505869\n",
      "5 1463956992\n",
      "step 11\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 12\n",
      "fre 2\n",
      "04/06 06:57:37 PM |\t   24.5% \t w_loss_avg:0.3128360\t v_loss_avg:0.1581326\n",
      "5 1463956992\n",
      "step 13\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 14\n",
      "fre 2\n",
      "04/06 06:57:40 PM |\t   28.6% \t w_loss_avg:0.3066116\t v_loss_avg:0.1755674\n",
      "5 1463956992\n",
      "step 15\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 16\n",
      "fre 2\n",
      "04/06 06:57:42 PM |\t   32.7% \t w_loss_avg:0.3065294\t v_loss_avg:0.1744692\n",
      "5 1463956992\n",
      "step 17\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 18\n",
      "fre 2\n",
      "04/06 06:57:45 PM |\t   36.7% \t w_loss_avg:0.3207347\t v_loss_avg:0.1760362\n",
      "5 1463956992\n",
      "step 19\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 20\n",
      "fre 2\n",
      "04/06 06:57:48 PM |\t   40.8% \t w_loss_avg:0.3287909\t v_loss_avg:0.1772720\n",
      "5 1463956992\n",
      "step 21\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 22\n",
      "fre 2\n",
      "04/06 06:57:51 PM |\t   44.9% \t w_loss_avg:0.3263093\t v_loss_avg:0.1794560\n",
      "5 1463956992\n",
      "step 23\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 24\n",
      "fre 2\n",
      "04/06 06:57:53 PM |\t   49.0% \t w_loss_avg:0.3345351\t v_loss_avg:0.1929819\n",
      "5 1463956992\n",
      "step 25\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 26\n",
      "fre 2\n",
      "04/06 06:57:56 PM |\t   53.1% \t w_loss_avg:0.3517704\t v_loss_avg:0.1974080\n",
      "5 1463956992\n",
      "step 27\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 28\n",
      "fre 2\n",
      "04/06 06:58:03 PM |\t   57.1% \t w_loss_avg:0.3688280\t v_loss_avg:0.2129753\n",
      "5 1463956992\n",
      "step 29\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 30\n",
      "fre 2\n",
      "04/06 06:58:07 PM |\t   61.2% \t w_loss_avg:0.3807699\t v_loss_avg:0.2139537\n",
      "5 1463956992\n",
      "step 31\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 32\n",
      "fre 2\n",
      "04/06 06:58:10 PM |\t   65.3% \t w_loss_avg:0.3892010\t v_loss_avg:0.2080203\n",
      "5 1463956992\n",
      "step 33\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 34\n",
      "fre 2\n",
      "04/06 06:58:13 PM |\t   69.4% \t w_loss_avg:0.3849088\t v_loss_avg:0.2090258\n",
      "5 1463956992\n",
      "step 35\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 36\n",
      "fre 2\n",
      "04/06 06:58:16 PM |\t   73.5% \t w_loss_avg:0.3729284\t v_loss_avg:0.2072238\n",
      "5 1463956992\n",
      "step 37\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 38\n",
      "fre 2\n",
      "04/06 06:58:18 PM |\t   77.6% \t w_loss_avg:0.3697925\t v_loss_avg:0.2056232\n",
      "5 1463956992\n",
      "step 39\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 40\n",
      "fre 2\n",
      "04/06 06:58:21 PM |\t   81.6% \t w_loss_avg:0.3719481\t v_loss_avg:0.2083830\n",
      "5 1463956992\n",
      "step 41\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 42\n",
      "fre 2\n",
      "04/06 06:58:24 PM |\t   85.7% \t w_loss_avg:0.3718189\t v_loss_avg:0.2053274\n",
      "5 1463956992\n",
      "step 43\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 44\n",
      "fre 2\n",
      "04/06 06:58:26 PM |\t   89.8% \t w_loss_avg:0.3657024\t v_loss_avg:0.2009384\n",
      "5 1463956992\n",
      "step 45\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 46\n",
      "fre 2\n",
      "04/06 06:58:29 PM |\t   93.9% \t w_loss_avg:0.3592642\t v_loss_avg:0.1991627\n",
      "5 1463956992\n",
      "step 47\n",
      "fre 2\n",
      "5 1463956992\n",
      "step 48\n",
      "fre 2\n",
      "04/06 06:58:32 PM |\t   98.0% \t w_loss_avg:0.3639681\t v_loss_avg:0.1990169\n",
      "5 1463956992\n",
      "step 49\n",
      "fre 2\n",
      "04/06 06:58:33 PM |\t  1e+02% \t w_loss_avg:0.3653250\t v_loss_avg:0.2003189\n",
      "04/06 06:58:34 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0025, 0.0025, 0.0025], device='cuda:0', requires_grad=True))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_117964/2317287350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mscheduler_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         return [group['lr'] * self.gamma\n\u001b[0m\u001b[0;32m    375\u001b[0m                 for group in self.optimizer.param_groups]\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         return [group['lr'] * self.gamma\n\u001b[0m\u001b[0;32m    375\u001b[0m                 for group in self.optimizer.param_groups]\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,model_w,-1) #before train\n",
    "    # my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "    \n",
    "    my_test(valid_dataloader,model_w,epoch) \n",
    "    my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
