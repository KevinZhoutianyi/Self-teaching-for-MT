{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=32,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=8,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=8,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=8,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=8,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--model_name', type=str,                   default='t5-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='smooth64',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default='25',      help='howmany step report once')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=64,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-4,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.7,    help='momentum')\n",
    "parser.add_argument('--smoothing', type=float,                   default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.9,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=0,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5\\wandb\\run-20220406_135353-2y3ce1mt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/CUDAOOM/runs/2y3ce1mt\" target=\"_blank\">smooth64</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/CUDAOOM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/CUDAOOM/runs/2y3ce1mt?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22907d9a0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "# os.environ['WANDB_NOTES']='train without A,withoutAandt5smallandbatch64 '\n",
    "wandb.init(project=\"CUDAOOM\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 01:54:01 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 01:54:01 PM |\t  Namespace(A_lr=0.0001, batch_size=32, decay=0.001, epochs=50, exp_name='smooth64', gpu=0, grad_acc_count=1, grad_clip=1, learning_rate_min=1e-08, model_name='t5-small', momentum=0.7, pre_epochs=0, rep_num=25, smoothing=0.1, syndata_loss_ratio=0.1, train_A=0, train_A_num_points=8, train_num_points=1000, train_v_num_points=8, train_v_synthetic_num_points=8, train_w_num_points=8, traindata_loss_ratio=0.9, v_lr=0.001, valid_begin=0, valid_num_points=100, w_lr=0.001)\n",
      "04/06 01:54:01 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "04/06 01:54:01 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 01:54:02 PM |\t  modelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,modelname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 01:54:31 PM |\t  train len: 992\n",
      "04/06 01:54:31 PM |\t  train_w_num_points_len: 248\n",
      "04/06 01:54:31 PM |\t  train_v_synthetic_num_points_len: 248\n",
      "04/06 01:54:31 PM |\t  train_v_num_points_len: 248\n",
      "04/06 01:54:31 PM |\t  train_A_num_points_len: 248\n",
      "04/06 01:54:31 PM |\t  valid len: 100\n",
      "04/06 01:54:31 PM |\t  test len: 3003\n",
      "04/06 01:54:31 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "04/06 01:54:31 PM |\t  {'de': 'Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.', 'en': 'translate English to German: However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.'}\n",
      "04/06 01:54:31 PM |\t  {'de': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'en': 'translate English to German: Two sets of lights so close to one another: intentional or just a silly error?'}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelname)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none',label_smoothing=0.1)#,ignore_index = tokenizer.pad_token_id)#\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][:args.valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en'] \n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "04/06 01:54:31 PM |\t  train data get\n",
      "04/06 01:54:31 PM |\t  train data loader get\n",
      "04/06 01:54:31 PM |\t  valid data loader get\n",
      "04/06 01:54:32 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler=SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=4)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=16, pin_memory=True, num_workers=4)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=16, pin_memory=True, num_workers=4)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = Adafactor(model_w.parameters(), scale_parameter=True, relative_step=True, warmup_init=False,clip_threshold=1,beta1=0)\n",
    "scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer =Adafactor(model_v.parameters(), scale_parameter=True, relative_step=True, warmup_init=False, clip_threshold=1,beta1=0)\n",
    "scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    metric_bleu =  load_metric('bleu')\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        with torch.no_grad():\n",
    "            ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "            acc+= ls\n",
    "            counter+= 1\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "            pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "            label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "            \n",
    "            pred_str = [x.replace('.', '')  for x in pred_decoded]\n",
    "            label_str = [[x.replace('.', '')] for x in label_decoded]\n",
    "            pred_list = [x.replace('.', '').split()  for x in pred_decoded]\n",
    "            label_list = [[x.replace('.', '').split()] for x in label_decoded]\n",
    "            if  step%100==0:\n",
    "                logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "            metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "            \n",
    "    logging.info('computing score...')            \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    writer.add_scalar(model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/sacreBLEU\",sacrebleu_score['score'], global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/BLEU\",bleu_score['bleu'], global_step=epoch)\n",
    "    \n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    \n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    wsize = args.train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points \n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points \n",
    "    grad_acc_count = args.grad_acc_count\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size=[wsize,synsize,vsize,Asize]\n",
    "    for step, batch in enumerate(_dataloader) :\n",
    "        print('1',torch.cuda.memory_allocated())\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=True) \n",
    "        (input_w,input_syn,input_v,input_A_v) = torch.split(train_x,split_size)\n",
    "        (input_w_attn,input_syn_attn,input_v_attn,input_A_v_attn) = torch.split(train_x_attn,split_size)\n",
    "        (output_w,_,output_v,output_A_v) = torch.split(train_y,split_size)\n",
    "        (output_w_attn,_,output_v_attn,output_A_v_attn) = torch.split(train_y_attn,split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "       \n",
    "\n",
    "        print('2',torch.cuda.memory_allocated())\n",
    "        if (epoch <= args.epochs) and (args.train_A == 1) and epoch >= args.pre_epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "        \n",
    "        \n",
    "        print('3',torch.cuda.memory_allocated())\n",
    "        if  epoch <= args.epochs:\n",
    "            for p in w_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "            loss_w.backward()\n",
    "            objs_w.update(loss_w.item(), wsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "                w_optimizer.step()\n",
    "                w_optimizer.zero_grad()\n",
    "            for p in w_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        print('4',torch.cuda.memory_allocated())\n",
    "        if epoch >= args.pre_epochs and epoch <= args.epochs:\n",
    "            \n",
    "            for p in v_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            # loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)#,input_v,input_v_attn,output_v,output_v_attn)\n",
    "            loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            v_loss =  (args.traindata_loss_ratio*loss)/num_batch\n",
    "            v_trainloss_acc+=v_loss.item()\n",
    "            v_loss.backward()\n",
    "            objs_v.update(v_loss.item(), vtrainsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "                v_optimizer.step()  \n",
    "                v_optimizer.zero_grad() \n",
    "            for p in v_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        print('5',torch.cuda.memory_allocated())\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        fre = (loader_len//args.rep_num)\n",
    "        print('step',step)\n",
    "        print('fre',fre)\n",
    "        if((step)%fre == 0 or (step)==(loader_len-1)):\n",
    "            logging.info(f\"{progress:5.3}% \\t w_loss_avg:{objs_w.avg*train_w_num_points_len:^.7f}\\t v_loss_avg:{objs_v.avg*vtrainsize_total:^.7f}\")\n",
    "  \n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    \n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 01:54:33 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 484056064\n",
      "2 484240384\n",
      "3 484240384\n",
      "4 972657664\n",
      "5 1461108736\n",
      "step 0\n",
      "fre 1\n",
      "04/06 01:54:37 PM |\t    0.0% \t w_loss_avg:28.1351709\t v_loss_avg:10.0156122\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 1\n",
      "fre 1\n",
      "04/06 01:54:37 PM |\t   3.33% \t w_loss_avg:19.0351214\t v_loss_avg:10.3483512\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 2\n",
      "fre 1\n",
      "04/06 01:54:38 PM |\t   6.67% \t w_loss_avg:20.9190703\t v_loss_avg:9.7430343\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 3\n",
      "fre 1\n",
      "04/06 01:54:38 PM |\t   10.0% \t w_loss_avg:20.2132868\t v_loss_avg:10.3745155\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 4\n",
      "fre 1\n",
      "04/06 01:54:39 PM |\t   13.3% \t w_loss_avg:19.5901624\t v_loss_avg:11.0952852\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 5\n",
      "fre 1\n",
      "04/06 01:54:39 PM |\t   16.7% \t w_loss_avg:18.0723523\t v_loss_avg:10.6096772\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 6\n",
      "fre 1\n",
      "04/06 01:54:39 PM |\t   20.0% \t w_loss_avg:17.0070286\t v_loss_avg:10.4686067\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 7\n",
      "fre 1\n",
      "04/06 01:54:40 PM |\t   23.3% \t w_loss_avg:16.4058884\t v_loss_avg:10.1934724\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 8\n",
      "fre 1\n",
      "04/06 01:54:40 PM |\t   26.7% \t w_loss_avg:15.4513052\t v_loss_avg:10.0154663\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 9\n",
      "fre 1\n",
      "04/06 01:54:41 PM |\t   30.0% \t w_loss_avg:14.9062393\t v_loss_avg:9.6625457\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 10\n",
      "fre 1\n",
      "04/06 01:54:41 PM |\t   33.3% \t w_loss_avg:14.5494252\t v_loss_avg:9.5174228\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 11\n",
      "fre 1\n",
      "04/06 01:54:41 PM |\t   36.7% \t w_loss_avg:13.8228356\t v_loss_avg:9.2019068\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 12\n",
      "fre 1\n",
      "04/06 01:54:42 PM |\t   40.0% \t w_loss_avg:13.3711289\t v_loss_avg:9.1067818\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 13\n",
      "fre 1\n",
      "04/06 01:54:42 PM |\t   43.3% \t w_loss_avg:13.0031565\t v_loss_avg:8.9863644\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 14\n",
      "fre 1\n",
      "04/06 01:54:43 PM |\t   46.7% \t w_loss_avg:12.7353848\t v_loss_avg:8.9204086\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 15\n",
      "fre 1\n",
      "04/06 01:54:43 PM |\t   50.0% \t w_loss_avg:12.7034163\t v_loss_avg:8.8738997\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 16\n",
      "fre 1\n",
      "04/06 01:54:43 PM |\t   53.3% \t w_loss_avg:12.3960322\t v_loss_avg:8.7324761\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 17\n",
      "fre 1\n",
      "04/06 01:54:44 PM |\t   56.7% \t w_loss_avg:12.0274635\t v_loss_avg:8.5961364\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 18\n",
      "fre 1\n",
      "04/06 01:54:44 PM |\t   60.0% \t w_loss_avg:11.8613683\t v_loss_avg:8.5141078\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 19\n",
      "fre 1\n",
      "04/06 01:54:45 PM |\t   63.3% \t w_loss_avg:11.8008889\t v_loss_avg:8.4120600\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 20\n",
      "fre 1\n",
      "04/06 01:54:45 PM |\t   66.7% \t w_loss_avg:11.5679510\t v_loss_avg:8.4267536\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 21\n",
      "fre 1\n",
      "04/06 01:54:46 PM |\t   70.0% \t w_loss_avg:11.4952132\t v_loss_avg:8.5531827\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 22\n",
      "fre 1\n",
      "04/06 01:54:46 PM |\t   73.3% \t w_loss_avg:11.4414105\t v_loss_avg:8.9371073\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 23\n",
      "fre 1\n",
      "04/06 01:54:46 PM |\t   76.7% \t w_loss_avg:11.6471164\t v_loss_avg:9.2641569\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 24\n",
      "fre 1\n",
      "04/06 01:54:47 PM |\t   80.0% \t w_loss_avg:11.5548612\t v_loss_avg:9.2918465\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 25\n",
      "fre 1\n",
      "04/06 01:54:47 PM |\t   83.3% \t w_loss_avg:11.3892941\t v_loss_avg:9.1560514\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 26\n",
      "fre 1\n",
      "04/06 01:54:48 PM |\t   86.7% \t w_loss_avg:11.3213881\t v_loss_avg:9.0707571\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 27\n",
      "fre 1\n",
      "04/06 01:54:48 PM |\t   90.0% \t w_loss_avg:11.2486719\t v_loss_avg:9.0944909\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 28\n",
      "fre 1\n",
      "04/06 01:54:48 PM |\t   93.3% \t w_loss_avg:11.2509550\t v_loss_avg:9.0963562\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 29\n",
      "fre 1\n",
      "04/06 01:54:49 PM |\t   96.7% \t w_loss_avg:11.1653174\t v_loss_avg:9.0260552\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 30\n",
      "fre 1\n",
      "04/06 01:54:49 PM |\t  1e+02% \t w_loss_avg:11.0626943\t v_loss_avg:9.0496615\n",
      "04/06 01:54:49 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "04/06 01:54:49 PM |\t  w_train_loss:11.06269434094429,v_train_loss:9.049661487340927\n",
      "04/06 01:54:55 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 01:54:55 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Führer haben ihre Politik aufgrund der Bekämpfung des Wahlbetrugs begründet.']\n",
      "04/06 01:54:55 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 01:55:10 PM |\t  computing score...\n",
      "04/06 01:55:10 PM |\t  model_w_in_main sacreBLEU : 13.045596\n",
      "04/06 01:55:10 PM |\t  model_w_in_main BLEU : 0.104308\n",
      "04/06 01:55:10 PM |\t  model_w_in_main test loss : 6.231764\n",
      "04/06 01:55:16 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 01:55:16 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie gegen die Wiederelection von Obama', 'Die republikanischen Führer haben ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug begründen müssen.']\n",
      "04/06 01:55:16 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 01:55:32 PM |\t  computing score...\n",
      "04/06 01:55:32 PM |\t  model_v_in_main sacreBLEU : 14.985075\n",
      "04/06 01:55:32 PM |\t  model_v_in_main BLEU : 0.123380\n",
      "04/06 01:55:32 PM |\t  model_v_in_main test loss : 6.257380\n",
      "04/06 01:55:32 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 1460922880\n",
      "2 1461107200\n",
      "3 1461107200\n",
      "4 1461107712\n",
      "5 1461108736\n",
      "step 0\n",
      "fre 1\n",
      "04/06 01:55:34 PM |\t    0.0% \t w_loss_avg:13.6579434\t v_loss_avg:6.0700436\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 1\n",
      "fre 1\n",
      "04/06 01:55:35 PM |\t   3.33% \t w_loss_avg:11.0403040\t v_loss_avg:7.5166814\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 2\n",
      "fre 1\n",
      "04/06 01:55:35 PM |\t   6.67% \t w_loss_avg:14.7250971\t v_loss_avg:7.2398059\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 3\n",
      "fre 1\n",
      "04/06 01:55:36 PM |\t   10.0% \t w_loss_avg:15.2351003\t v_loss_avg:8.0764190\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 4\n",
      "fre 1\n",
      "04/06 01:55:36 PM |\t   13.3% \t w_loss_avg:15.3178644\t v_loss_avg:9.0041825\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 5\n",
      "fre 1\n",
      "04/06 01:55:36 PM |\t   16.7% \t w_loss_avg:14.3657625\t v_loss_avg:8.7186207\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 6\n",
      "fre 1\n",
      "04/06 01:55:37 PM |\t   20.0% \t w_loss_avg:13.7530330\t v_loss_avg:8.7538289\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 7\n",
      "fre 1\n",
      "04/06 01:55:37 PM |\t   23.3% \t w_loss_avg:13.4926984\t v_loss_avg:8.6270465\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 8\n",
      "fre 1\n",
      "04/06 01:55:38 PM |\t   26.7% \t w_loss_avg:12.8369103\t v_loss_avg:8.5684521\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 9\n",
      "fre 1\n",
      "04/06 01:55:38 PM |\t   30.0% \t w_loss_avg:12.4914861\t v_loss_avg:8.3103015\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 10\n",
      "fre 1\n",
      "04/06 01:55:39 PM |\t   33.3% \t w_loss_avg:12.3342800\t v_loss_avg:8.2463920\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 11\n",
      "fre 1\n",
      "04/06 01:55:39 PM |\t   36.7% \t w_loss_avg:11.7667667\t v_loss_avg:8.0152271\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 12\n",
      "fre 1\n",
      "04/06 01:55:40 PM |\t   40.0% \t w_loss_avg:11.4323468\t v_loss_avg:7.9725284\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 13\n",
      "fre 1\n",
      "04/06 01:55:40 PM |\t   43.3% \t w_loss_avg:11.1808100\t v_loss_avg:7.9146887\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 14\n",
      "fre 1\n",
      "04/06 01:55:40 PM |\t   46.7% \t w_loss_avg:10.9910214\t v_loss_avg:7.8935726\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 15\n",
      "fre 1\n",
      "04/06 01:55:41 PM |\t   50.0% \t w_loss_avg:11.0290773\t v_loss_avg:7.8767278\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 16\n",
      "fre 1\n",
      "04/06 01:55:41 PM |\t   53.3% \t w_loss_avg:10.7839029\t v_loss_avg:7.7703113\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 17\n",
      "fre 1\n",
      "04/06 01:55:42 PM |\t   56.7% \t w_loss_avg:10.4808683\t v_loss_avg:7.6586934\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 18\n",
      "fre 1\n",
      "04/06 01:55:42 PM |\t   60.0% \t w_loss_avg:10.3631232\t v_loss_avg:7.6023624\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 19\n",
      "fre 1\n",
      "04/06 01:55:43 PM |\t   63.3% \t w_loss_avg:10.3688316\t v_loss_avg:7.5323993\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 20\n",
      "fre 1\n",
      "04/06 01:55:43 PM |\t   66.7% \t w_loss_avg:10.1883795\t v_loss_avg:7.5597135\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 21\n",
      "fre 1\n",
      "04/06 01:55:44 PM |\t   70.0% \t w_loss_avg:10.1601540\t v_loss_avg:7.6959826\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 22\n",
      "fre 1\n",
      "04/06 01:55:44 PM |\t   73.3% \t w_loss_avg:10.1360467\t v_loss_avg:8.0920900\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 23\n",
      "fre 1\n",
      "04/06 01:55:44 PM |\t   76.7% \t w_loss_avg:10.3724138\t v_loss_avg:8.4480427\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 24\n",
      "fre 1\n",
      "04/06 01:55:45 PM |\t   80.0% \t w_loss_avg:10.3148934\t v_loss_avg:8.4861513\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 25\n",
      "fre 1\n",
      "04/06 01:55:45 PM |\t   83.3% \t w_loss_avg:10.1850006\t v_loss_avg:8.3693434\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 26\n",
      "fre 1\n",
      "04/06 01:55:46 PM |\t   86.7% \t w_loss_avg:10.1364644\t v_loss_avg:8.2971434\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 27\n",
      "fre 1\n",
      "04/06 01:55:46 PM |\t   90.0% \t w_loss_avg:10.0877526\t v_loss_avg:8.3281083\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 28\n",
      "fre 1\n",
      "04/06 01:55:47 PM |\t   93.3% \t w_loss_avg:10.1119545\t v_loss_avg:8.3332648\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 29\n",
      "fre 1\n",
      "04/06 01:55:47 PM |\t   96.7% \t w_loss_avg:10.0497429\t v_loss_avg:8.2731465\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 30\n",
      "fre 1\n",
      "04/06 01:55:47 PM |\t  1e+02% \t w_loss_avg:9.9658528\t v_loss_avg:8.2967038\n",
      "04/06 01:55:48 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "04/06 01:55:48 PM |\t  w_train_loss:9.96585276722908,v_train_loss:8.296703800559044\n",
      "04/06 01:55:54 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 01:55:54 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzuwirken.', 'Die republikanischen Fraktion hat ihre Politiken durch den Anspruch der Bekämpfung des Wahlbetrugs gerechtfertigt.']\n",
      "04/06 01:55:54 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 01:56:09 PM |\t  computing score...\n",
      "04/06 01:56:09 PM |\t  model_w_in_main sacreBLEU : 16.339697\n",
      "04/06 01:56:09 PM |\t  model_w_in_main BLEU : 0.123207\n",
      "04/06 01:56:09 PM |\t  model_w_in_main test loss : 6.293993\n",
      "04/06 01:56:15 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 01:56:15 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanische Führer haben ihre Politik begründet, indem sie den Wahlbetrug bekämpfen müssen.']\n",
      "04/06 01:56:15 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 01:56:29 PM |\t  computing score...\n",
      "04/06 01:56:29 PM |\t  model_v_in_main sacreBLEU : 14.071387\n",
      "04/06 01:56:29 PM |\t  model_v_in_main BLEU : 0.108804\n",
      "04/06 01:56:29 PM |\t  model_v_in_main test loss : 6.347928\n",
      "04/06 01:56:30 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 1460922880\n",
      "2 1461107200\n",
      "3 1461107200\n",
      "4 1461107712\n",
      "5 1461108736\n",
      "step 0\n",
      "fre 1\n",
      "04/06 01:56:32 PM |\t    0.0% \t w_loss_avg:12.5416815\t v_loss_avg:5.3188410\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 1\n",
      "fre 1\n",
      "04/06 01:56:32 PM |\t   3.33% \t w_loss_avg:10.1742909\t v_loss_avg:6.9439586\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 2\n",
      "fre 1\n",
      "04/06 01:56:33 PM |\t   6.67% \t w_loss_avg:13.8985597\t v_loss_avg:6.7284058\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 3\n",
      "fre 1\n",
      "04/06 01:56:33 PM |\t   10.0% \t w_loss_avg:14.4661124\t v_loss_avg:7.5861132\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 4\n",
      "fre 1\n",
      "04/06 01:56:33 PM |\t   13.3% \t w_loss_avg:14.6280537\t v_loss_avg:8.5621530\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 5\n",
      "fre 1\n",
      "04/06 01:56:34 PM |\t   16.7% \t w_loss_avg:13.7957489\t v_loss_avg:8.3102790\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 6\n",
      "fre 1\n",
      "04/06 01:56:34 PM |\t   20.0% \t w_loss_avg:13.2295792\t v_loss_avg:8.3540781\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 7\n",
      "fre 1\n",
      "04/06 01:56:35 PM |\t   23.3% \t w_loss_avg:13.0004357\t v_loss_avg:8.2424762\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 8\n",
      "fre 1\n",
      "04/06 01:56:35 PM |\t   26.7% \t w_loss_avg:12.3499808\t v_loss_avg:8.1925636\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 9\n",
      "fre 1\n",
      "04/06 01:56:36 PM |\t   30.0% \t w_loss_avg:12.0084773\t v_loss_avg:7.9516527\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 10\n",
      "fre 1\n",
      "04/06 01:56:36 PM |\t   33.3% \t w_loss_avg:11.8434957\t v_loss_avg:7.8936751\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 11\n",
      "fre 1\n",
      "04/06 01:56:36 PM |\t   36.7% \t w_loss_avg:11.3021034\t v_loss_avg:7.6750876\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 12\n",
      "fre 1\n",
      "04/06 01:56:37 PM |\t   40.0% \t w_loss_avg:10.9877952\t v_loss_avg:7.6437558\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 13\n",
      "fre 1\n",
      "04/06 01:56:37 PM |\t   43.3% \t w_loss_avg:10.7628809\t v_loss_avg:7.5850934\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 14\n",
      "fre 1\n",
      "04/06 01:56:38 PM |\t   46.7% \t w_loss_avg:10.5811819\t v_loss_avg:7.5603417\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 15\n",
      "fre 1\n",
      "04/06 01:56:38 PM |\t   50.0% \t w_loss_avg:10.6230456\t v_loss_avg:7.5386243\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 16\n",
      "fre 1\n",
      "04/06 01:56:39 PM |\t   53.3% \t w_loss_avg:10.3777148\t v_loss_avg:7.4379637\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 17\n",
      "fre 1\n",
      "04/06 01:56:39 PM |\t   56.7% \t w_loss_avg:10.0743059\t v_loss_avg:7.3268878\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 18\n",
      "fre 1\n",
      "04/06 01:56:39 PM |\t   60.0% \t w_loss_avg:9.9541612\t v_loss_avg:7.2694991\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 19\n",
      "fre 1\n",
      "04/06 01:56:40 PM |\t   63.3% \t w_loss_avg:9.9522855\t v_loss_avg:7.1991266\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 20\n",
      "fre 1\n",
      "04/06 01:56:40 PM |\t   66.7% \t w_loss_avg:9.7710618\t v_loss_avg:7.2282518\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 21\n",
      "fre 1\n",
      "04/06 01:56:41 PM |\t   70.0% \t w_loss_avg:9.7473665\t v_loss_avg:7.3501682\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 22\n",
      "fre 1\n",
      "04/06 01:56:41 PM |\t   73.3% \t w_loss_avg:9.7368963\t v_loss_avg:7.7496578\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 23\n",
      "fre 1\n",
      "04/06 01:56:42 PM |\t   76.7% \t w_loss_avg:9.9702807\t v_loss_avg:8.0850875\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 24\n",
      "fre 1\n",
      "04/06 01:56:42 PM |\t   80.0% \t w_loss_avg:9.9136357\t v_loss_avg:8.1251762\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 25\n",
      "fre 1\n",
      "04/06 01:56:42 PM |\t   83.3% \t w_loss_avg:9.7770569\t v_loss_avg:8.0103035\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 26\n",
      "fre 1\n",
      "04/06 01:56:43 PM |\t   86.7% \t w_loss_avg:9.7320257\t v_loss_avg:7.9428193\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 27\n",
      "fre 1\n",
      "04/06 01:56:43 PM |\t   90.0% \t w_loss_avg:9.6836567\t v_loss_avg:7.9793933\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 28\n",
      "fre 1\n",
      "04/06 01:56:44 PM |\t   93.3% \t w_loss_avg:9.7090952\t v_loss_avg:7.9894187\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 29\n",
      "fre 1\n",
      "04/06 01:56:44 PM |\t   96.7% \t w_loss_avg:9.6497121\t v_loss_avg:7.9276584\n",
      "1 1461108736\n",
      "2 1461108736\n",
      "3 1461108736\n",
      "4 1461108736\n",
      "5 1461108736\n",
      "step 30\n",
      "fre 1\n",
      "04/06 01:56:44 PM |\t  1e+02% \t w_loss_avg:9.5689269\t v_loss_avg:7.9446995\n",
      "04/06 01:56:45 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "04/06 01:56:45 PM |\t  w_train_loss:9.568926870822906,v_train_loss:7.9446994960308075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76292/2317287350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mmy_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76292/1187995882.py\u001b[0m in \u001b[0;36mmy_test\u001b[1;34m(_dataloader, model, epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmetric_sacrebleu\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sacrebleu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmetric_bleu\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bleu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_metric\u001b[1;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, script_version, **metric_init_kwargs)\u001b[0m\n\u001b[0;32m   1336\u001b[0m         )\n\u001b[0;32m   1337\u001b[0m         \u001b[0mrevision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m     metric_module = metric_module_factory(\n\u001b[0m\u001b[0;32m   1339\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m     ).module_path\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mmetric_module_factory\u001b[1;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_relative_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforce_local_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1226\u001b[1;33m             return CanonicalMetricModuleFactory(\n\u001b[0m\u001b[0;32m   1227\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mget_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m                 )\n\u001b[0;32m    581\u001b[0m         \u001b[0mimports\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_imports\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         local_imports = _download_additional_modules(\n\u001b[0m\u001b[0;32m    583\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mbase_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhf_github_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36m_download_additional_modules\u001b[1;34m(name, base_path, imports, download_config)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wrong import_type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         local_import_path = cached_path(\n\u001b[0m\u001b[0;32m    276\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m    296\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mconnected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mftp_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             response = http_head(\n\u001b[0m\u001b[0;32m    551\u001b[0m                 \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36mhttp_head\u001b[1;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[0mheaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"user-agent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_datasets_user_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user-agent\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     response = _request_with_retry(\n\u001b[0m\u001b[0;32m    473\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HEAD\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001b[0m in \u001b[0;36m_request_with_retry\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mtries\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConnectTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 resp = self.send(\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mtls_in_tls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             )\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,model_w,-1) #before train\n",
    "    # my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "    \n",
    "    my_test(valid_dataloader,model_w,epoch) \n",
    "    my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
