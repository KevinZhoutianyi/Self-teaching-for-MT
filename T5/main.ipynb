{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 1000, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=20,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=8,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=4,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=4,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=4,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--model_name', type=str,                   default='t5-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='smooth64',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default='25',      help='howmany step report once')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=128,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=1e-3,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=1e-3,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-4,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.7,    help='momentum')\n",
    "parser.add_argument('--smoothing', type=float,                   default=0.0,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0.9,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=0.1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=0,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=0 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5\\wandb\\run-20220406_151128-kykef9an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/CUDAOOM/runs/kykef9an\" target=\"_blank\">smooth64</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/CUDAOOM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/CUDAOOM/runs/kykef9an?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c95c0d2fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "# os.environ['WANDB_NOTES']='train without A,withoutAandt5smallandbatch64 '\n",
    "wandb.init(project=\"BeforeA\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 03:11:39 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 31.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 03:11:39 PM |\t  Namespace(A_lr=0.0001, batch_size=32, decay=0.001, epochs=50, exp_name='smooth64', gpu=0, grad_acc_count=64, grad_clip=1, learning_rate_min=1e-08, model_name='t5-small', momentum=0.7, pre_epochs=0, rep_num=25, smoothing=0, syndata_loss_ratio=0.1, train_A=0, train_A_num_points=8, train_num_points=1000, train_v_num_points=8, train_v_synthetic_num_points=8, train_w_num_points=8, traindata_loss_ratio=0.9, v_lr=0.001, valid_begin=0, valid_num_points=100, w_lr=0.001)\n",
      "04/06 03:11:39 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "04/06 03:11:39 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\"./log/\", now+'.txt'),'w',encoding = \"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14','de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('tensorboard')\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 03:11:40 PM |\t  modelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,modelname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 03:12:10 PM |\t  train len: 992\n",
      "04/06 03:12:10 PM |\t  train_w_num_points_len: 248\n",
      "04/06 03:12:10 PM |\t  train_v_synthetic_num_points_len: 248\n",
      "04/06 03:12:10 PM |\t  train_v_num_points_len: 248\n",
      "04/06 03:12:10 PM |\t  train_A_num_points_len: 248\n",
      "04/06 03:12:10 PM |\t  valid len: 100\n",
      "04/06 03:12:10 PM |\t  test len: 3003\n",
      "04/06 03:12:10 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "04/06 03:12:10 PM |\t  {'de': 'Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.', 'en': 'translate English to German: However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.'}\n",
      "04/06 03:12:10 PM |\t  {'de': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'en': 'translate English to German: Two sets of lights so close to one another: intentional or just a silly error?'}\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "import random\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelname)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none',label_smoothing=args.smoothing)#,ignore_index = tokenizer.pad_token_id)#\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['validation']['translation'][:args.valid_num_points]\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en'] \n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "preprocess(test)\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "04/06 03:12:10 PM |\t  train data get\n",
      "04/06 03:12:10 PM |\t  train data loader get\n",
      "04/06 03:12:10 PM |\t  valid data loader get\n",
      "04/06 03:12:11 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler=SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=True, num_workers=4)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=16, pin_memory=True, num_workers=4)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=16, pin_memory=True, num_workers=4)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = Adafactor(model_w.parameters(), scale_parameter=True, relative_step=True, warmup_init=False,clip_threshold=1,beta1=0)\n",
    "scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer =Adafactor(model_v.parameters(), scale_parameter=True, relative_step=True, warmup_init=False, clip_threshold=1,beta1=0)\n",
    "scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    metric_bleu =  load_metric('bleu')\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(_dataloader,desc =\"test for epoch\"+str(epoch))):\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        with torch.no_grad():\n",
    "            ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "            acc+= ls\n",
    "            counter+= 1\n",
    "            pre = model.generate(test_dataloaderx)\n",
    "            x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "            pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "            label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "            \n",
    "            pred_str = [x.replace('.', '')  for x in pred_decoded]\n",
    "            label_str = [[x.replace('.', '')] for x in label_decoded]\n",
    "            pred_list = [x.replace('.', '').split()  for x in pred_decoded]\n",
    "            label_list = [[x.replace('.', '').split()] for x in label_decoded]\n",
    "            if  step%100==0:\n",
    "                logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "                logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "                logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "            metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "            \n",
    "    logging.info('computing score...')            \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    writer.add_scalar(model.name+\"/test_loss\", acc/counter, global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/sacreBLEU\",sacrebleu_score['score'], global_step=epoch)\n",
    "    writer.add_scalar(model.name+\"/BLEU\",bleu_score['bleu'], global_step=epoch)\n",
    "    \n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    \n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, ):\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    wsize = args.train_w_num_points #now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points \n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points \n",
    "    grad_acc_count = args.grad_acc_count\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size=[wsize,synsize,vsize,Asize]\n",
    "    for step, batch in enumerate(_dataloader) :\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(device, non_blocking=True)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=True) \n",
    "        (input_w,input_syn,input_v,input_A_v) = torch.split(train_x,split_size)\n",
    "        (input_w_attn,input_syn_attn,input_v_attn,input_A_v_attn) = torch.split(train_x_attn,split_size)\n",
    "        (output_w,_,output_v,output_A_v) = torch.split(train_y,split_size)\n",
    "        (output_w_attn,_,output_v_attn,output_A_v_attn) = torch.split(train_y_attn,split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "       \n",
    "\n",
    "        if (epoch <= args.epochs) and (args.train_A == 1) and epoch >= args.pre_epochs:\n",
    "            architect.step(input_w,  output_w,input_w_attn, output_w_attn, w_optimizer, input_syn, input_syn_attn,input_A_v, input_A_v_attn, output_A_v, \n",
    "                output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\n",
    "        \n",
    "        \n",
    "        if  epoch <= args.epochs:\n",
    "            for p in w_model.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            loss_w = CTG_loss(input_w, input_w_attn, output_w, output_w_attn, attn_idx, A, w_model)\n",
    "            \n",
    "            w_trainloss_acc+=loss_w.item()\n",
    "            loss_w.backward()\n",
    "            objs_w.update(loss_w.item(), wsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(w_model.parameters(), args.grad_clip)\n",
    "                w_optimizer.step()\n",
    "                w_optimizer.zero_grad()\n",
    "            for p in w_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        if epoch >= args.pre_epochs and epoch <= args.epochs:\n",
    "            \n",
    "            for p in v_model.parameters():\n",
    "                p.requires_grad = True\n",
    "            loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "            loss = my_loss2(input_v,input_v_attn,output_v,output_v_attn,model_v)\n",
    "            v_loss =  (args.traindata_loss_ratio*loss+loss_aug*args.syndata_loss_ratio)/num_batch\n",
    "            v_trainloss_acc+=v_loss.item()\n",
    "            v_loss.backward()\n",
    "            objs_v.update(v_loss.item(), vtrainsize)\n",
    "            if ((step + 1) % grad_acc_count == 0) or (step + 1 == loader_len): \n",
    "                # nn.utils.clip_grad_norm(v_model.parameters(), args.grad_clip)\n",
    "                v_optimizer.step()  \n",
    "                v_optimizer.zero_grad() \n",
    "            for p in v_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        print('5',torch.cuda.memory_allocated())\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        fre = (loader_len//args.rep_num)\n",
    "        print('step',step)\n",
    "        print('fre',fre)\n",
    "        if((step)%fre == 0 or (step)==(loader_len-1)):\n",
    "            logging.info(f\"{progress:5.3}% \\t w_loss_avg:{objs_w.avg*train_w_num_points_len:^.7f}\\t v_loss_avg:{objs_v.avg*vtrainsize_total:^.7f}\")\n",
    "  \n",
    "    logging.info(str((\"Attention Weights A : \", A.alpha)))\n",
    "    \n",
    "    return w_trainloss_acc,v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06 03:14:58 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 1461624832\n",
      "2 1461809152\n",
      "3 1461809152\n",
      "31 1461809152\n",
      "32 2853700608\n",
      "4 1461809664\n",
      "41 1461809664\n",
      "42 2505740800\n",
      "43 3889371136\n",
      "5 1461811200\n",
      "step 0\n",
      "fre 1\n",
      "04/06 03:15:02 PM |\t    0.0% \t w_loss_avg:15.4144541\t v_loss_avg:6.2797492\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2078016000\n",
      "43 3462478336\n",
      "5 1461811200\n",
      "step 1\n",
      "fre 1\n",
      "04/06 03:15:04 PM |\t   3.33% \t w_loss_avg:12.4487007\t v_loss_avg:8.3817386\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2429981696\n",
      "43 3813169152\n",
      "5 1461811200\n",
      "step 2\n",
      "fre 1\n",
      "04/06 03:15:06 PM |\t   6.67% \t w_loss_avg:17.0911573\t v_loss_avg:8.0689209\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2490822144\n",
      "43 3873853952\n",
      "5 1461811200\n",
      "step 3\n",
      "fre 1\n",
      "04/06 03:15:09 PM |\t   10.0% \t w_loss_avg:17.7390096\t v_loss_avg:9.1259872\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2259295232\n",
      "43 3639233536\n",
      "5 1461811200\n",
      "step 4\n",
      "fre 1\n",
      "04/06 03:15:11 PM |\t   13.3% \t w_loss_avg:17.8552773\t v_loss_avg:10.3610620\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2577089536\n",
      "43 3959243264\n",
      "5 1461811200\n",
      "step 5\n",
      "fre 1\n",
      "04/06 03:15:14 PM |\t   16.7% \t w_loss_avg:16.7849493\t v_loss_avg:10.0970060\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2149118976\n",
      "43 3532173312\n",
      "5 1461811200\n",
      "step 6\n",
      "fre 1\n",
      "04/06 03:15:16 PM |\t   20.0% \t w_loss_avg:16.0734534\t v_loss_avg:10.1805096\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2517710848\n",
      "43 3898467328\n",
      "5 1461811200\n",
      "step 7\n",
      "fre 1\n",
      "04/06 03:15:19 PM |\t   23.3% \t w_loss_avg:15.7897129\t v_loss_avg:10.0678431\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2279522304\n",
      "43 3661008896\n",
      "5 1461811200\n",
      "step 8\n",
      "fre 1\n",
      "04/06 03:15:21 PM |\t   26.7% \t w_loss_avg:14.9884771\t v_loss_avg:10.0211436\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2097054720\n",
      "43 3484695040\n",
      "5 1461811200\n",
      "step 9\n",
      "fre 1\n",
      "04/06 03:15:23 PM |\t   30.0% \t w_loss_avg:14.5795561\t v_loss_avg:9.7340419\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2168764416\n",
      "43 3549875712\n",
      "5 1461811200\n",
      "step 10\n",
      "fre 1\n",
      "04/06 03:15:25 PM |\t   33.3% \t w_loss_avg:14.3650750\t v_loss_avg:9.6712852\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2577089536\n",
      "43 3959243264\n",
      "5 1461811200\n",
      "step 11\n",
      "fre 1\n",
      "04/06 03:15:28 PM |\t   36.7% \t w_loss_avg:13.6952620\t v_loss_avg:9.4029508\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2368889856\n",
      "43 3750104064\n",
      "5 1461811200\n",
      "step 12\n",
      "fre 1\n",
      "04/06 03:15:31 PM |\t   40.0% \t w_loss_avg:13.3257880\t v_loss_avg:9.3739164\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2248205824\n",
      "43 3627980800\n",
      "5 1461811200\n",
      "step 13\n",
      "fre 1\n",
      "04/06 03:15:33 PM |\t   43.3% \t w_loss_avg:13.0393780\t v_loss_avg:9.3068059\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2271090688\n",
      "43 3654205952\n",
      "5 1461811200\n",
      "step 14\n",
      "fre 1\n",
      "04/06 03:15:35 PM |\t   46.7% \t w_loss_avg:12.8388321\t v_loss_avg:9.2894786\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2436797440\n",
      "43 3819240960\n",
      "5 1461811200\n",
      "step 15\n",
      "fre 1\n",
      "04/06 03:15:37 PM |\t   50.0% \t w_loss_avg:12.9057894\t v_loss_avg:9.2782705\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2239529984\n",
      "43 3618542592\n",
      "5 1461811200\n",
      "step 16\n",
      "fre 1\n",
      "04/06 03:15:39 PM |\t   53.3% \t w_loss_avg:12.6161163\t v_loss_avg:9.1561292\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2577089536\n",
      "43 3959243264\n",
      "5 1461811200\n",
      "step 17\n",
      "fre 1\n",
      "04/06 03:15:42 PM |\t   56.7% \t w_loss_avg:12.2439729\t v_loss_avg:9.0314151\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2372900352\n",
      "43 3757516288\n",
      "5 1461811200\n",
      "step 18\n",
      "fre 1\n",
      "04/06 03:15:44 PM |\t   60.0% \t w_loss_avg:12.1172785\t v_loss_avg:8.9638533\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2577089536\n",
      "43 3959243264\n",
      "5 1461811200\n",
      "step 19\n",
      "fre 1\n",
      "04/06 03:15:48 PM |\t   63.3% \t w_loss_avg:12.1163431\t v_loss_avg:8.8801999\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2343481856\n",
      "43 3725416448\n",
      "5 1461811200\n",
      "step 20\n",
      "fre 1\n",
      "04/06 03:15:50 PM |\t   66.7% \t w_loss_avg:11.8992403\t v_loss_avg:8.9220254\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2388907008\n",
      "43 3772055552\n",
      "5 1461811200\n",
      "step 21\n",
      "fre 1\n",
      "04/06 03:15:52 PM |\t   70.0% \t w_loss_avg:11.8752378\t v_loss_avg:9.0757389\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2312441344\n",
      "43 3693899264\n",
      "5 1461811200\n",
      "step 22\n",
      "fre 1\n",
      "04/06 03:15:54 PM |\t   73.3% \t w_loss_avg:11.8639203\t v_loss_avg:9.5524776\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2302020096\n",
      "43 3682987520\n",
      "5 1461811200\n",
      "step 23\n",
      "fre 1\n",
      "04/06 03:15:57 PM |\t   76.7% \t w_loss_avg:12.1620060\t v_loss_avg:9.9544551\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2462757888\n",
      "43 3845845504\n",
      "5 1461811200\n",
      "step 24\n",
      "fre 1\n",
      "04/06 03:15:59 PM |\t   80.0% \t w_loss_avg:12.0882975\t v_loss_avg:9.9996558\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2534794752\n",
      "43 3917713408\n",
      "5 1461811200\n",
      "step 25\n",
      "fre 1\n",
      "04/06 03:16:02 PM |\t   83.3% \t w_loss_avg:11.9279494\t v_loss_avg:9.8601556\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2248205824\n",
      "43 3627980800\n",
      "5 1461811200\n",
      "step 26\n",
      "fre 1\n",
      "04/06 03:16:04 PM |\t   86.7% \t w_loss_avg:11.8839332\t v_loss_avg:9.7791512\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2135652352\n",
      "43 3523735552\n",
      "5 1461811200\n",
      "step 27\n",
      "fre 1\n",
      "04/06 03:16:06 PM |\t   90.0% \t w_loss_avg:11.8268694\t v_loss_avg:9.8285472\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2317186560\n",
      "43 3698390016\n",
      "5 1461811200\n",
      "step 28\n",
      "fre 1\n",
      "04/06 03:16:08 PM |\t   93.3% \t w_loss_avg:11.8631122\t v_loss_avg:9.8457841\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2084417536\n",
      "43 3467982336\n",
      "5 1461811200\n",
      "step 29\n",
      "fre 1\n",
      "04/06 03:16:09 PM |\t   96.7% \t w_loss_avg:11.7903356\t v_loss_avg:9.7769915\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 2853702144\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2205776896\n",
      "43 3587341312\n",
      "5 1461811200\n",
      "step 30\n",
      "fre 1\n",
      "04/06 03:16:11 PM |\t  1e+02% \t w_loss_avg:11.6892179\t v_loss_avg:9.8097329\n",
      "04/06 03:16:12 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "04/06 03:16:12 PM |\t  w_train_loss:11.68921785056591,v_train_loss:9.809732913970947\n",
      "04/06 03:16:17 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:16:17 PM |\t  pred_decoded[:2]:['republikanische strategy to counter there-election of Obama', 'Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:16:17 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 03:16:32 PM |\t  computing score...\n",
      "04/06 03:16:32 PM |\t  model_w_in_main sacreBLEU : 2.629343\n",
      "04/06 03:16:32 PM |\t  model_w_in_main BLEU : 0.011296\n",
      "04/06 03:16:32 PM |\t  model_w_in_main test loss : 6.848110\n",
      "04/06 03:16:38 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:16:38 PM |\t  pred_decoded[:2]:['Republican strategy to counter the re-lection of Obama', 'Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:16:38 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 03:16:55 PM |\t  computing score...\n",
      "04/06 03:16:55 PM |\t  model_v_in_main sacreBLEU : 5.311380\n",
      "04/06 03:16:55 PM |\t  model_v_in_main BLEU : 0.036737\n",
      "04/06 03:16:55 PM |\t  model_v_in_main test loss : 6.654513\n",
      "04/06 03:16:55 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 1461624832\n",
      "2 1461809152\n",
      "3 1461809152\n",
      "31 1461809152\n",
      "32 3255421440\n",
      "4 1461809664\n",
      "41 1461809664\n",
      "42 2321931264\n",
      "43 4099550720\n",
      "5 1461811200\n",
      "step 0\n",
      "fre 1\n",
      "04/06 03:16:59 PM |\t    0.0% \t w_loss_avg:14.8900246\t v_loss_avg:6.3842419\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2218370048\n",
      "43 3999971328\n",
      "5 1461811200\n",
      "step 1\n",
      "fre 1\n",
      "04/06 03:17:00 PM |\t   3.33% \t w_loss_avg:12.2634625\t v_loss_avg:8.2657465\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2438168064\n",
      "43 4218459648\n",
      "5 1461811200\n",
      "step 2\n",
      "fre 1\n",
      "04/06 03:17:02 PM |\t   6.67% \t w_loss_avg:16.4627353\t v_loss_avg:8.0298565\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2627750400\n",
      "43 4404528128\n",
      "5 1461811200\n",
      "step 3\n",
      "fre 1\n",
      "04/06 03:17:05 PM |\t   10.0% \t w_loss_avg:17.0153766\t v_loss_avg:8.9791124\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2269809152\n",
      "43 4047622144\n",
      "5 1461811200\n",
      "step 4\n",
      "fre 1\n",
      "04/06 03:17:06 PM |\t   13.3% \t w_loss_avg:17.1424576\t v_loss_avg:10.0877080\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2565222912\n",
      "43 4343377920\n",
      "5 1461811200\n",
      "step 5\n",
      "fre 1\n",
      "04/06 03:17:08 PM |\t   16.7% \t w_loss_avg:16.1650673\t v_loss_avg:9.8280462\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2556435968\n",
      "43 4331496448\n",
      "5 1461811200\n",
      "step 6\n",
      "fre 1\n",
      "04/06 03:17:11 PM |\t   20.0% \t w_loss_avg:15.5385813\t v_loss_avg:9.8968276\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2775748096\n",
      "43 4547922432\n",
      "5 1461811200\n",
      "step 7\n",
      "fre 1\n",
      "04/06 03:17:14 PM |\t   23.3% \t w_loss_avg:15.2860408\t v_loss_avg:9.7916713\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2422004224\n",
      "43 4201426944\n",
      "5 1461811200\n",
      "step 8\n",
      "fre 1\n",
      "04/06 03:17:16 PM |\t   26.7% \t w_loss_avg:14.5658084\t v_loss_avg:9.7601727\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2297072640\n",
      "43 4073909248\n",
      "5 1461811200\n",
      "step 9\n",
      "fre 1\n",
      "04/06 03:17:17 PM |\t   30.0% \t w_loss_avg:14.2100119\t v_loss_avg:9.4959598\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2359909376\n",
      "43 4138017280\n",
      "5 1461811200\n",
      "step 10\n",
      "fre 1\n",
      "04/06 03:17:19 PM |\t   33.3% \t w_loss_avg:14.0142197\t v_loss_avg:9.4433606\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2507074560\n",
      "43 4281860096\n",
      "5 1461811200\n",
      "step 11\n",
      "fre 1\n",
      "04/06 03:17:23 PM |\t   36.7% \t w_loss_avg:13.3944291\t v_loss_avg:9.2005672\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2351324160\n",
      "43 4129357312\n",
      "5 1461811200\n",
      "step 12\n",
      "fre 1\n",
      "04/06 03:17:25 PM |\t   40.0% \t w_loss_avg:13.0775657\t v_loss_avg:9.1731079\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2301743104\n",
      "43 4079902720\n",
      "5 1461811200\n",
      "step 13\n",
      "fre 1\n",
      "04/06 03:17:27 PM |\t   43.3% \t w_loss_avg:12.8144643\t v_loss_avg:9.1077175\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2479319552\n",
      "43 4255942144\n",
      "5 1461811200\n",
      "step 14\n",
      "fre 1\n",
      "04/06 03:17:29 PM |\t   46.7% \t w_loss_avg:12.6330749\t v_loss_avg:9.0940940\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2499297792\n",
      "43 4275566592\n",
      "5 1461811200\n",
      "step 15\n",
      "fre 1\n",
      "04/06 03:17:32 PM |\t   50.0% \t w_loss_avg:12.6885830\t v_loss_avg:9.0825391\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2245454848\n",
      "43 4026126336\n",
      "5 1461811200\n",
      "step 16\n",
      "fre 1\n",
      "04/06 03:17:33 PM |\t   53.3% \t w_loss_avg:12.4292985\t v_loss_avg:8.9792638\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2827252736\n",
      "43 4601373184\n",
      "5 1461811200\n",
      "step 17\n",
      "fre 1\n",
      "04/06 03:17:37 PM |\t   56.7% \t w_loss_avg:12.0972127\t v_loss_avg:8.8651847\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2540931072\n",
      "43 4318466048\n",
      "5 1461811200\n",
      "step 18\n",
      "fre 1\n",
      "04/06 03:17:39 PM |\t   60.0% \t w_loss_avg:11.9776819\t v_loss_avg:8.8056006\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2713359872\n",
      "43 4489646080\n",
      "5 1461811200\n",
      "step 19\n",
      "fre 1\n",
      "04/06 03:17:42 PM |\t   63.3% \t w_loss_avg:11.9828366\t v_loss_avg:8.7262051\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2432500224\n",
      "43 4207751168\n",
      "5 1461811200\n",
      "step 20\n",
      "fre 1\n",
      "04/06 03:17:44 PM |\t   66.7% \t w_loss_avg:11.7907902\t v_loss_avg:8.7640728\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2432500224\n",
      "43 4207751168\n",
      "5 1461811200\n",
      "step 21\n",
      "fre 1\n",
      "04/06 03:17:46 PM |\t   70.0% \t w_loss_avg:11.7676811\t v_loss_avg:8.9106117\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2375891456\n",
      "43 4151780864\n",
      "5 1461811200\n",
      "step 22\n",
      "fre 1\n",
      "04/06 03:17:48 PM |\t   73.3% \t w_loss_avg:11.7661519\t v_loss_avg:9.3370908\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2827252736\n",
      "43 4601373184\n",
      "5 1461811200\n",
      "step 23\n",
      "fre 1\n",
      "04/06 03:17:51 PM |\t   76.7% \t w_loss_avg:12.0394950\t v_loss_avg:9.7009157\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2446036480\n",
      "43 4223937536\n",
      "5 1461811200\n",
      "step 24\n",
      "fre 1\n",
      "04/06 03:17:53 PM |\t   80.0% \t w_loss_avg:11.9758755\t v_loss_avg:9.7481465\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2572844032\n",
      "43 4350500864\n",
      "5 1461811200\n",
      "step 25\n",
      "fre 1\n",
      "04/06 03:17:56 PM |\t   83.3% \t w_loss_avg:11.8320142\t v_loss_avg:9.6209697\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2329529856\n",
      "43 4108982784\n",
      "5 1461811200\n",
      "step 26\n",
      "fre 1\n",
      "04/06 03:17:57 PM |\t   86.7% \t w_loss_avg:11.7856295\t v_loss_avg:9.5459995\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2351324160\n",
      "43 4129357312\n",
      "5 1461811200\n",
      "step 27\n",
      "fre 1\n",
      "04/06 03:17:59 PM |\t   90.0% \t w_loss_avg:11.7311621\t v_loss_avg:9.5869832\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2572844032\n",
      "43 4350500864\n",
      "5 1461811200\n",
      "step 28\n",
      "fre 1\n",
      "04/06 03:18:01 PM |\t   93.3% \t w_loss_avg:11.7647013\t v_loss_avg:9.6038296\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2314734592\n",
      "43 4093719552\n",
      "5 1461811200\n",
      "step 29\n",
      "fre 1\n",
      "04/06 03:18:03 PM |\t   96.7% \t w_loss_avg:11.6957228\t v_loss_avg:9.5411361\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3251870720\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2627750400\n",
      "43 4404528128\n",
      "5 1461811200\n",
      "step 30\n",
      "fre 1\n",
      "04/06 03:18:06 PM |\t  1e+02% \t w_loss_avg:11.6006152\t v_loss_avg:9.5743195\n",
      "04/06 03:18:06 PM |\t  ('Attention Weights A : ', Parameter containing:\n",
      "tensor([0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n",
      "        0.0040, 0.0040, 0.0040, 0.0040, 0.0040], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "04/06 03:18:06 PM |\t  w_train_loss:11.600615233182907,v_train_loss:9.574319496750832\n",
      "04/06 03:18:14 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:18:14 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Politiker haben ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug begründet.']\n",
      "04/06 03:18:14 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 03:18:34 PM |\t  computing score...\n",
      "04/06 03:18:34 PM |\t  model_w_in_main sacreBLEU : 22.426429\n",
      "04/06 03:18:34 PM |\t  model_w_in_main BLEU : 0.188755\n",
      "04/06 03:18:34 PM |\t  model_w_in_main test loss : 7.019894\n",
      "04/06 03:18:40 PM |\t  x_decoded[:2]:['translate English to German: A Republican strategy to counter the re-election of Obama', 'translate English to German: Republican leaders justified their policy by the need to combat electoral fraud.']\n",
      "04/06 03:18:40 PM |\t  pred_decoded[:2]:['Eine republikanische Strategie zur Bekämpfung der Wiederwahl von Obama', 'Die republikanischen Führer haben ihre Politik durch die Notwendigkeit der Bekämpfung von Wahlbetrug begründet.']\n",
      "04/06 03:18:40 PM |\t  label_decoded[:2]:['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.']\n",
      "04/06 03:18:58 PM |\t  computing score...\n",
      "04/06 03:18:58 PM |\t  model_v_in_main sacreBLEU : 22.104986\n",
      "04/06 03:18:58 PM |\t  model_v_in_main BLEU : 0.190536\n",
      "04/06 03:18:58 PM |\t  model_v_in_main test loss : 7.074385\n",
      "04/06 03:18:58 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:None,\t\tlr_v:None----------------\n",
      "1 1461624832\n",
      "2 1461809152\n",
      "3 1461809152\n",
      "31 1461809152\n",
      "32 3255421440\n",
      "4 1461809664\n",
      "41 1461809664\n",
      "42 2672154624\n",
      "43 4452343296\n",
      "5 1461811200\n",
      "step 0\n",
      "fre 1\n",
      "04/06 03:19:03 PM |\t    0.0% \t w_loss_avg:14.8157712\t v_loss_avg:6.3053663\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2262145536\n",
      "43 4043945472\n",
      "5 1461811200\n",
      "step 1\n",
      "fre 1\n",
      "04/06 03:19:05 PM |\t   3.33% \t w_loss_avg:12.0815179\t v_loss_avg:8.3250193\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2629242880\n",
      "43 4408972800\n",
      "5 1461811200\n",
      "step 2\n",
      "fre 1\n",
      "04/06 03:19:07 PM |\t   6.67% \t w_loss_avg:16.4784898\t v_loss_avg:8.0430978\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 3\n",
      "fre 1\n",
      "04/06 03:19:10 PM |\t   10.0% \t w_loss_avg:17.1419088\t v_loss_avg:9.0860399\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2534213632\n",
      "43 4314404864\n",
      "5 1461811200\n",
      "step 4\n",
      "fre 1\n",
      "04/06 03:19:13 PM |\t   13.3% \t w_loss_avg:17.2406083\t v_loss_avg:10.2478021\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 5\n",
      "fre 1\n",
      "04/06 03:19:24 PM |\t   16.7% \t w_loss_avg:16.2557533\t v_loss_avg:9.9879573\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2394071552\n",
      "43 4173369856\n",
      "5 1461811200\n",
      "step 6\n",
      "fre 1\n",
      "04/06 03:19:27 PM |\t   20.0% \t w_loss_avg:15.5937188\t v_loss_avg:10.0749695\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 7\n",
      "fre 1\n",
      "04/06 03:19:30 PM |\t   23.3% \t w_loss_avg:15.3613406\t v_loss_avg:9.9688951\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2503383040\n",
      "43 4282759680\n",
      "5 1461811200\n",
      "step 8\n",
      "fre 1\n",
      "04/06 03:19:32 PM |\t   26.7% \t w_loss_avg:14.6207696\t v_loss_avg:9.9311089\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2293834752\n",
      "43 4075420160\n",
      "5 1461811200\n",
      "step 9\n",
      "fre 1\n",
      "04/06 03:19:34 PM |\t   30.0% \t w_loss_avg:14.2415092\t v_loss_avg:9.6669403\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2424706560\n",
      "43 4205915648\n",
      "5 1461811200\n",
      "step 10\n",
      "fre 1\n",
      "04/06 03:19:37 PM |\t   33.3% \t w_loss_avg:14.0501261\t v_loss_avg:9.6033033\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 11\n",
      "fre 1\n",
      "04/06 03:19:41 PM |\t   36.7% \t w_loss_avg:13.4216875\t v_loss_avg:9.3538016\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2747943936\n",
      "43 4529951744\n",
      "5 1461811200\n",
      "step 12\n",
      "fre 1\n",
      "04/06 03:19:44 PM |\t   40.0% \t w_loss_avg:13.0683250\t v_loss_avg:9.3206287\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2439260672\n",
      "43 4220676608\n",
      "5 1461811200\n",
      "step 13\n",
      "fre 1\n",
      "04/06 03:19:47 PM |\t   43.3% \t w_loss_avg:12.7995062\t v_loss_avg:9.2639558\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2612924416\n",
      "43 4392760320\n",
      "5 1461811200\n",
      "step 14\n",
      "fre 1\n",
      "04/06 03:19:49 PM |\t   46.7% \t w_loss_avg:12.6052240\t v_loss_avg:9.2445272\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 15\n",
      "fre 1\n",
      "04/06 03:19:53 PM |\t   50.0% \t w_loss_avg:12.6760456\t v_loss_avg:9.2295027\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2401205248\n",
      "43 4179684864\n",
      "5 1461811200\n",
      "step 16\n",
      "fre 1\n",
      "04/06 03:19:56 PM |\t   53.3% \t w_loss_avg:12.4040992\t v_loss_avg:9.1166957\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n",
      "42 2826057216\n",
      "43 4602098688\n",
      "5 1461811200\n",
      "step 17\n",
      "fre 1\n",
      "04/06 03:20:04 PM |\t   56.7% \t w_loss_avg:12.0491689\t v_loss_avg:8.9926303\n",
      "1 1461811200\n",
      "2 1461811200\n",
      "3 1461811200\n",
      "31 1461811200\n",
      "32 3252920320\n",
      "4 1461811200\n",
      "41 1461811200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_94904/2317287350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_94904/3224930238.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(epoch, _dataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'41'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mloss_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,input_v,input_v_attn,output_v,output_v_attn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'42'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_loss2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_v_attn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_v_attn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\losses.py\u001b[0m in \u001b[0;36mcalc_loss_aug\u001b[1;34m(input_syn_ids, input_syn_attn, w_model, v_model)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0matt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mw_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# print(\"start of : generate\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             )\n\u001b[0;32m   1250\u001b[0m             \u001b[1;31m# 12. run beam search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1252\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2035\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   2036\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m         \u001b[1;31m# Decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1635\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1636\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                 )\n\u001b[0;32m   1029\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1031\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    689\u001b[0m                 \u001b[0mquery_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[0;32m    692\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     ):\n\u001b[1;32m--> 605\u001b[1;33m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m         attention_output = self.EncDecAttention(\n\u001b[0;32m    607\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;31m# half-precision inputs is done in fp32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.valid_begin==1):\n",
    "    my_test(valid_dataloader,model_w,-1) #before train\n",
    "    # my_test(valid_dataloader,model_v,-1)  \n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "    writer.add_scalar(\"MT/model_w_in_main/w_trainloss\", w_train_loss, global_step=epoch)\n",
    "    writer.add_scalar(\"MT/model_v_in_main/v_trainloss\", v_train_loss, global_step=epoch)\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "    \n",
    "    my_test(valid_dataloader,model_w,epoch) \n",
    "    my_test(valid_dataloader,model_v,epoch)  \n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65768f95ed3f1ad80799466926a66640b39a99ef5d94bbece814e59aa067606e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
