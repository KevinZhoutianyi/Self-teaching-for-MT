{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from T5 import *\n",
    "import torch\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import T5Tokenizer\n",
    "import torch_optimizer as optim\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from MT_hyperparams import seed_,max_length,target_language\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils import *\n",
    "from attention_params import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from losses import *\n",
    "from architect import *\n",
    "import logging\n",
    "import sys\n",
    "import transformers\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"main\")\n",
    "\n",
    "\n",
    "parser.add_argument('--valid_num_points', type=int,             default = 100, help='validation data number')\n",
    "parser.add_argument('--train_num_points', type=int,             default = 200, help='train data number')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int,                   default=16,     help='Batch size')\n",
    "parser.add_argument('--train_w_num_points', type=int,           default=4,      help='train_w_num_points for each batch')\n",
    "parser.add_argument('--train_v_synthetic_num_points', type=int, default=2,      help='train_v_synthetic_num_points for each batch')\n",
    "parser.add_argument('--train_v_num_points', type=int,           default=4,      help='train_v_num_points for each batch')\n",
    "parser.add_argument('--train_A_num_points', type=int,           default=6,      help='train_A_num_points decay for each batch')\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu', type=int,                          default=0,      help='gpu device id')\n",
    "parser.add_argument('--model_name_teacher', type=str,           default='t5-small',      help='model_name')\n",
    "parser.add_argument('--model_name_student', type=str,           default='t5-small',      help='model_name')\n",
    "parser.add_argument('--exp_name', type=str,                     default='T5spec',      help='experiment name')\n",
    "parser.add_argument('--rep_num', type=int,                      default=50,      help='report times for 1 epoch')\n",
    "parser.add_argument('--test_num', type=int,                     default=200,      help='test times for 1 epoch')\n",
    "\n",
    "parser.add_argument('--epochs', type=int,                       default=50,     help='num of training epochs')\n",
    "parser.add_argument('--pre_epochs', type=int,                   default=0,      help='train model W for x epoch first')\n",
    "parser.add_argument('--grad_clip', type=float,                  default=1,      help='gradient clipping')\n",
    "parser.add_argument('--grad_acc_count', type=float,             default=-1,      help='gradient accumulate steps')\n",
    "\n",
    "parser.add_argument('--w_lr', type=float,                       default=5e-4,   help='learning rate for w')\n",
    "parser.add_argument('--unrolled_w_lr', type=float,              default=5e-4,   help='learning rate for w')\n",
    "parser.add_argument('--v_lr', type=float,                       default=5e-4,   help='learning rate for v')\n",
    "parser.add_argument('--unrolled_v_lr', type=float,              default=5e-4,   help='learning rate for v')\n",
    "parser.add_argument('--A_lr', type=float,                       default=1e-1,   help='learning rate for A')\n",
    "parser.add_argument('--learning_rate_min', type=float,          default=1e-8,   help='learning_rate_min')\n",
    "parser.add_argument('--decay', type=float,                      default=1e-3,   help='weight decay')\n",
    "parser.add_argument('--momentum', type=float,                   default=0.9,    help='momentum')\n",
    "# parser.add_argument('--smoothing', type=float,                  default=0.1,    help='labelsmoothing')\n",
    "\n",
    "\n",
    "parser.add_argument('--traindata_loss_ratio', type=float,       default=0,    help='human translated data ratio')\n",
    "parser.add_argument('--syndata_loss_ratio', type=float,         default=1,    help='augmented dataset ratio')\n",
    "\n",
    "parser.add_argument('--valid_begin', type=int,                  default=1,      help='whether valid before train')\n",
    "parser.add_argument('--train_A', type=int,                      default=1 ,     help='whether train A')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])#(args=['--batch_size', '8',  '--no_cuda'])#used in ipynb\n",
    "args.test_num = args.test_num//args.batch_size * args.batch_size\n",
    "args.rep_num = args.rep_num//args.batch_size * args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monlydrinkwater\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>g:\\GitCode\\Self-teaching-for-machine-translation\\T5\\wandb\\run-20220611_222750-222py95n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/222py95n\" target=\"_blank\">T5spec</a></strong> to <a href=\"https://wandb.ai/onlydrinkwater/Selftraining\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/onlydrinkwater/Selftraining/runs/222py95n?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x18c7d842a00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://wandb.ai/ check the running status online\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY']='a166474b1b7ad33a0549adaaec19a2f6d3f91d87'\n",
    "os.environ['WANDB_NAME']=args.exp_name\n",
    "wandb.init(project=\"Selftraining\",config=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 10:27:59 PM |\t  Reusing dataset wmt14 (C:\\Users\\kevin\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\d239eaf0ff090d28da19b6bc9758e24634d84de0a1ef092f0b5c54e6f132d7e2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 33.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 10:27:59 PM |\t  Namespace(A_lr=0.001, batch_size=16, decay=0.001, epochs=50, exp_name='T5spec', gpu=0, grad_acc_count=-1, grad_clip=1, learning_rate_min=1e-08, model_name_student='t5-small', model_name_teacher='t5-small', momentum=0.9, pre_epochs=0, rep_num=48, syndata_loss_ratio=1, test_num=192, train_A=1, train_A_num_points=6, train_num_points=200, train_v_num_points=4, train_v_synthetic_num_points=2, train_w_num_points=4, traindata_loss_ratio=0, unrolled_v_lr=0.0005, unrolled_w_lr=0.0005, v_lr=0.0005, valid_begin=1, valid_num_points=100, w_lr=0.0005)\n",
      "06/11 10:27:59 PM |\t  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 4508785\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "06/11 10:27:59 PM |\t  {'translation': {'de': 'Ich bitte Sie, sich zu einer Schweigeminute zu erheben.', 'en': \"Please rise, then, for this minute' s silence.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# logging file\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "\n",
    "log_format = '%(asctime)s |\\t  %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(\n",
    "    \"./log/\", now+'.txt'), 'w', encoding=\"UTF-8\")\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "dataset = load_dataset('wmt14', 'de-en')\n",
    "\n",
    "logging.info(args)\n",
    "logging.info(dataset)\n",
    "logging.info(dataset['train'][5])\n",
    "\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(seed_)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(seed_)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 10:28:00 PM |\t  modelsize:60.506624MB\n",
      "06/11 10:28:02 PM |\t  modelsize:60.506624MB\n"
     ]
    }
   ],
   "source": [
    "modelname = args.model_name_teacher\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,pathname+'.pt')\n",
    "\n",
    "modelname = args.model_name_student\n",
    "pretrained  =  T5ForConditionalGeneration.from_pretrained(modelname)\n",
    "pathname = modelname.replace('/','')\n",
    "logging.info(f'modelsize:{count_parameters_in_MB(pretrained)}MB')\n",
    "torch.save(pretrained,pathname+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-small\n",
      "06/11 10:29:01 PM |\t  train len: 192\n",
      "06/11 10:29:01 PM |\t  train_w_num_points_len: 48\n",
      "06/11 10:29:01 PM |\t  train_v_synthetic_num_points_len: 24\n",
      "06/11 10:29:01 PM |\t  train_v_num_points_len: 48\n",
      "06/11 10:29:01 PM |\t  train_A_num_points_len: 72\n",
      "06/11 10:29:01 PM |\t  valid len: 100\n",
      "06/11 10:29:01 PM |\t  test len: 3003\n",
      "06/11 10:29:01 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "06/11 10:29:01 PM |\t  {'de': 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.', 'en': \"translate English to German: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"}\n",
      "06/11 10:29:01 PM |\t  {'de': 'Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?', 'en': 'Two sets of lights so close to one another: intentional or just a silly error?'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess the data, make a dataloader\n",
    "import random\n",
    "print(modelname)\n",
    "tokenizer = T5Tokenizer.from_pretrained(modelname)\n",
    "criterion = torch.nn.CrossEntropyLoss( reduction='none')#teacher shouldn't have label smoothing, especially when student got same size.\n",
    "criterion_v = torch.nn.CrossEntropyLoss( reduction='none')#,label_smoothing=args.smoothing) #without LS, V may be too confident to that syn data, and LS do well for real data also.\n",
    "# dataset = dataset.shuffle(seed=seed_)\n",
    "train = dataset['train']['translation'][:args.train_num_points]\n",
    "valid = dataset['train']['translation'][:args.valid_num_points]#TODO:change dataset['validation']['translation'][:args.valid_num_points]args.train_num_points:args.train_num_points+args.valid_num_points\n",
    "test = dataset['test']['translation']#[L_t+L_v:L_t+L_v+L_test]\n",
    "\n",
    "def preprocess(dat):\n",
    "    for t in dat:\n",
    "        t['en'] = \"translate English to German: \" + t['en']  #needed for T5\n",
    "preprocess(train)\n",
    "preprocess(valid)\n",
    "# preprocess(test)#TODO:\n",
    "#TODO: Syn_input should be monolingual data, should try en-fo's en. cuz wmt may align\n",
    "num_batch = args.train_num_points//args.batch_size\n",
    "train = train[:args.batch_size*num_batch]\n",
    "logging.info(\"train len: %d\",len(train))\n",
    "\n",
    "'''\n",
    "each mini batch consist of : \n",
    "1. data to train W\n",
    "2. monolingual data to generate parallel data\n",
    "3. data to train V\n",
    "4. data to train A\n",
    "'''\n",
    "train_w_num_points_len = num_batch * args.train_w_num_points\n",
    "train_v_synthetic_num_points_len = num_batch * args.train_v_synthetic_num_points\n",
    "train_v_num_points_len = num_batch * args.train_v_num_points\n",
    "train_A_num_points_len = num_batch * args.train_A_num_points\n",
    "logging.info(\"train_w_num_points_len: %d\",train_w_num_points_len)\n",
    "logging.info(\"train_v_synthetic_num_points_len: %d\",train_v_synthetic_num_points_len)\n",
    "logging.info(\"train_v_num_points_len: %d\",train_v_num_points_len)\n",
    "logging.info(\"train_A_num_points_len: %d\",train_A_num_points_len)\n",
    "\n",
    "attn_idx_list = torch.arange(train_w_num_points_len).cuda()\n",
    "logging.info(\"valid len: %d\",len(valid))\n",
    "logging.info(\"test len: %d\" ,len(test))\n",
    "logging.info(train[2])\n",
    "logging.info(valid[2])\n",
    "logging.info(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data start\n",
      "get train data end\n",
      "06/11 10:29:01 PM |\t  train data get\n",
      "06/11 10:29:01 PM |\t  train data loader get\n",
      "06/11 10:29:01 PM |\t  valid data loader get\n",
      "06/11 10:29:02 PM |\t  test data loader get\n"
     ]
    }
   ],
   "source": [
    "target_language  = 'de'\n",
    "train_data = get_train_Dataset(train, tokenizer)# Create the DataLoader for our training set.\n",
    "logging.info('train data get')\n",
    "train_dataloader = DataLoader(train_data, sampler= SequentialSampler(train_data), \n",
    "                        batch_size=args.batch_size, pin_memory=False, num_workers=0)\n",
    "logging.info('train data loader get')\n",
    "valid_data = get_aux_dataset(valid, tokenizer)# Create the DataLoader for our training set.\n",
    "valid_dataloader = DataLoader(valid_data, sampler=SequentialSampler(valid_data), \n",
    "                        batch_size=args.batch_size, pin_memory=False, num_workers=0)\n",
    "logging.info('valid data loader get')\n",
    "test_data = get_aux_dataset(test, tokenizer)# Create the DataLoader for our training set.\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data),\n",
    "                        batch_size=args.batch_size, pin_memory=False, num_workers=0)#, sampler=RandomSampler(test_data)\n",
    "logging.info('test data loader get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = attention_params(train_w_num_points_len)#half of train regarded as u\n",
    "A = A.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: model loaded from saved model\n",
    "model_w = T5(criterion=criterion, tokenizer= tokenizer, args = args, name = 'model_w_in_main')\n",
    "model_w = model_w.cuda()\n",
    "w_optimizer = torch.optim.Adam(model_w.parameters(),  lr= args.w_lr ,  betas=(0, args.momentum)  )\n",
    "# w_optimizer = Adafactor(model_w.parameters(), lr = args.w_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_w  = torch.optim.lr_scheduler.StepLR(w_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "model_v = T5(criterion=criterion_v, tokenizer= tokenizer, args = args, name = 'model_v_in_main')\n",
    "model_v = model_v.cuda()\n",
    "v_optimizer = torch.optim.Adam(model_v.parameters(),  lr= args.v_lr ,  betas=(0, args.momentum)  )\n",
    "# v_optimizer =Adafactor(model_v.parameters(), lr = args.v_lr ,scale_parameter=False, relative_step=False , warmup_init=False,clip_threshold=1,beta1=0,eps=( 1e-30,0.001))\n",
    "scheduler_v  = torch.optim.lr_scheduler.StepLR(v_optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "\n",
    "architect = Architect(model_w, model_v,  A, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def my_test(_dataloader,model,epoch):\n",
    "    # logging.info(f\"GPU mem before test:{getGPUMem(device)}%\")\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    metric_sacrebleu =  load_metric('sacrebleu')\n",
    "    # metric_bleu =  load_metric('bleu')\n",
    "    \n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        \n",
    "        test_dataloaderx = Variable(batch[0], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloaderx_attn = Variable(batch[1], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery = Variable(batch[2], requires_grad=False).to(device, non_blocking=False)\n",
    "        test_dataloadery_attn = Variable(batch[3], requires_grad=False).to(device, non_blocking=False)\n",
    "        ls = my_loss(test_dataloaderx,test_dataloaderx_attn,test_dataloadery,test_dataloadery_attn,model)\n",
    "        acc+= ls.item()\n",
    "        counter+= 1\n",
    "        pre = model.generate(test_dataloaderx)\n",
    "        x_decoded = tokenizer.batch_decode(test_dataloaderx,skip_special_tokens=True)\n",
    "        pred_decoded = tokenizer.batch_decode(pre,skip_special_tokens=True)\n",
    "        label_decoded =  tokenizer.batch_decode(test_dataloadery,skip_special_tokens=True)\n",
    "        \n",
    "        pred_str = [x  for x in pred_decoded]\n",
    "        label_str = [[x] for x in label_decoded]\n",
    "        # pred_list = [x.split()  for x in pred_decoded]\n",
    "        # label_list = [[x.split()] for x in label_decoded]\n",
    "        metric_sacrebleu.add_batch(predictions=pred_str, references=label_str)\n",
    "        # metric_bleu.add_batch(predictions=pred_list, references=label_list)\n",
    "        if  step==0:\n",
    "            logging.info(f'x_decoded[:2]:{x_decoded[:2]}')\n",
    "            logging.info(f'pred_decoded[:2]:{pred_decoded[:2]}')\n",
    "            logging.info(f'label_decoded[:2]:{label_decoded[:2]}')\n",
    "            \n",
    "            \n",
    "    logging.info('computing score...') \n",
    "    sacrebleu_score = metric_sacrebleu.compute()\n",
    "    # bleu_score = metric_bleu.compute()\n",
    "    logging.info('%s sacreBLEU : %f',model.name,sacrebleu_score['score'])#TODO:bleu may be wrong cuz max length\n",
    "    # logging.info('%s BLEU : %f',model.name,bleu_score['bleu'])\n",
    "    logging.info('%s test loss : %f',model.name,acc/(counter))\n",
    "    wandb.log({'sacreBLEU'+model.name: sacrebleu_score['score']})\n",
    "    wandb.log({'test_loss'+model.name: acc/counter})\n",
    "    # del test_dataloaderx,acc,counter,test_dataloaderx_attn,sacrebleu_score,bleu_score,test_dataloadery,test_dataloadery_attn,ls,pre,x_decoded,pred_decoded,label_decoded,pred_str,label_str,pred_list,label_list\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter):\n",
    "\n",
    "    objs_w = AvgrageMeter()\n",
    "    objs_v_syn = AvgrageMeter()\n",
    "    objs_v_train = AvgrageMeter()\n",
    "    objs_v_star_val = AvgrageMeter()\n",
    "    objs_v_val = AvgrageMeter()\n",
    "    v_trainloss_acc = 0\n",
    "    w_trainloss_acc = 0\n",
    "    # now  train_x is [num of batch, datasize], so its seperate batch for the code below\n",
    "    wsize = args.train_w_num_points\n",
    "    synsize = args.train_v_synthetic_num_points\n",
    "    vsize = args.train_v_num_points\n",
    "    vtrainsize = vsize+synsize\n",
    "    vtrainsize_total = train_v_num_points_len+train_v_synthetic_num_points_len\n",
    "    Asize = args.train_A_num_points\n",
    "    loader_len = len(_dataloader)\n",
    "    split_size = [wsize, synsize, vsize, Asize]\n",
    "    bs = args.batch_size\n",
    "\n",
    "    logging.info(f\"split size:{split_size}\")\n",
    "    for step, batch in enumerate(_dataloader):\n",
    "        tot_iter[0] += bs\n",
    "        \n",
    "        w_model.eval()#!train\n",
    "        v_model.eval()\n",
    "\n",
    "        # logging.info(f\"GPU mem :{getGPUMem(device)}%\")\n",
    "        train_x = Variable(batch[0], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_x_attn = Variable(batch[1], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y = Variable(batch[2], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        train_y_attn = Variable(batch[3], requires_grad=False).to(\n",
    "            device, non_blocking=False)\n",
    "        (input_w, input_syn, input_v, input_A_v) = torch.split(train_x, split_size)\n",
    "        (input_w_attn, input_syn_attn, input_v_attn,\n",
    "         input_A_v_attn) = torch.split(train_x_attn, split_size)\n",
    "        (output_w, _, output_v, output_A_v) = torch.split(train_y, split_size)\n",
    "        (output_w_attn, _, output_v_attn, output_A_v_attn) = torch.split(\n",
    "            train_y_attn, split_size)\n",
    "        attn_idx = attn_idx_list[wsize*step:(wsize*step+wsize)]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     print('out_old_loss',my_loss2(input_A_v, input_A_v_attn,  output_A_v, output_A_v_attn,v_model))\n",
    "\n",
    "        if (args.train_A == 1):\n",
    "            epsilon_w = args.unrolled_w_lr\n",
    "            epsilon_v  = args.unrolled_v_lr\n",
    "            v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, output_w_attn, w_optimizer,\n",
    "                                             input_v, input_v_attn, output_v, output_v_attn, input_syn, input_syn_attn,\n",
    "                                             input_A_v, input_A_v_attn, output_A_v, output_A_v_attn, v_optimizer,\n",
    "                                             attn_idx, lr_w, lr_v)\n",
    "            objs_v_star_val.update(v_star_val_loss, Asize)\n",
    "        \n",
    "\n",
    "\n",
    "        w_model.eval()\n",
    "        w_optimizer.zero_grad()\n",
    "        loss_w = CTG_loss(input_w, input_w_attn, output_w,\n",
    "                          output_w_attn, attn_idx, A, w_model)\n",
    "        w_trainloss_acc += loss_w.item()\n",
    "        loss_w.backward()\n",
    "        objs_w.update(loss_w.item(), wsize)\n",
    "        w_optimizer.step()\n",
    "        \n",
    "\n",
    "        w_model.eval()\n",
    "\n",
    "        v_optimizer.zero_grad()\n",
    "        loss_aug = calc_loss_aug(input_syn, input_syn_attn, w_model, v_model)\n",
    "        loss = my_loss2(input_v, input_v_attn, output_v,\n",
    "                        output_v_attn, v_model)\n",
    "        v_loss = (args.traindata_loss_ratio*loss +\n",
    "                  loss_aug*args.syndata_loss_ratio)\n",
    "        v_trainloss_acc += v_loss.item()\n",
    "        v_loss.backward()\n",
    "        objs_v_syn.update(loss_aug.item(), synsize)\n",
    "        objs_v_train.update(loss.item(), vsize)\n",
    "        v_optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valloss = my_loss2(input_A_v, input_A_v_attn,  output_A_v, output_A_v_attn,v_model)\n",
    "            objs_v_val.update(valloss.item(), Asize)\n",
    "\n",
    "        progress = 100*(step)/(loader_len-1)\n",
    "        if(tot_iter[0] % args.test_num == 0 and tot_iter[0] != 0):\n",
    "            my_test(validdataloader, model_w, epoch)\n",
    "            my_test(validdataloader, model_v, epoch)\n",
    "            logging.info(str((\"Attention Weights A : \", A.ReLU(A.alpha))))\n",
    "\n",
    "        if(tot_iter[0] % args.rep_num == 0 and tot_iter[0] != 0):\n",
    "            logging.info(f\"{progress:5.3}%:\\t  W_train_loss:{objs_w.avg:^.7f}\\tV_train_syn_loss:{objs_v_syn.avg:^.7f}\\tV_train_loss:{objs_v_train.avg:^.7f}\\t  V_star_val_loss:{objs_v_star_val.avg:^.7f}\\t  V_val_loss:{objs_v_val.avg:^.7f}\")\n",
    "            wandb.log({'W_train_loss': objs_w.avg})\n",
    "            wandb.log({'V_train_syn_loss': objs_v_syn.avg})\n",
    "            wandb.log({'V_train_loss': objs_v_train.avg})\n",
    "            wandb.log({'V_star_val_loss': objs_v_star_val.avg})\n",
    "            wandb.log({'V_val_loss': objs_v_val.avg})\n",
    "            objs_v_syn.reset()\n",
    "            objs_v_train.reset()\n",
    "            objs_w.reset()\n",
    "            objs_v_star_val.reset()\n",
    "            objs_v_val.reset()\n",
    "\n",
    "    return w_trainloss_acc, v_trainloss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 10:29:03 PM |\t  \n",
      "\n",
      "  ----------------epoch:0,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:29:03 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:29:19 PM |\t   18.2%:\t  W_train_loss:1.0977718\tV_train_syn_loss:0.3267541\tV_train_loss:0.8467456\t  V_star_val_loss:1.1186879\t  V_val_loss:1.1154877\n",
      "06/11 10:29:30 PM |\t   45.5%:\t  W_train_loss:1.1423897\tV_train_syn_loss:0.4541283\tV_train_loss:1.1400205\t  V_star_val_loss:0.9555952\t  V_val_loss:0.9555694\n",
      "06/11 10:29:40 PM |\t   72.7%:\t  W_train_loss:0.7431404\tV_train_syn_loss:0.1465696\tV_train_loss:0.9083264\t  V_star_val_loss:1.0914839\t  V_val_loss:1.0914097\n",
      "06/11 10:29:56 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:29:56 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich wünsche Ihnen nochmals ein glückliches neues Jahr, in der Hoffnung, daß Sie einen angenehmen feierlichen Tag']\n",
      "06/11 10:29:56 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:30:10 PM |\t  computing score...\n",
      "06/11 10:30:10 PM |\t  model_w_in_main sacreBLEU : 29.561091\n",
      "06/11 10:30:10 PM |\t  model_w_in_main test loss : 0.797566\n",
      "06/11 10:30:13 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:30:13 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wieder aufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr wünschen, in der Hoffnung, daß Sie eine angenehme Festzeit genoss']\n",
      "06/11 10:30:13 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:30:27 PM |\t  computing score...\n",
      "06/11 10:30:27 PM |\t  model_v_in_main sacreBLEU : 25.433419\n",
      "06/11 10:30:27 PM |\t  model_v_in_main test loss : 1.038342\n",
      "06/11 10:30:27 PM |\t  ('Attention Weights A : ', tensor([1.0008, 1.0010, 0.9990, 1.0010, 0.9986, 1.0014, 0.9986, 1.0014, 0.9984,\n",
      "        0.9984, 1.0012, 0.9984, 0.9982, 1.0018, 0.9982, 0.9982, 1.0020, 0.9980,\n",
      "        1.0020, 1.0020, 0.9979, 1.0022, 0.9978, 0.9979, 0.9977, 1.0023, 1.0022,\n",
      "        1.0023, 0.9976, 0.9976, 0.9977, 1.0023, 1.0024, 0.9975, 1.0024, 0.9975,\n",
      "        1.0025, 0.9975, 1.0025, 0.9975, 0.9974, 0.9976, 0.9974, 0.9974, 1.0027,\n",
      "        0.9973, 0.9973, 0.9974], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:30:27 PM |\t  1e+02%:\t  W_train_loss:0.9437178\tV_train_syn_loss:0.4580103\tV_train_loss:1.1313501\t  V_star_val_loss:1.0818186\t  V_val_loss:1.0822513\n",
      "06/11 10:30:27 PM |\t  w_train_loss:11.78105890750885,v_train_loss:4.156386815011501\n",
      "06/11 10:30:27 PM |\t  \n",
      "\n",
      "  ----------------epoch:1,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:30:27 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:30:39 PM |\t   18.2%:\t  W_train_loss:0.6283941\tV_train_syn_loss:0.8257509\tV_train_loss:0.8840169\t  V_star_val_loss:1.1374840\t  V_val_loss:1.1375574\n",
      "06/11 10:30:50 PM |\t   45.5%:\t  W_train_loss:0.5845399\tV_train_syn_loss:0.5278290\tV_train_loss:1.1461107\t  V_star_val_loss:0.9636894\t  V_val_loss:0.9636454\n",
      "06/11 10:31:01 PM |\t   72.7%:\t  W_train_loss:0.3036379\tV_train_syn_loss:0.4482884\tV_train_loss:0.9221104\t  V_star_val_loss:1.1004280\t  V_val_loss:1.1004502\n",
      "06/11 10:31:17 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:31:17 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie einen angenehmen und erholsamen']\n",
      "06/11 10:31:17 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:31:31 PM |\t  computing score...\n",
      "06/11 10:31:31 PM |\t  model_w_in_main sacreBLEU : 36.789377\n",
      "06/11 10:31:31 PM |\t  model_w_in_main test loss : 0.738956\n",
      "06/11 10:31:33 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:31:33 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wieder aufgenommen, und ich wünsche Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung, daß Sie einen angenehmen Festtag genossen']\n",
      "06/11 10:31:33 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:31:47 PM |\t  computing score...\n",
      "06/11 10:31:47 PM |\t  model_v_in_main sacreBLEU : 25.067351\n",
      "06/11 10:31:47 PM |\t  model_v_in_main test loss : 1.031822\n",
      "06/11 10:31:47 PM |\t  ('Attention Weights A : ', tensor([1.0023, 0.9999, 0.9965, 1.0011, 0.9975, 1.0011, 0.9980, 1.0040, 0.9977,\n",
      "        0.9961, 1.0040, 0.9976, 0.9986, 0.9989, 0.9954, 1.0010, 1.0008, 0.9975,\n",
      "        1.0020, 1.0016, 1.0005, 1.0020, 0.9986, 0.9955, 0.9972, 1.0010, 1.0019,\n",
      "        1.0036, 0.9962, 0.9978, 0.9948, 1.0032, 1.0054, 0.9949, 0.9997, 0.9946,\n",
      "        1.0005, 0.9952, 1.0013, 0.9970, 0.9975, 0.9946, 0.9973, 0.9999, 0.9998,\n",
      "        1.0002, 0.9996, 1.0003], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:31:47 PM |\t  1e+02%:\t  W_train_loss:0.3992942\tV_train_syn_loss:0.5001584\tV_train_loss:1.0860281\t  V_star_val_loss:1.0505948\t  V_val_loss:1.0507115\n",
      "06/11 10:31:47 PM |\t  w_train_loss:5.747598275542259,v_train_loss:6.90608012676239\n",
      "06/11 10:31:47 PM |\t  \n",
      "\n",
      "  ----------------epoch:2,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:31:47 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:31:59 PM |\t   18.2%:\t  W_train_loss:0.3876388\tV_train_syn_loss:0.7075274\tV_train_loss:0.9024591\t  V_star_val_loss:1.1537409\t  V_val_loss:1.1536330\n",
      "06/11 10:32:10 PM |\t   45.5%:\t  W_train_loss:0.3085678\tV_train_syn_loss:0.3689243\tV_train_loss:1.1757449\t  V_star_val_loss:1.0195393\t  V_val_loss:1.0193858\n",
      "06/11 10:32:20 PM |\t   72.7%:\t  W_train_loss:0.1254739\tV_train_syn_loss:0.4351690\tV_train_loss:0.9211944\t  V_star_val_loss:1.0821571\t  V_val_loss:1.0822519\n",
      "06/11 10:32:34 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:32:34 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzung', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:32:34 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:32:48 PM |\t  computing score...\n",
      "06/11 10:32:48 PM |\t  model_w_in_main sacreBLEU : 41.329454\n",
      "06/11 10:32:48 PM |\t  model_w_in_main test loss : 0.741864\n",
      "06/11 10:32:50 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:32:50 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wieder aufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung wünschen, daß Sie einen angenehmen Festtag genossen']\n",
      "06/11 10:32:50 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:33:04 PM |\t  computing score...\n",
      "06/11 10:33:04 PM |\t  model_v_in_main sacreBLEU : 25.424413\n",
      "06/11 10:33:04 PM |\t  model_v_in_main test loss : 1.029466\n",
      "06/11 10:33:04 PM |\t  ('Attention Weights A : ', tensor([1.0043, 1.0007, 0.9993, 1.0025, 0.9970, 0.9999, 1.0007, 1.0031, 0.9946,\n",
      "        0.9966, 1.0050, 0.9989, 1.0017, 0.9979, 0.9925, 0.9984, 1.0007, 0.9998,\n",
      "        1.0020, 1.0015, 1.0034, 0.9998, 0.9993, 0.9984, 0.9966, 0.9998, 1.0008,\n",
      "        1.0051, 0.9977, 0.9975, 0.9957, 1.0048, 1.0066, 0.9942, 0.9974, 0.9933,\n",
      "        1.0012, 0.9940, 1.0025, 0.9978, 0.9952, 0.9946, 1.0003, 1.0006, 0.9985,\n",
      "        1.0019, 1.0016, 0.9984], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:33:04 PM |\t  1e+02%:\t  W_train_loss:0.1523071\tV_train_syn_loss:0.6796282\tV_train_loss:1.1048505\t  V_star_val_loss:1.0747025\t  V_val_loss:1.0747476\n",
      "06/11 10:33:04 PM |\t  w_train_loss:2.9219628535211086,v_train_loss:6.573746717534959\n",
      "06/11 10:33:04 PM |\t  \n",
      "\n",
      "  ----------------epoch:3,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:33:04 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:33:16 PM |\t   18.2%:\t  W_train_loss:0.1843104\tV_train_syn_loss:0.5583703\tV_train_loss:0.8758179\t  V_star_val_loss:1.1441338\t  V_val_loss:1.1441608\n",
      "06/11 10:33:26 PM |\t   45.5%:\t  W_train_loss:0.1268395\tV_train_syn_loss:0.6483152\tV_train_loss:1.1710573\t  V_star_val_loss:1.0231640\t  V_val_loss:1.0232199\n",
      "06/11 10:33:37 PM |\t   72.7%:\t  W_train_loss:0.0439606\tV_train_syn_loss:0.7168953\tV_train_loss:0.9246027\t  V_star_val_loss:1.0536621\t  V_val_loss:1.0536467\n",
      "06/11 10:33:51 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:33:51 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzung', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:33:51 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:34:05 PM |\t  computing score...\n",
      "06/11 10:34:05 PM |\t  model_w_in_main sacreBLEU : 48.393856\n",
      "06/11 10:34:05 PM |\t  model_w_in_main test loss : 0.791174\n",
      "06/11 10:34:08 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:34:08 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wieder aufgenommen, und wünsche Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung, daß Sie einen angenehmen Festtag genießen.']\n",
      "06/11 10:34:08 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:34:21 PM |\t  computing score...\n",
      "06/11 10:34:21 PM |\t  model_v_in_main sacreBLEU : 24.458622\n",
      "06/11 10:34:21 PM |\t  model_v_in_main test loss : 1.045166\n",
      "06/11 10:34:21 PM |\t  ('Attention Weights A : ', tensor([1.0023, 0.9981, 0.9965, 1.0020, 0.9977, 0.9968, 1.0032, 1.0017, 0.9954,\n",
      "        0.9970, 1.0079, 1.0011, 1.0040, 0.9949, 0.9918, 1.0013, 0.9990, 0.9976,\n",
      "        1.0021, 0.9997, 1.0015, 0.9974, 0.9988, 0.9957, 0.9997, 0.9992, 1.0002,\n",
      "        1.0042, 0.9947, 0.9979, 0.9953, 1.0075, 1.0059, 0.9944, 0.9967, 0.9907,\n",
      "        1.0037, 0.9957, 1.0015, 0.9986, 0.9955, 0.9941, 1.0002, 1.0003, 0.9984,\n",
      "        1.0030, 0.9991, 0.9983], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:34:21 PM |\t  1e+02%:\t  W_train_loss:0.0387467\tV_train_syn_loss:0.4033146\tV_train_loss:1.1543648\t  V_star_val_loss:1.0778231\t  V_val_loss:1.0777543\n",
      "06/11 10:34:21 PM |\t  w_train_loss:1.181571600958705,v_train_loss:6.9806860983371735\n",
      "06/11 10:34:21 PM |\t  \n",
      "\n",
      "  ----------------epoch:4,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:34:21 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:34:33 PM |\t   18.2%:\t  W_train_loss:0.0898085\tV_train_syn_loss:0.3952048\tV_train_loss:0.8904622\t  V_star_val_loss:1.1737183\t  V_val_loss:1.1737703\n",
      "06/11 10:34:45 PM |\t   45.5%:\t  W_train_loss:0.0640859\tV_train_syn_loss:0.7804531\tV_train_loss:1.1980422\t  V_star_val_loss:1.0669099\t  V_val_loss:1.0670084\n",
      "06/11 10:34:55 PM |\t   72.7%:\t  W_train_loss:0.0291181\tV_train_syn_loss:0.6276429\tV_train_loss:0.9645195\t  V_star_val_loss:1.1027290\t  V_val_loss:1.1027230\n",
      "06/11 10:35:10 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:35:10 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlament für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:35:10 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:35:24 PM |\t  computing score...\n",
      "06/11 10:35:24 PM |\t  model_w_in_main sacreBLEU : 49.035962\n",
      "06/11 10:35:24 PM |\t  model_w_in_main test loss : 0.815649\n",
      "06/11 10:35:27 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:35:27 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999, unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr wünschen, in der Hoffnung, daß Sie einen angenehmen Festtag']\n",
      "06/11 10:35:27 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:35:40 PM |\t  computing score...\n",
      "06/11 10:35:40 PM |\t  model_v_in_main sacreBLEU : 22.556686\n",
      "06/11 10:35:40 PM |\t  model_v_in_main test loss : 1.086446\n",
      "06/11 10:35:40 PM |\t  ('Attention Weights A : ', tensor([1.0023, 0.9992, 0.9993, 1.0052, 1.0005, 0.9973, 1.0035, 1.0047, 0.9985,\n",
      "        1.0000, 1.0055, 0.9994, 1.0011, 0.9944, 0.9947, 0.9999, 0.9993, 1.0004,\n",
      "        1.0021, 0.9966, 1.0047, 1.0004, 1.0008, 0.9989, 0.9999, 0.9990, 0.9980,\n",
      "        1.0050, 0.9956, 0.9982, 0.9959, 1.0094, 1.0086, 0.9943, 0.9935, 0.9931,\n",
      "        1.0033, 0.9975, 1.0040, 0.9977, 0.9950, 0.9946, 1.0002, 0.9997, 0.9993,\n",
      "        1.0032, 1.0020, 0.9983], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:35:40 PM |\t  1e+02%:\t  W_train_loss:0.0128911\tV_train_syn_loss:0.3379886\tV_train_loss:1.1613113\t  V_star_val_loss:1.0963931\t  V_val_loss:1.0964249\n",
      "06/11 10:35:40 PM |\t  w_train_loss:0.5877111600711942,v_train_loss:6.423868164420128\n",
      "06/11 10:35:40 PM |\t  \n",
      "\n",
      "  ----------------epoch:5,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:35:40 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:35:52 PM |\t   18.2%:\t  W_train_loss:0.0561497\tV_train_syn_loss:1.0217177\tV_train_loss:0.9517877\t  V_star_val_loss:1.1770085\t  V_val_loss:1.1771071\n",
      "06/11 10:36:03 PM |\t   45.5%:\t  W_train_loss:0.0362921\tV_train_syn_loss:0.4127210\tV_train_loss:1.1931520\t  V_star_val_loss:1.0342006\t  V_val_loss:1.0341085\n",
      "06/11 10:36:13 PM |\t   72.7%:\t  W_train_loss:0.0106318\tV_train_syn_loss:0.4393709\tV_train_loss:0.9522755\t  V_star_val_loss:1.0675425\t  V_val_loss:1.0673875\n",
      "06/11 10:36:27 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:36:27 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments ab, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:36:27 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:36:41 PM |\t  computing score...\n",
      "06/11 10:36:41 PM |\t  model_w_in_main sacreBLEU : 50.117836\n",
      "06/11 10:36:41 PM |\t  model_w_in_main test loss : 0.831303\n",
      "06/11 10:36:44 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:36:44 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich wünsche Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung, daß Sie eine angenehme Festzeit genoss']\n",
      "06/11 10:36:44 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:36:57 PM |\t  computing score...\n",
      "06/11 10:36:57 PM |\t  model_v_in_main sacreBLEU : 24.178344\n",
      "06/11 10:36:57 PM |\t  model_v_in_main test loss : 1.067873\n",
      "06/11 10:36:57 PM |\t  ('Attention Weights A : ', tensor([1.0045, 0.9960, 1.0002, 1.0081, 1.0029, 0.9975, 1.0007, 1.0076, 0.9980,\n",
      "        0.9986, 1.0035, 0.9995, 1.0008, 0.9945, 0.9946, 1.0007, 1.0015, 1.0006,\n",
      "        1.0017, 0.9975, 1.0059, 0.9987, 1.0031, 0.9995, 0.9999, 1.0008, 0.9993,\n",
      "        1.0049, 0.9955, 0.9980, 0.9950, 1.0094, 1.0075, 0.9937, 0.9939, 0.9932,\n",
      "        1.0032, 1.0003, 1.0063, 0.9988, 0.9950, 0.9946, 1.0002, 1.0026, 0.9999,\n",
      "        1.0033, 1.0025, 0.9988], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:36:57 PM |\t  1e+02%:\t  W_train_loss:0.0079543\tV_train_syn_loss:0.4116593\tV_train_loss:1.1553080\t  V_star_val_loss:1.0954822\t  V_val_loss:1.0955175\n",
      "06/11 10:36:57 PM |\t  w_train_loss:0.3330836854875088,v_train_loss:6.856406681239605\n",
      "06/11 10:36:57 PM |\t  \n",
      "\n",
      "  ----------------epoch:6,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:36:57 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:37:09 PM |\t   18.2%:\t  W_train_loss:0.0573585\tV_train_syn_loss:0.5618899\tV_train_loss:0.9146362\t  V_star_val_loss:1.1750883\t  V_val_loss:1.1750575\n",
      "06/11 10:37:20 PM |\t   45.5%:\t  W_train_loss:0.0187863\tV_train_syn_loss:0.3652821\tV_train_loss:1.2671863\t  V_star_val_loss:1.0986060\t  V_val_loss:1.0985869\n",
      "06/11 10:37:30 PM |\t   72.7%:\t  W_train_loss:0.0050699\tV_train_syn_loss:0.3596816\tV_train_loss:0.9566353\t  V_star_val_loss:1.1340826\t  V_val_loss:1.1340150\n",
      "06/11 10:37:45 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:37:45 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem Freitag unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:37:45 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:37:58 PM |\t  computing score...\n",
      "06/11 10:37:58 PM |\t  model_w_in_main sacreBLEU : 50.286184\n",
      "06/11 10:37:58 PM |\t  model_w_in_main test loss : 0.849465\n",
      "06/11 10:38:01 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:38:01 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung wünschen, daß Sie einen angenehmen Festtag genoss']\n",
      "06/11 10:38:01 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:38:14 PM |\t  computing score...\n",
      "06/11 10:38:14 PM |\t  model_v_in_main sacreBLEU : 24.609573\n",
      "06/11 10:38:14 PM |\t  model_v_in_main test loss : 1.070207\n",
      "06/11 10:38:14 PM |\t  ('Attention Weights A : ', tensor([1.0072, 0.9991, 1.0006, 1.0075, 1.0061, 0.9984, 0.9979, 1.0107, 0.9988,\n",
      "        0.9981, 1.0034, 0.9997, 1.0014, 0.9959, 0.9955, 1.0014, 0.9996, 1.0005,\n",
      "        1.0010, 0.9971, 1.0069, 1.0014, 1.0056, 0.9994, 0.9987, 1.0012, 0.9984,\n",
      "        1.0036, 0.9959, 0.9973, 0.9928, 1.0077, 1.0071, 0.9937, 0.9932, 0.9938,\n",
      "        1.0012, 1.0001, 1.0075, 0.9983, 0.9949, 0.9943, 1.0002, 1.0033, 0.9998,\n",
      "        1.0033, 1.0028, 0.9984], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:38:14 PM |\t  1e+02%:\t  W_train_loss:0.0032765\tV_train_syn_loss:0.5801059\tV_train_loss:1.1644251\t  V_star_val_loss:1.1221976\t  V_val_loss:1.1222787\n",
      "06/11 10:38:14 PM |\t  w_train_loss:0.25347352982498705,v_train_loss:5.600878154858947\n",
      "06/11 10:38:14 PM |\t  \n",
      "\n",
      "  ----------------epoch:7,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:38:14 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:38:26 PM |\t   18.2%:\t  W_train_loss:0.0389818\tV_train_syn_loss:0.7403291\tV_train_loss:0.9080119\t  V_star_val_loss:1.1697776\t  V_val_loss:1.1698729\n",
      "06/11 10:38:37 PM |\t   45.5%:\t  W_train_loss:0.0089635\tV_train_syn_loss:0.3406238\tV_train_loss:1.2801566\t  V_star_val_loss:1.1013066\t  V_val_loss:1.1013155\n",
      "06/11 10:38:47 PM |\t   72.7%:\t  W_train_loss:0.0027110\tV_train_syn_loss:0.1852336\tV_train_loss:0.9616116\t  V_star_val_loss:1.1180856\t  V_val_loss:1.1180818\n",
      "06/11 10:39:02 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:39:02 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999 abgehaltene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:39:02 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:39:15 PM |\t  computing score...\n",
      "06/11 10:39:15 PM |\t  model_w_in_main sacreBLEU : 51.885603\n",
      "06/11 10:39:15 PM |\t  model_w_in_main test loss : 0.863067\n",
      "06/11 10:39:18 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:39:18 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr in der Hoffnung wünschen, daß Sie eine angenehme Festzeit genoss']\n",
      "06/11 10:39:18 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:39:32 PM |\t  computing score...\n",
      "06/11 10:39:32 PM |\t  model_v_in_main sacreBLEU : 25.007750\n",
      "06/11 10:39:32 PM |\t  model_v_in_main test loss : 1.108448\n",
      "06/11 10:39:32 PM |\t  ('Attention Weights A : ', tensor([1.0067, 0.9964, 0.9990, 1.0103, 1.0065, 0.9981, 0.9979, 1.0104, 0.9958,\n",
      "        0.9953, 1.0038, 1.0010, 1.0011, 0.9951, 0.9946, 1.0026, 0.9993, 0.9998,\n",
      "        1.0012, 0.9987, 1.0085, 1.0029, 1.0085, 0.9986, 0.9981, 1.0028, 0.9970,\n",
      "        1.0032, 0.9955, 0.9977, 0.9929, 1.0092, 1.0081, 0.9938, 0.9932, 0.9931,\n",
      "        1.0024, 1.0007, 1.0066, 0.9990, 0.9948, 0.9935, 1.0002, 1.0035, 0.9998,\n",
      "        1.0035, 1.0030, 0.9981], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:39:32 PM |\t  1e+02%:\t  W_train_loss:0.0020596\tV_train_syn_loss:0.3762684\tV_train_loss:1.1801640\t  V_star_val_loss:1.1447532\t  V_val_loss:1.1445779\n",
      "06/11 10:39:32 PM |\t  w_train_loss:0.15814781736116856,v_train_loss:4.927364580333233\n",
      "06/11 10:39:32 PM |\t  \n",
      "\n",
      "  ----------------epoch:8,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:39:32 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:39:44 PM |\t   18.2%:\t  W_train_loss:0.0326071\tV_train_syn_loss:0.6618839\tV_train_loss:0.9345182\t  V_star_val_loss:1.1683729\t  V_val_loss:1.1683061\n",
      "06/11 10:39:55 PM |\t   45.5%:\t  W_train_loss:0.0056478\tV_train_syn_loss:0.3763049\tV_train_loss:1.2933208\t  V_star_val_loss:1.1071183\t  V_val_loss:1.1069764\n",
      "06/11 10:40:05 PM |\t   72.7%:\t  W_train_loss:0.0017325\tV_train_syn_loss:0.1328058\tV_train_loss:0.9821730\t  V_star_val_loss:1.1167544\t  V_val_loss:1.1167371\n",
      "06/11 10:40:20 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:40:20 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:40:20 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:40:33 PM |\t  computing score...\n",
      "06/11 10:40:33 PM |\t  model_w_in_main sacreBLEU : 51.509249\n",
      "06/11 10:40:33 PM |\t  model_w_in_main test loss : 0.877862\n",
      "06/11 10:40:36 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:40:36 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr wünschen, in der Hoffnung, daß Sie einen angenehmen Festzeitgenoss']\n",
      "06/11 10:40:36 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:40:50 PM |\t  computing score...\n",
      "06/11 10:40:50 PM |\t  model_v_in_main sacreBLEU : 24.370455\n",
      "06/11 10:40:50 PM |\t  model_v_in_main test loss : 1.119173\n",
      "06/11 10:40:50 PM |\t  ('Attention Weights A : ', tensor([1.0076, 0.9977, 1.0007, 1.0110, 1.0067, 0.9990, 0.9988, 1.0104, 0.9957,\n",
      "        0.9950, 1.0036, 1.0020, 1.0017, 0.9954, 0.9952, 1.0023, 0.9998, 0.9989,\n",
      "        1.0019, 1.0018, 1.0084, 1.0047, 1.0082, 0.9984, 0.9981, 1.0048, 0.9980,\n",
      "        1.0036, 0.9970, 0.9992, 0.9955, 1.0107, 1.0073, 0.9939, 0.9934, 0.9931,\n",
      "        1.0047, 1.0011, 1.0046, 0.9981, 0.9942, 0.9924, 1.0008, 1.0035, 1.0002,\n",
      "        1.0039, 1.0036, 0.9975], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:40:50 PM |\t  1e+02%:\t  W_train_loss:0.0014089\tV_train_syn_loss:0.1738516\tV_train_loss:1.2010973\t  V_star_val_loss:1.1566403\t  V_val_loss:1.1569264\n",
      "06/11 10:40:50 PM |\t  w_train_loss:0.12418914202135056,v_train_loss:4.034538393840194\n",
      "06/11 10:40:50 PM |\t  \n",
      "\n",
      "  ----------------epoch:9,\t\tlr_w:0.0005,\t\tlr_v:0.0005,\t\tlr_A:0.001----------------\n",
      "06/11 10:40:50 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:41:02 PM |\t   18.2%:\t  W_train_loss:0.0031309\tV_train_syn_loss:0.3349262\tV_train_loss:0.9567473\t  V_star_val_loss:1.2100244\t  V_val_loss:1.2101400\n",
      "06/11 10:41:13 PM |\t   45.5%:\t  W_train_loss:0.0028198\tV_train_syn_loss:0.1389126\tV_train_loss:1.3062750\t  V_star_val_loss:1.1350642\t  V_val_loss:1.1350786\n",
      "06/11 10:41:23 PM |\t   72.7%:\t  W_train_loss:0.0011316\tV_train_syn_loss:0.0447675\tV_train_loss:0.9829599\t  V_star_val_loss:1.1455228\t  V_val_loss:1.1455625\n",
      "06/11 10:41:37 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:41:37 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:41:37 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:41:51 PM |\t  computing score...\n",
      "06/11 10:41:51 PM |\t  model_w_in_main sacreBLEU : 51.340810\n",
      "06/11 10:41:51 PM |\t  model_w_in_main test loss : 0.891208\n",
      "06/11 10:41:54 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:41:54 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, den 17. Dezember 1999 unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr wünschen, in der Hoffnung, daß Sie eine angenehme Festzeit genießen']\n",
      "06/11 10:41:54 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:42:07 PM |\t  computing score...\n",
      "06/11 10:42:07 PM |\t  model_v_in_main sacreBLEU : 26.131788\n",
      "06/11 10:42:07 PM |\t  model_v_in_main test loss : 1.127941\n",
      "06/11 10:42:07 PM |\t  ('Attention Weights A : ', tensor([1.0088, 0.9977, 1.0010, 1.0108, 1.0062, 0.9973, 1.0004, 1.0105, 0.9956,\n",
      "        0.9938, 1.0034, 1.0009, 1.0015, 0.9954, 0.9942, 1.0029, 1.0001, 0.9976,\n",
      "        1.0027, 1.0017, 1.0087, 1.0038, 1.0083, 0.9983, 0.9980, 1.0047, 0.9987,\n",
      "        1.0030, 0.9955, 1.0001, 0.9965, 1.0114, 1.0066, 0.9937, 0.9937, 0.9935,\n",
      "        1.0060, 1.0013, 1.0023, 0.9985, 0.9934, 0.9932, 1.0008, 1.0028, 1.0007,\n",
      "        1.0038, 1.0041, 0.9970], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:42:07 PM |\t  1e+02%:\t  W_train_loss:0.0009262\tV_train_syn_loss:0.2206012\tV_train_loss:1.2291766\t  V_star_val_loss:1.1632249\t  V_val_loss:1.1632708\n",
      "06/11 10:42:07 PM |\t  w_train_loss:0.024025614839047194,v_train_loss:2.217622574418783\n",
      "06/11 10:42:07 PM |\t  \n",
      "\n",
      "  ----------------epoch:10,\t\tlr_w:0.00040500000000000003,\t\tlr_v:0.00040500000000000003,\t\tlr_A:0.001----------------\n",
      "06/11 10:42:07 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:42:19 PM |\t   18.2%:\t  W_train_loss:0.0015016\tV_train_syn_loss:0.3133724\tV_train_loss:0.9482152\t  V_star_val_loss:1.2048947\t  V_val_loss:1.2056978\n",
      "06/11 10:42:30 PM |\t   45.5%:\t  W_train_loss:0.0018069\tV_train_syn_loss:0.2085292\tV_train_loss:1.3425731\t  V_star_val_loss:1.1434338\t  V_val_loss:1.1481059\n",
      "06/11 10:42:40 PM |\t   72.7%:\t  W_train_loss:0.0010114\tV_train_syn_loss:0.1151796\tV_train_loss:1.0209303\t  V_star_val_loss:1.1682381\t  V_val_loss:1.1713665\n",
      "06/11 10:42:54 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:42:54 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsumgebung', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:42:54 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:43:08 PM |\t  computing score...\n",
      "06/11 10:43:08 PM |\t  model_w_in_main sacreBLEU : 50.196377\n",
      "06/11 10:43:08 PM |\t  model_w_in_main test loss : 0.899433\n",
      "06/11 10:43:11 PM |\t  x_decoded[:2]:['translate English to German: Resumption of the session', 'translate English to German: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.']\n",
      "06/11 10:43:11 PM |\t  pred_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember 1999, unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, und ich möchte Ihnen noch einmal ein glückliches neues Jahr wünschen, in der Hoffnung, dass Sie einen angenehmen Festzeitgenossen']\n",
      "06/11 10:43:11 PM |\t  label_decoded[:2]:['Wiederaufnahme der Sitzungsperiode', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.']\n",
      "06/11 10:43:24 PM |\t  computing score...\n",
      "06/11 10:43:24 PM |\t  model_v_in_main sacreBLEU : 24.407064\n",
      "06/11 10:43:24 PM |\t  model_v_in_main test loss : 1.156949\n",
      "06/11 10:43:24 PM |\t  ('Attention Weights A : ', tensor([1.0086, 0.9977, 1.0012, 1.0108, 1.0064, 0.9975, 1.0005, 1.0106, 0.9956,\n",
      "        0.9936, 1.0037, 1.0003, 1.0021, 0.9958, 0.9943, 1.0027, 0.9997, 0.9985,\n",
      "        1.0025, 1.0016, 1.0083, 1.0036, 1.0079, 0.9981, 0.9981, 1.0044, 0.9979,\n",
      "        1.0034, 0.9957, 0.9999, 0.9964, 1.0113, 1.0096, 0.9939, 0.9936, 0.9932,\n",
      "        1.0064, 1.0044, 1.0033, 0.9986, 0.9936, 0.9928, 1.0008, 1.0029, 1.0010,\n",
      "        1.0038, 1.0040, 0.9996], device='cuda:0', grad_fn=<ReluBackward0>))\n",
      "06/11 10:43:24 PM |\t  1e+02%:\t  W_train_loss:0.0087254\tV_train_syn_loss:0.2170801\tV_train_loss:1.2161107\t  V_star_val_loss:1.1764119\t  V_val_loss:1.1702073\n",
      "06/11 10:43:24 PM |\t  w_train_loss:0.0391357698244974,v_train_loss:2.5624838690273464\n",
      "06/11 10:43:24 PM |\t  \n",
      "\n",
      "  ----------------epoch:11,\t\tlr_w:0.00045000000000000004,\t\tlr_v:0.00045000000000000004,\t\tlr_A:0.001----------------\n",
      "06/11 10:43:24 PM |\t  split size:[4, 2, 4, 6]\n",
      "06/11 10:43:37 PM |\t   18.2%:\t  W_train_loss:0.0021717\tV_train_syn_loss:0.2719641\tV_train_loss:0.9613936\t  V_star_val_loss:1.2409665\t  V_val_loss:1.2409916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_132508/2462265064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{args.A_lr}----------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mw_train_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv_train_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmy_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0marchitect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtot_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mscheduler_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_132508/486369397.py\u001b[0m in \u001b[0;36mmy_train\u001b[1;34m(epoch, _dataloader, validdataloader, w_model, v_model, architect, A, w_optimizer, v_optimizer, lr_w, lr_v, tot_iter)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mepsilon_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munrolled_w_lr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mepsilon_v\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munrolled_v_lr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             v_star_val_loss = architect.step(input_w,  output_w, input_w_attn, output_w_attn, w_optimizer,\n\u001b[0m\u001b[0;32m     52\u001b[0m                                              \u001b[0minput_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_v_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_v_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                                              \u001b[0minput_A_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_A_v_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_A_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_A_v_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\architect.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, input_w, output_w, input_w_attn, output_w_attn, w_optimizer, input_v, input_v_attn, output_v, output_v_attn, input_syn, input_syn_attn, input_A_v, input_A_v_attn, output_A_v, output_A_v_attn, v_optimizer, attn_idx, lr_w, lr_v)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mvector_s_dash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munrolled_v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         implicit_grads_A = self._outer_A(vector_s_dash, input_w, output_w, input_w_attn,\n\u001b[0m\u001b[0;32m    175\u001b[0m                                          output_w_attn, input_v, input_v_attn, attn_idx, unrolled_w_model, lr_w, lr_v)\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\architect.py\u001b[0m in \u001b[0;36m_outer_A\u001b[1;34m(self, vector_s_dash, w_input, w_target, w_input_attn, w_target_attn, input_v, input_v_attn, attn_idx, unrolled_w_model, eta_w, eta_v, r)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mR1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         loss_aug_m = calc_loss_aug(\n\u001b[0m\u001b[0;32m    237\u001b[0m             input_v, input_v_attn, unrolled_w_model, self.v_model)\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\losses.py\u001b[0m in \u001b[0;36mcalc_loss_aug\u001b[1;34m(input_syn_ids, input_syn_attn, w_model, v_model)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_loss_aug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_syn_attn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mw_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mturnoff_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_syn_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;31m# print(output_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0matt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput_ids\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\GitCode\\Self-teaching-for-machine-translation\\T5\\T5.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, num_beams, max_length)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m## sampling with top_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# output_ids = self.model.generate( input_ids = input_ids, num_beams = 1, max_length = max_length, top_p = 0.95, top_k = 50, no_repeat_ngram_size = 2, repetition_penalty = 1.2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[1;31m# 10. run greedy search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m             return self.greedy_search(\n\u001b[0m\u001b[0;32m   1255\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m             \u001b[1;31m# forward pass to get next token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   1639\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m         \u001b[1;31m# Decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1638\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1639\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                 )\n\u001b[0;32m   1032\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m   1034\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;31m# Apply Feed Forward layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mforwarded_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if(args.valid_begin==1):\n",
    "#     my_test(valid_dataloader,model_w,-1) #before train\n",
    "    # my_test(valid_dataloader,model_v,-1)  \n",
    "\n",
    "tot_iter = [0]\n",
    "for epoch in range(args.epochs):\n",
    "    lr_w = scheduler_w.get_lr()[0]\n",
    "    lr_v = scheduler_v.get_lr()[0]\n",
    "\n",
    "    logging.info(f\"\\n\\n  ----------------epoch:{epoch},\\t\\tlr_w:{lr_w},\\t\\tlr_v:{lr_v},\\t\\tlr_A:{args.A_lr}----------------\")\n",
    "\n",
    "    w_train_loss,v_train_loss =  my_train(epoch, train_dataloader, valid_dataloader, model_w, model_v,  architect, A, w_optimizer, v_optimizer, lr_w,lr_v,tot_iter)\n",
    "    \n",
    "    scheduler_w.step()\n",
    "    scheduler_v.step()\n",
    "\n",
    "\n",
    "    logging.info(f\"w_train_loss:{w_train_loss},v_train_loss:{v_train_loss}\")\n",
    "    # wandb.log({'w_train_loss': w_train_loss, 'v_train_loss':v_train_loss})\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model_v,'./model/'+now+'model_w.pt')\n",
    "torch.save(model_v,'./model/'+now+'model_v.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.shared.weight model.shared.weight\n",
      "tensor(0.0762, device='cuda:0')\n",
      "model.encoder.embed_tokens.weight model.encoder.embed_tokens.weight\n",
      "tensor(0.0762, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.SelfAttention.q.weight model.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0664, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.SelfAttention.k.weight model.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0661, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.SelfAttention.v.weight model.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0696, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.SelfAttention.o.weight model.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0699, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "tensor(6.2782e-05, device='cuda:0')\n",
      "model.encoder.block.0.layer.0.layer_norm.weight model.encoder.block.0.layer.0.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.0.layer.1.DenseReluDense.wi.weight model.encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.1929, device='cuda:0')\n",
      "model.encoder.block.0.layer.1.DenseReluDense.wo.weight model.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.2144, device='cuda:0')\n",
      "model.encoder.block.0.layer.1.layer_norm.weight model.encoder.block.0.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.1.layer.0.SelfAttention.q.weight model.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0681, device='cuda:0')\n",
      "model.encoder.block.1.layer.0.SelfAttention.k.weight model.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0678, device='cuda:0')\n",
      "model.encoder.block.1.layer.0.SelfAttention.v.weight model.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0689, device='cuda:0')\n",
      "model.encoder.block.1.layer.0.SelfAttention.o.weight model.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0679, device='cuda:0')\n",
      "model.encoder.block.1.layer.0.layer_norm.weight model.encoder.block.1.layer.0.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.1.layer.1.DenseReluDense.wi.weight model.encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.2198, device='cuda:0')\n",
      "model.encoder.block.1.layer.1.DenseReluDense.wo.weight model.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.2205, device='cuda:0')\n",
      "model.encoder.block.1.layer.1.layer_norm.weight model.encoder.block.1.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.2.layer.0.SelfAttention.q.weight model.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0803, device='cuda:0')\n",
      "model.encoder.block.2.layer.0.SelfAttention.k.weight model.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0771, device='cuda:0')\n",
      "model.encoder.block.2.layer.0.SelfAttention.v.weight model.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0743, device='cuda:0')\n",
      "model.encoder.block.2.layer.0.SelfAttention.o.weight model.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0739, device='cuda:0')\n",
      "model.encoder.block.2.layer.0.layer_norm.weight model.encoder.block.2.layer.0.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.2.layer.1.DenseReluDense.wi.weight model.encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.1614, device='cuda:0')\n",
      "model.encoder.block.2.layer.1.DenseReluDense.wo.weight model.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.1900, device='cuda:0')\n",
      "model.encoder.block.2.layer.1.layer_norm.weight model.encoder.block.2.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.3.layer.0.SelfAttention.q.weight model.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0866, device='cuda:0')\n",
      "model.encoder.block.3.layer.0.SelfAttention.k.weight model.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0865, device='cuda:0')\n",
      "model.encoder.block.3.layer.0.SelfAttention.v.weight model.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0760, device='cuda:0')\n",
      "model.encoder.block.3.layer.0.SelfAttention.o.weight model.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0780, device='cuda:0')\n",
      "model.encoder.block.3.layer.0.layer_norm.weight model.encoder.block.3.layer.0.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.3.layer.1.DenseReluDense.wi.weight model.encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.1897, device='cuda:0')\n",
      "model.encoder.block.3.layer.1.DenseReluDense.wo.weight model.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.1963, device='cuda:0')\n",
      "model.encoder.block.3.layer.1.layer_norm.weight model.encoder.block.3.layer.1.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.encoder.block.4.layer.0.SelfAttention.q.weight model.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0906, device='cuda:0')\n",
      "model.encoder.block.4.layer.0.SelfAttention.k.weight model.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0927, device='cuda:0')\n",
      "model.encoder.block.4.layer.0.SelfAttention.v.weight model.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0954, device='cuda:0')\n",
      "model.encoder.block.4.layer.0.SelfAttention.o.weight model.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0936, device='cuda:0')\n",
      "model.encoder.block.4.layer.0.layer_norm.weight model.encoder.block.4.layer.0.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.encoder.block.4.layer.1.DenseReluDense.wi.weight model.encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.2229, device='cuda:0')\n",
      "model.encoder.block.4.layer.1.DenseReluDense.wo.weight model.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.2206, device='cuda:0')\n",
      "model.encoder.block.4.layer.1.layer_norm.weight model.encoder.block.4.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.encoder.block.5.layer.0.SelfAttention.q.weight model.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0915, device='cuda:0')\n",
      "model.encoder.block.5.layer.0.SelfAttention.k.weight model.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0874, device='cuda:0')\n",
      "model.encoder.block.5.layer.0.SelfAttention.v.weight model.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0982, device='cuda:0')\n",
      "model.encoder.block.5.layer.0.SelfAttention.o.weight model.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0977, device='cuda:0')\n",
      "model.encoder.block.5.layer.0.layer_norm.weight model.encoder.block.5.layer.0.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.encoder.block.5.layer.1.DenseReluDense.wi.weight model.encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "tensor(0.2736, device='cuda:0')\n",
      "model.encoder.block.5.layer.1.DenseReluDense.wo.weight model.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "tensor(0.3177, device='cuda:0')\n",
      "model.encoder.block.5.layer.1.layer_norm.weight model.encoder.block.5.layer.1.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.encoder.final_layer_norm.weight model.encoder.final_layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.decoder.embed_tokens.weight model.decoder.embed_tokens.weight\n",
      "tensor(0.0762, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.SelfAttention.q.weight model.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "tensor(0.1262, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.SelfAttention.k.weight model.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "tensor(0.1346, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.SelfAttention.v.weight model.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "tensor(0.1916, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.SelfAttention.o.weight model.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "tensor(0.1862, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.decoder.block.0.layer.0.layer_norm.weight model.decoder.block.0.layer.0.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.0.layer.1.EncDecAttention.q.weight model.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.1668, device='cuda:0')\n",
      "model.decoder.block.0.layer.1.EncDecAttention.k.weight model.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.1485, device='cuda:0')\n",
      "model.decoder.block.0.layer.1.EncDecAttention.v.weight model.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.1946, device='cuda:0')\n",
      "model.decoder.block.0.layer.1.EncDecAttention.o.weight model.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.1884, device='cuda:0')\n",
      "model.decoder.block.0.layer.1.layer_norm.weight model.decoder.block.0.layer.1.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.0.layer.2.DenseReluDense.wi.weight model.decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.7417, device='cuda:0')\n",
      "model.decoder.block.0.layer.2.DenseReluDense.wo.weight model.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.5468, device='cuda:0')\n",
      "model.decoder.block.0.layer.2.layer_norm.weight model.decoder.block.0.layer.2.layer_norm.weight\n",
      "tensor(0.0004, device='cuda:0')\n",
      "model.decoder.block.1.layer.0.SelfAttention.q.weight model.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "tensor(0.1235, device='cuda:0')\n",
      "model.decoder.block.1.layer.0.SelfAttention.k.weight model.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "tensor(0.1195, device='cuda:0')\n",
      "model.decoder.block.1.layer.0.SelfAttention.v.weight model.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "tensor(0.1573, device='cuda:0')\n",
      "model.decoder.block.1.layer.0.SelfAttention.o.weight model.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "tensor(0.1659, device='cuda:0')\n",
      "model.decoder.block.1.layer.0.layer_norm.weight model.decoder.block.1.layer.0.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.1.layer.1.EncDecAttention.q.weight model.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.1922, device='cuda:0')\n",
      "model.decoder.block.1.layer.1.EncDecAttention.k.weight model.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.1943, device='cuda:0')\n",
      "model.decoder.block.1.layer.1.EncDecAttention.v.weight model.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.1697, device='cuda:0')\n",
      "model.decoder.block.1.layer.1.EncDecAttention.o.weight model.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.2004, device='cuda:0')\n",
      "model.decoder.block.1.layer.1.layer_norm.weight model.decoder.block.1.layer.1.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.1.layer.2.DenseReluDense.wi.weight model.decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.4204, device='cuda:0')\n",
      "model.decoder.block.1.layer.2.DenseReluDense.wo.weight model.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.4489, device='cuda:0')\n",
      "model.decoder.block.1.layer.2.layer_norm.weight model.decoder.block.1.layer.2.layer_norm.weight\n",
      "tensor(0.0004, device='cuda:0')\n",
      "model.decoder.block.2.layer.0.SelfAttention.q.weight model.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "tensor(0.1091, device='cuda:0')\n",
      "model.decoder.block.2.layer.0.SelfAttention.k.weight model.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "tensor(0.1046, device='cuda:0')\n",
      "model.decoder.block.2.layer.0.SelfAttention.v.weight model.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "tensor(0.1173, device='cuda:0')\n",
      "model.decoder.block.2.layer.0.SelfAttention.o.weight model.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "tensor(0.1574, device='cuda:0')\n",
      "model.decoder.block.2.layer.0.layer_norm.weight model.decoder.block.2.layer.0.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.2.layer.1.EncDecAttention.q.weight model.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.1380, device='cuda:0')\n",
      "model.decoder.block.2.layer.1.EncDecAttention.k.weight model.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.1284, device='cuda:0')\n",
      "model.decoder.block.2.layer.1.EncDecAttention.v.weight model.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.1898, device='cuda:0')\n",
      "model.decoder.block.2.layer.1.EncDecAttention.o.weight model.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.2662, device='cuda:0')\n",
      "model.decoder.block.2.layer.1.layer_norm.weight model.decoder.block.2.layer.1.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.2.layer.2.DenseReluDense.wi.weight model.decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.3442, device='cuda:0')\n",
      "model.decoder.block.2.layer.2.DenseReluDense.wo.weight model.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.3545, device='cuda:0')\n",
      "model.decoder.block.2.layer.2.layer_norm.weight model.decoder.block.2.layer.2.layer_norm.weight\n",
      "tensor(0.0007, device='cuda:0')\n",
      "model.decoder.block.3.layer.0.SelfAttention.q.weight model.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "tensor(0.1224, device='cuda:0')\n",
      "model.decoder.block.3.layer.0.SelfAttention.k.weight model.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "tensor(0.1163, device='cuda:0')\n",
      "model.decoder.block.3.layer.0.SelfAttention.v.weight model.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "tensor(0.1727, device='cuda:0')\n",
      "model.decoder.block.3.layer.0.SelfAttention.o.weight model.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "tensor(0.1802, device='cuda:0')\n",
      "model.decoder.block.3.layer.0.layer_norm.weight model.decoder.block.3.layer.0.layer_norm.weight\n",
      "tensor(0.0003, device='cuda:0')\n",
      "model.decoder.block.3.layer.1.EncDecAttention.q.weight model.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.1042, device='cuda:0')\n",
      "model.decoder.block.3.layer.1.EncDecAttention.k.weight model.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.1056, device='cuda:0')\n",
      "model.decoder.block.3.layer.1.EncDecAttention.v.weight model.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.1760, device='cuda:0')\n",
      "model.decoder.block.3.layer.1.EncDecAttention.o.weight model.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.2523, device='cuda:0')\n",
      "model.decoder.block.3.layer.1.layer_norm.weight model.decoder.block.3.layer.1.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.decoder.block.3.layer.2.DenseReluDense.wi.weight model.decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.2827, device='cuda:0')\n",
      "model.decoder.block.3.layer.2.DenseReluDense.wo.weight model.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.2755, device='cuda:0')\n",
      "model.decoder.block.3.layer.2.layer_norm.weight model.decoder.block.3.layer.2.layer_norm.weight\n",
      "tensor(0.0009, device='cuda:0')\n",
      "model.decoder.block.4.layer.0.SelfAttention.q.weight model.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0888, device='cuda:0')\n",
      "model.decoder.block.4.layer.0.SelfAttention.k.weight model.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0850, device='cuda:0')\n",
      "model.decoder.block.4.layer.0.SelfAttention.v.weight model.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0822, device='cuda:0')\n",
      "model.decoder.block.4.layer.0.SelfAttention.o.weight model.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0883, device='cuda:0')\n",
      "model.decoder.block.4.layer.0.layer_norm.weight model.decoder.block.4.layer.0.layer_norm.weight\n",
      "tensor(0.0002, device='cuda:0')\n",
      "model.decoder.block.4.layer.1.EncDecAttention.q.weight model.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.0811, device='cuda:0')\n",
      "model.decoder.block.4.layer.1.EncDecAttention.k.weight model.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.0860, device='cuda:0')\n",
      "model.decoder.block.4.layer.1.EncDecAttention.v.weight model.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.0931, device='cuda:0')\n",
      "model.decoder.block.4.layer.1.EncDecAttention.o.weight model.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.1142, device='cuda:0')\n",
      "model.decoder.block.4.layer.1.layer_norm.weight model.decoder.block.4.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.decoder.block.4.layer.2.DenseReluDense.wi.weight model.decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.1859, device='cuda:0')\n",
      "model.decoder.block.4.layer.2.DenseReluDense.wo.weight model.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.1863, device='cuda:0')\n",
      "model.decoder.block.4.layer.2.layer_norm.weight model.decoder.block.4.layer.2.layer_norm.weight\n",
      "tensor(0.0005, device='cuda:0')\n",
      "model.decoder.block.5.layer.0.SelfAttention.q.weight model.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "tensor(0.0737, device='cuda:0')\n",
      "model.decoder.block.5.layer.0.SelfAttention.k.weight model.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "tensor(0.0742, device='cuda:0')\n",
      "model.decoder.block.5.layer.0.SelfAttention.v.weight model.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "tensor(0.0506, device='cuda:0')\n",
      "model.decoder.block.5.layer.0.SelfAttention.o.weight model.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "tensor(0.0578, device='cuda:0')\n",
      "model.decoder.block.5.layer.0.layer_norm.weight model.decoder.block.5.layer.0.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.decoder.block.5.layer.1.EncDecAttention.q.weight model.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "tensor(0.0687, device='cuda:0')\n",
      "model.decoder.block.5.layer.1.EncDecAttention.k.weight model.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "tensor(0.0645, device='cuda:0')\n",
      "model.decoder.block.5.layer.1.EncDecAttention.v.weight model.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "tensor(0.0546, device='cuda:0')\n",
      "model.decoder.block.5.layer.1.EncDecAttention.o.weight model.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "tensor(0.0530, device='cuda:0')\n",
      "model.decoder.block.5.layer.1.layer_norm.weight model.decoder.block.5.layer.1.layer_norm.weight\n",
      "tensor(0.0001, device='cuda:0')\n",
      "model.decoder.block.5.layer.2.DenseReluDense.wi.weight model.decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "tensor(0.1197, device='cuda:0')\n",
      "model.decoder.block.5.layer.2.DenseReluDense.wo.weight model.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "tensor(0.1547, device='cuda:0')\n",
      "model.decoder.block.5.layer.2.layer_norm.weight model.decoder.block.5.layer.2.layer_norm.weight\n",
      "tensor(8.2999e-05, device='cuda:0')\n",
      "model.decoder.final_layer_norm.weight model.decoder.final_layer_norm.weight\n",
      "tensor(6.4970e-05, device='cuda:0')\n",
      "model.lm_head.weight model.lm_head.weight\n",
      "tensor(0.0762, device='cuda:0')\n",
      "embedding.embedding.weight embedding.embedding.weight\n",
      "tensor(0.0762, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.load('main_v.pt')\n",
    "m1.eval()\n",
    "m2 = torch.load('unrolled_v.pt')\n",
    "m2.eval()\n",
    "''\n",
    "for k1, k2 in zip(m1.state_dict(), m2.state_dict()):\n",
    "    v1= m1.state_dict()[k1]\n",
    "    v2= m2.state_dict()[k2]\n",
    "    print(k1,k2)\n",
    "    # print(v1,v2)\n",
    "    print((abs(v1.data-v2.data)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(1)\n",
    "a.add(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d33c3b0ef123e851f98887a8750ca7da758e4ff258891935cfe6ff9c0394387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
